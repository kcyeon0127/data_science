#!/usr/bin/env python3
"""
ë°°ì¹˜ ì‘ì—…ìœ¼ë¡œ ë¶„í•  ì²˜ë¦¬ - ë©”ëª¨ë¦¬ ì•ˆì „í•œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
"""

import os
import pandas as pd
import subprocess
import time
from tqdm.auto import tqdm

class BatchPreprocessor:
    def __init__(self):
        self.batch_dir = 'batch_data'
        self.output_dir = 'processed_data'
        os.makedirs(self.batch_dir, exist_ok=True)
        os.makedirs(self.output_dir, exist_ok=True)

    def step1_split_data(self, input_file='data/train.parquet', batch_size=500000):
        """1ë‹¨ê³„: ëŒ€ìš©ëŸ‰ íŒŒì¼ì„ ë°°ì¹˜ë¡œ ë¶„í• """

        print("ğŸ”„ 1ë‹¨ê³„: ë°ì´í„° ë¶„í•  ì‹œì‘...")
        print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {batch_size:,}í–‰")

        try:
            # ì „ì²´ íŒŒì¼ ë¡œë“œ (í•œ ë²ˆë§Œ)
            print("ğŸ“‚ ì „ì²´ íŒŒì¼ ë¡œë”© ì¤‘...")
            df = pd.read_parquet(input_file)
            total_rows = len(df)

            print(f"ğŸ“Š ì´ ë°ì´í„°: {total_rows:,}í–‰")

            num_batches = (total_rows // batch_size) + 1
            print(f"ğŸ“¦ ìƒì„±í•  ë°°ì¹˜ ìˆ˜: {num_batches}ê°œ")

            # ë°°ì¹˜ë³„ë¡œ ë¶„í•  ì €ì¥
            batch_files = []

            for i in tqdm(range(num_batches), desc="ë°°ì¹˜ ë¶„í• "):
                start_idx = i * batch_size
                end_idx = min(start_idx + batch_size, total_rows)

                if start_idx >= total_rows:
                    break

                batch_data = df.iloc[start_idx:end_idx].copy()
                batch_file = os.path.join(self.batch_dir, f'batch_{i:03d}.parquet')
                batch_data.to_parquet(batch_file, compression='snappy')
                batch_files.append(batch_file)

                print(f"ğŸ“ ë°°ì¹˜ {i}: {len(batch_data):,}í–‰ â†’ {batch_file}")

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del df
            import gc
            gc.collect()

            print(f"âœ… ë¶„í•  ì™„ë£Œ: {len(batch_files)}ê°œ ë°°ì¹˜ íŒŒì¼")
            return batch_files

        except Exception as e:
            print(f"âŒ ë¶„í•  ì‹¤íŒ¨: {e}")
            return []

    def step2_process_batches(self, batch_files, chunk_size=100000):
        """2ë‹¨ê³„: ê° ë°°ì¹˜ë¥¼ ê°œë³„ í”„ë¡œì„¸ìŠ¤ë¡œ ì²˜ë¦¬"""

        print(f"\nğŸ”„ 2ë‹¨ê³„: ë°°ì¹˜ë³„ ì²˜ë¦¬ ì‹œì‘ ({len(batch_files)}ê°œ)")

        processed_files = []

        for i, batch_file in enumerate(batch_files):
            print(f"\nğŸ“¦ ë°°ì¹˜ {i+1}/{len(batch_files)} ì²˜ë¦¬: {batch_file}")

            output_file = os.path.join(self.output_dir, f'processed_batch_{i:03d}.parquet')

            # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ì´ ìˆìœ¼ë©´ ìŠ¤í‚µ
            if os.path.exists(output_file):
                print(f"âœ… ì´ë¯¸ ì²˜ë¦¬ë¨: {output_file}")
                processed_files.append(output_file)
                continue

            # ê°œë³„ ë°°ì¹˜ ì²˜ë¦¬ (ìƒˆ í”„ë¡œì„¸ìŠ¤)
            cmd = [
                'python', 'run_preprocessing.py',
                '--data-path', batch_file,
                '--chunk-size', str(chunk_size),
                '--retrain'  # ê° ë°°ì¹˜ë§ˆë‹¤ ì „ì²˜ë¦¬ê¸° ì¬í•™ìŠµ
            ]

            print(f"ğŸš€ ì‹¤í–‰: {' '.join(cmd)}")

            try:
                # ìƒˆ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰ (ë©”ëª¨ë¦¬ ì™„ì „ ë¶„ë¦¬)
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)

                if result.returncode == 0:
                    print(f"âœ… ë°°ì¹˜ {i} ì²˜ë¦¬ ì™„ë£Œ")

                    # ì²˜ë¦¬ëœ ê²°ê³¼ë¥¼ ì§€ì •ëœ ìœ„ì¹˜ë¡œ ì´ë™
                    if os.path.exists('processed_data/X_train.parquet'):
                        # ê²°ê³¼ íŒŒì¼ë“¤ì„ ë°°ì¹˜ë³„ë¡œ ì´ë¦„ ë³€ê²½
                        import shutil
                        shutil.move('processed_data/X_train.parquet', output_file)
                        processed_files.append(output_file)

                else:
                    print(f"âŒ ë°°ì¹˜ {i} ì²˜ë¦¬ ì‹¤íŒ¨:")
                    print(result.stderr)

            except subprocess.TimeoutExpired:
                print(f"â° ë°°ì¹˜ {i} íƒ€ì„ì•„ì›ƒ")
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ {i} ì‹¤í–‰ ì˜¤ë¥˜: {e}")

        return processed_files

    def step3_combine_results(self, processed_files):
        """3ë‹¨ê³„: ì²˜ë¦¬ëœ ë°°ì¹˜ë“¤ì„ ìµœì¢… ê²°í•©"""

        print(f"\nğŸ”„ 3ë‹¨ê³„: ê²°ê³¼ ê²°í•© ({len(processed_files)}ê°œ íŒŒì¼)")

        if not processed_files:
            print("âŒ ì²˜ë¦¬ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False

        try:
            # ë°°ì¹˜ë³„ë¡œ ë¡œë“œí•˜ì—¬ ê²°í•©
            combined_data = []

            for file in tqdm(processed_files, desc="ë°°ì¹˜ ê²°í•©"):
                batch_data = pd.read_parquet(file)
                combined_data.append(batch_data)
                print(f"ğŸ“ ë¡œë“œ: {file} ({batch_data.shape})")

            # ìµœì¢… ê²°í•©
            final_data = pd.concat(combined_data, ignore_index=True)
            print(f"ğŸ“Š ìµœì¢… ë°ì´í„° í¬ê¸°: {final_data.shape}")

            # Train/Val ë¶„í• 
            feature_cols = [col for col in final_data.columns if col != 'clicked']
            X = final_data[feature_cols]
            y = final_data['clicked']

            from sklearn.model_selection import train_test_split
            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )

            # ìµœì¢… ì €ì¥
            X_train.to_parquet('processed_data/X_train_final.parquet')
            X_val.to_parquet('processed_data/X_val_final.parquet')
            y_train.to_parquet('processed_data/y_train_final.parquet')
            y_val.to_parquet('processed_data/y_val_final.parquet')

            print("âœ… ìµœì¢… ê²°ê³¼ ì €ì¥ ì™„ë£Œ!")
            print(f"ğŸ“Š í›ˆë ¨: {X_train.shape}, ê²€ì¦: {X_val.shape}")

            return True

        except Exception as e:
            print(f"âŒ ê²°í•© ì‹¤íŒ¨: {e}")
            return False

    def run_full_pipeline(self, input_file='data/train.parquet', batch_size=500000, chunk_size=100000):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""

        print("ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘!")
        print(f"ğŸ“‚ ì…ë ¥ íŒŒì¼: {input_file}")
        print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {batch_size:,}í–‰")
        print(f"ğŸ”§ ì²­í¬ í¬ê¸°: {chunk_size:,}í–‰")

        # 1ë‹¨ê³„: ë¶„í• 
        batch_files = self.step1_split_data(input_file, batch_size)
        if not batch_files:
            return False

        # 2ë‹¨ê³„: ë°°ì¹˜ë³„ ì²˜ë¦¬
        processed_files = self.step2_process_batches(batch_files, chunk_size)
        if not processed_files:
            return False

        # 3ë‹¨ê³„: ê²°í•©
        success = self.step3_combine_results(processed_files)

        if success:
            print("\nğŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
            print("ğŸ’¡ ì„ì‹œ íŒŒì¼ ì •ë¦¬ëŠ” ìˆ˜ë™ìœ¼ë¡œ í•˜ì„¸ìš”:")
            print(f"rm -rf {self.batch_dir}")

        return success

def main():
    processor = BatchPreprocessor()

    print("ğŸ“‹ ë°°ì¹˜ ì²˜ë¦¬ ì˜µì…˜:")
    print("1. ì†Œí˜• ë°°ì¹˜ (50ë§Œí–‰) - ì•ˆì „")
    print("2. ì¤‘í˜• ë°°ì¹˜ (100ë§Œí–‰) - ê· í˜•")
    print("3. ëŒ€í˜• ë°°ì¹˜ (200ë§Œí–‰) - ë¹ ë¦„")

    choice = input("ì„ íƒ (1-3): ").strip()

    batch_sizes = {'1': 500000, '2': 1000000, '3': 2000000}
    batch_size = batch_sizes.get(choice, 500000)

    success = processor.run_full_pipeline(
        input_file='data/train.parquet',
        batch_size=batch_size,
        chunk_size=100000
    )

    if success:
        print("âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„±ê³µ!")
    else:
        print("âŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨")

if __name__ == "__main__":
    main()