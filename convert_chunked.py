#!/usr/bin/env python3
"""
ì²­í¬ë³„ Parquet â†’ CSV ë³€í™˜ (ë©”ëª¨ë¦¬ ì•ˆì „)
"""

import pandas as pd
import os

def convert_parquet_to_csv_chunked():
    """Parquetì„ ì²­í¬ë³„ë¡œ CSVë¡œ ë³€í™˜"""

    parquet_path = 'data/train.parquet'
    csv_path = 'data/train.csv'

    if not os.path.exists(parquet_path):
        print(f"âŒ {parquet_path} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return False

    print("ğŸ”„ ì²­í¬ë³„ Parquet â†’ CSV ë³€í™˜ ì‹œì‘...")

    # ë¨¼ì € ì´ í–‰ ìˆ˜ í™•ì¸
    try:
        import pyarrow.parquet as pq
        parquet_file = pq.ParquetFile(parquet_path)
        total_rows = parquet_file.metadata.num_rows
        print(f"ğŸ“Š ì´ ë°ì´í„° í–‰ ìˆ˜: {total_rows:,}í–‰")
    except:
        print("âŒ PyArrowë¡œ ë©”íƒ€ë°ì´í„° ì½ê¸° ì‹¤íŒ¨")
        return False

    chunk_size = 500000  # 50ë§Œ í–‰ì”© ì²˜ë¦¬
    total_chunks = (total_rows // chunk_size) + 1

    print(f"ğŸ“¦ {total_chunks}ê°œ ì²­í¬ë¡œ ë¶„í•  ì²˜ë¦¬ (ì²­í¬ë‹¹ {chunk_size:,}í–‰)")

    # CSV íŒŒì¼ ì´ˆê¸°í™” (í—¤ë” ë¨¼ì € ì“°ê¸°)
    header_written = False

    try:
        for chunk_idx in range(total_chunks):
            start_row = chunk_idx * chunk_size
            end_row = min(start_row + chunk_size, total_rows)

            if start_row >= total_rows:
                break

            print(f"ğŸ”„ ì²­í¬ {chunk_idx+1}/{total_chunks}: {start_row:,} ~ {end_row:,}")

            # ì „ì²´ íŒŒì¼ ë¡œë“œ í›„ ì²­í¬ ì¶”ì¶œ
            full_df = pd.read_parquet(parquet_path)
            chunk_df = full_df.iloc[start_row:end_row].copy()
            del full_df

            # CSV ì“°ê¸°
            if not header_written:
                chunk_df.to_csv(csv_path, mode='w', index=False, header=True)
                header_written = True
            else:
                chunk_df.to_csv(csv_path, mode='a', index=False, header=False)

            del chunk_df
            import gc
            gc.collect()

            progress = ((chunk_idx + 1) / total_chunks) * 100
            print(f"âœ… ì²­í¬ {chunk_idx+1} ì™„ë£Œ ({progress:.1f}%)")

        print(f"âœ… ë³€í™˜ ì™„ë£Œ: {csv_path}")

        # íŒŒì¼ í¬ê¸° í™•ì¸
        parquet_size = os.path.getsize(parquet_path) / 1024 / 1024  # MB
        csv_size = os.path.getsize(csv_path) / 1024 / 1024  # MB

        print(f"ğŸ“ íŒŒì¼ í¬ê¸°:")
        print(f"  Parquet: {parquet_size:.1f}MB")
        print(f"  CSV: {csv_size:.1f}MB ({csv_size/parquet_size:.1f}ë°°)")

        return True

    except Exception as e:
        print(f"âŒ ë³€í™˜ ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    success = convert_parquet_to_csv_chunked()
    if success:
        print("\nğŸ‰ CSV ë³€í™˜ ì™„ë£Œ!")
        print("ì´ì œ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì‹¤í–‰í•˜ì„¸ìš”:")
        print("python run_preprocessing.py --data-path data/train.csv --resume --chunk-size 100000")
    else:
        print("âŒ ë³€í™˜ ì‹¤íŒ¨")