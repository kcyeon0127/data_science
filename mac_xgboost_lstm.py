#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Macìš© XGBoost ì „ìš© CTR ì˜ˆì¸¡ (TensorFlow ì—†ìŒ)
- ì„¸ê·¸í´íŠ¸ ë°©ì§€: MPS ì›Œí„°ë§ˆí¬, ë°°ì¹˜ ë‹¨ìœ„ ì´ë™, ë‹¨ë°©í–¥ GRU, ìƒ˜í”Œ í•™ìŠµâ†’ì „ëŸ‰ ì¶”ì¶œ
- ìë™ ë°°ì¹˜ ì¶•ì†Œ(Adaptive Batch): MPS OOM ë°œìƒ ì‹œ ì¦‰ì‹œ ë°°ì¹˜ ì¶•ì†Œ í›„ ì¬ì‹œë„
- ì‹œí€€ìŠ¤: GRU + ê°„ë‹¨ Attention ì„ë² ë”© ì¶”ì¶œ â†’ XGBoost ê²°í•©
- XGBoost: ëª¨ë“  í”¼ì²˜ë¥¼ float32/int32ë¡œ ê°•ì œ â†’ ì‹¤íŒ¨ ì‹œ QuantileDMatrix í´ë°±
"""

import os
import sys
import platform

# ======== MPS OOM ì‹œ ì„¸ê·¸í´íŠ¸ ëŒ€ì‹  ì˜ˆì™¸ë¡œ ì „í™˜ (í™˜ê²½ë³€ìˆ˜ ìš°ì„ ) ========
DEFAULT_WATERMARK = "0.5"  # ë””ë²„ê¹… ì‹œ "0.0", ê´€ëŒ€ ëª¨ë“œ "0.8"
if "PYTORCH_MPS_HIGH_WATERMARK_RATIO" not in os.environ:
    os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = DEFAULT_WATERMARK

# (ì„ íƒ) ê³¼ë„í•œ ìŠ¤ë ˆë“œ ì¦ê°€ ë°©ì§€
os.environ.setdefault("OMP_NUM_THREADS", "1")

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, average_precision_score
import xgboost as xgb
from tqdm.auto import tqdm
import gc
import time

# =========================
# ê³µìš© ìœ í‹¸
# =========================
def _ensure_numeric32(df: pd.DataFrame) -> pd.DataFrame:
    """XGBoost ì¹œí™”ì ìœ¼ë¡œ ëª¨ë“  ì»¬ëŸ¼ì„ float32/int32ë¡œ ê°•ì œ ë³€í™˜."""
    out = {}
    for c in df.columns:
        s = df[c]
        if pd.api.types.is_bool_dtype(s):
            out[c] = s.astype(np.uint8)
        elif pd.api.types.is_integer_dtype(s):
            # pandas nullable(Int64 ë“±)ë„ int32ë¡œ
            out[c] = pd.to_numeric(s, errors='coerce').fillna(0).astype(np.int32)
        elif pd.api.types.is_float_dtype(s):
            out[c] = s.astype(np.float32)
        elif pd.api.types.is_categorical_dtype(s):
            # category â†’ ì½”ë“œ(int32)
            out[c] = s.cat.codes.replace(-1, 0).astype(np.int32)
        else:
            # object/ArrowDtype/ê¸°íƒ€ â†’ ìˆ«ì ë³€í™˜ ì‹¤íŒ¨ëŠ” 0ìœ¼ë¡œ
            out[c] = pd.to_numeric(s, errors='coerce').fillna(0).astype(np.float32)
    return pd.DataFrame(out, index=df.index)

# =========================
# PyTorch (ì„ íƒ) - ì‹œí€€ìŠ¤ ì„ë² ë”© ì „ìš©
# =========================
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F

    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print(f"ğŸ Apple Silicon MPS ê°€ì† í™œì„±í™” (ì›Œí„°ë§ˆí¬={os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO']})")
    else:
        device = torch.device("cpu")
        print("ğŸ’» CPU ëª¨ë“œ")

    TORCH_AVAILABLE = True
    print("ğŸ§  PyTorch ë¡œë“œë¨ - GRU + Attention ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    TORCH_AVAILABLE = False
    device = None
    print("âš ï¸ PyTorch ì—†ìŒ - ì‹œí€€ìŠ¤ í†µê³„ íŠ¹ì„±ìœ¼ë¡œ ëŒ€ì²´")

# =========================
# í‰ê°€ ì§€í‘œ
# =========================
def calculate_weighted_logloss(y_true, y_pred, eps=1e-15):
    """ê°€ì¤‘ LogLoss (0/1 í´ë˜ìŠ¤ ê¸°ì—¬ 50:50)"""
    y_pred = np.clip(y_pred, eps, 1 - eps)
    mask_0 = (y_true == 0)
    mask_1 = (y_true == 1)
    ll_0 = -np.mean(np.log(1 - y_pred[mask_0])) if mask_0.sum() > 0 else 0.0
    ll_1 = -np.mean(np.log(y_pred[mask_1])) if mask_1.sum() > 0 else 0.0
    return 0.5 * ll_0 + 0.5 * ll_1

def calculate_competition_score(y_true, y_pred):
    """ëŒ€íšŒ í‰ê°€: 0.5*AP + 0.5*(1/(1+WLL))"""
    ap = average_precision_score(y_true, y_pred)
    wll = calculate_weighted_logloss(y_true, y_pred)
    score = 0.5 * ap + 0.5 * (1.0 / (1.0 + wll))
    return score, ap, wll

# =========================
# GRU + Attention (ë‹¨ë°©í–¥, ê²½ëŸ‰)
# =========================
if TORCH_AVAILABLE:
    class GRUAttentionModel(nn.Module):
        def __init__(self, vocab_size=50000, embedding_dim=64, hidden=96, output_dim=32):
            super().__init__()
            self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
            self.gru = nn.GRU(embedding_dim, hidden, batch_first=True)  # ë‹¨ë°©í–¥
            self.attn = nn.Linear(hidden, 1)
            self.fc1 = nn.Linear(hidden, 96)
            self.dropout = nn.Dropout(0.2)
            self.fc2 = nn.Linear(96, output_dim)

        def forward(self, x):  # x: (B, T)
            emb = self.embedding(x)                       # (B, T, E)
            out, _ = self.gru(emb)                        # (B, T, H)
            w = torch.softmax(self.attn(out), dim=1)      # (B, T, 1)
            pooled = torch.sum(out * w, dim=1)            # (B, H)
            x = torch.relu(self.fc1(pooled))
            x = self.dropout(x)
            x = torch.relu(self.fc2(x))                   # (B, 32)
            return x

def _is_mps_oom(err: Exception) -> bool:
    s = str(err).lower()
    return ("mps" in s and "memory" in s) or "out of memory" in s

class PyTorchSequenceProcessor:
    """ë°°ì¹˜ ë‹¨ìœ„ ë””ë°”ì´ìŠ¤ ì´ë™ + ìƒ˜í”Œ í•™ìŠµ â†’ ì „ëŸ‰ ì¶”ì¶œ + ìë™ ë°°ì¹˜ ì¶•ì†Œ"""
    def __init__(self, vocab_size=50000, embedding_dim=64, hidden=96,
                 output_dim=32, train_cap=100_000, batch_size=256, max_len=50):
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.hidden = hidden
        self.output_dim = output_dim
        self.train_cap = train_cap
        self.batch_size = batch_size
        self.max_len = max_len
        self.model = None
        self.device = device if TORCH_AVAILABLE else None

    def create_model(self):
        if not TORCH_AVAILABLE:
            return None
        self.model = GRUAttentionModel(
            vocab_size=self.vocab_size,
            embedding_dim=self.embedding_dim,
            hidden=self.hidden,
            output_dim=self.output_dim
        )
        if self.device:
            self.model = self.model.to(self.device)
        return self.model

    def _make_targets(self, batch_np):
        """ê°„ë‹¨ ìê¸°ì§€ë„ í‘œì : ë¹„ì˜(0 ì œì™¸) í‰ê· ê°’ì„ 32ì°¨ë¡œ broadcast"""
        nz_mean = np.where(batch_np != 0, batch_np, np.nan).astype(np.float32)
        means = np.nanmean(nz_mean, axis=1)
        means = np.nan_to_num(means, nan=0.0).astype(np.float32)
        return torch.tensor(np.repeat(means[:, None], self.output_dim, axis=1),
                            dtype=torch.float32)

    def _safe_train_step(self, batch_np, optimizer, criterion):
        x = torch.tensor(batch_np, dtype=torch.long, device=self.device)
        y = self._make_targets(batch_np).to(self.device)
        optimizer.zero_grad(set_to_none=True)
        out = self.model(x)
        loss = criterion(out, y)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        optimizer.step()
        return loss.item()

    def _safe_infer_step(self, batch_np):
        with torch.no_grad():
            x = torch.tensor(batch_np, dtype=torch.long, device=self.device)
            out = self.model(x)
            return out.detach().cpu().numpy()

    def train_model(self, sequences, epochs=2):
        """ëŒ€ìš©ëŸ‰ì¼ ë•Œ ìƒ˜í”Œë§Œìœ¼ë¡œ í‘œí˜„ í•™ìŠµ + OOM ì‹œ ë°°ì¹˜ ìë™ ì¶•ì†Œ"""
        if (not TORCH_AVAILABLE) or (self.model is None):
            return

        n = len(sequences)
        use_n = min(n, self.train_cap)
        rng = np.random.RandomState(42)
        idx = rng.choice(n, use_n, replace=False)
        train_seqs = sequences[idx]

        print(f"ğŸš€ PyTorch ì‹œí€€ìŠ¤ ëª¨ë¸ í›ˆë ¨: ì‚¬ìš© ìƒ˜í”Œ {use_n:,}/{n:,} (epochs={epochs}, max_len={self.max_len})")
        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)
        criterion = nn.MSELoss()

        for ep in range(epochs):
            total, steps = 0.0, 0
            i = 0
            cur_bs = self.batch_size
            while i < use_n:
                ok = False
                # ìµœëŒ€ 3íšŒê¹Œì§€ ë°°ì¹˜ ì¶•ì†Œ ì‹œë„
                for _ in range(3):
                    batch_np = train_seqs[i:i+cur_bs]
                    try:
                        loss_val = self._safe_train_step(batch_np, optimizer, criterion)
                        total += loss_val; steps += 1
                        i += len(batch_np)
                        ok = True
                        break
                    except RuntimeError as e:
                        if _is_mps_oom(e):
                            if torch.backends.mps.is_available():
                                torch.mps.empty_cache()
                            new_bs = max(cur_bs // 2, 32)
                            if new_bs == cur_bs:
                                raise
                            print(f"âš ï¸ MPS OOM ê°ì§€ â†’ ë°°ì¹˜ {cur_bs} â†’ {new_bs} ì¶•ì†Œ")
                            cur_bs = new_bs
                            continue
                        else:
                            raise
                if not ok:
                    print("âŒ ë°°ì¹˜ 32ì—ì„œë„ OOM â†’ hidden/max_len/ì›Œí„°ë§ˆí¬ ì¡°ì • í•„ìš”")
                    break

                if (steps % 50 == 0) and TORCH_AVAILABLE and torch.backends.mps.is_available():
                    torch.mps.empty_cache()

            print(f"   Epoch {ep+1}: Loss={total/max(steps,1):.4f} (ìµœì¢… ë°°ì¹˜={cur_bs})")

    def extract_features(self, sequences):
        """ì „ëŸ‰ ë°°ì¹˜ ì¶”ë¡ (ë°°ì¹˜ë§Œ ë””ë°”ì´ìŠ¤ ì´ë™) ë˜ëŠ” í†µê³„ í”¼ì²˜ í´ë°± + OOM ìë™ ì¶•ì†Œ"""
        if (not TORCH_AVAILABLE) or (self.model is None):
            return self.extract_statistical_features(sequences)

        self.model.eval()
        feats = []
        i = 0
        n = len(sequences)
        cur_bs = self.batch_size
        with torch.no_grad():
            while i < n:
                ok = False
                for _ in range(3):
                    batch_np = sequences[i:i+cur_bs]
                    try:
                        out = self._safe_infer_step(batch_np)
                        feats.append(out)
                        i += len(batch_np)
                        ok = True
                        break
                    except RuntimeError as e:
                        if _is_mps_oom(e):
                            if torch.backends.mps.is_available():
                                torch.mps.empty_cache()
                            new_bs = max(cur_bs // 2, 32)
                            if new_bs == cur_bs:
                                raise
                            print(f"âš ï¸ MPS OOM(ì¶”ë¡ ) â†’ ë°°ì¹˜ {cur_bs} â†’ {new_bs} ì¶•ì†Œ")
                            cur_bs = new_bs
                            continue
                        else:
                            raise
                if not ok:
                    print("âŒ (ì¶”ë¡ ) ë°°ì¹˜ 32ì—ì„œë„ OOM â†’ hidden/max_len/ì›Œí„°ë§ˆí¬ ì¡°ì • í•„ìš”")
                    break

                if TORCH_AVAILABLE and torch.backends.mps.is_available() and (i // cur_bs) % 50 == 0:
                    torch.mps.empty_cache()

        return np.concatenate(feats, axis=0).astype(np.float32)

    def preprocess_sequences(self, df, seq_col='seq', max_len=None):
        """ë¬¸ìì—´ '1,2,3' â†’ ê¸¸ì´ max_lenì˜ int32 ì‹œí€€ìŠ¤(íŒ¨ë”©=0)"""
        if max_len is None:
            max_len = self.max_len
        sequences = []
        for seq_str in tqdm(df[seq_col], desc="ì‹œí€€ìŠ¤ ì „ì²˜ë¦¬"):
            if pd.isna(seq_str) or seq_str == '':
                seq = [0]*max_len
            else:
                try:
                    seq = [int(x) for x in str(seq_str).split(',') if x != '']
                    seq = (seq[:max_len] if len(seq) >= max_len
                           else seq + [0]*(max_len-len(seq)))
                except Exception:
                    seq = [0]*max_len
            sequences.append(seq)
        return np.asarray(sequences, dtype=np.int32)

    def extract_statistical_features(self, sequences):
        """ê°„ë‹¨ í†µê³„ í”¼ì²˜(ê¸¸ì´, í‰ê· , í‘œì¤€í¸ì°¨, min/max, median, ìœ ë‹ˆí¬ ìˆ˜, ë§ˆì§€ë§‰, ìµœê·¼5í‰ê· )"""
        features = []
        for seq in sequences:
            nz = seq[seq != 0]
            if len(nz) == 0:
                feat = [0]*32
            else:
                feat = [
                    int(len(nz)),
                    float(np.mean(nz)),
                    float(np.std(nz)),
                    int(np.min(nz)),
                    int(np.max(nz)),
                    float(np.median(nz)),
                    int(len(np.unique(nz))),
                    int(nz[-1]),
                    float(np.mean(nz[-5:])) if len(nz) >= 5 else float(np.mean(nz)),
                ]
                feat = (feat + [0]*(32-len(feat)))[:32]
            features.append(feat)
        return np.asarray(features, dtype=np.float32)

print("ğŸš€ Macìš© XGBoost + (ì„ íƒ) GRU Attention ê¸°ë°˜ ì‹œí€€ìŠ¤ ì„ë² ë”© CTR ì˜ˆì¸¡!")
print("ğŸ“Š ëŒ€íšŒ í‰ê°€ì§€í‘œ: 0.5*AP + 0.5*(1/(1+WLL))")
print("=" * 68)

# =========================
# íŒŒì´í”„ë¼ì¸
# =========================
class MacXGBoostCTR:
    def __init__(self):
        self.models = {}
        self.encoders = {}
        self.feature_cols = []
        self.sequence_model = None
        print("ğŸ Macìš© XGBoost + GRU Attention CTR ì´ˆê¸°í™” ì™„ë£Œ")

    # ---------- ë°ì´í„° ë¡œë”© ----------
    def load_data_efficiently(self, sample_ratio=0.3, use_batch=False):
        print("\nğŸ“‚ ë°ì´í„° ë¡œë”© ì‹œì‘...")
        try:
            train_size_gb = os.path.getsize('data/train.parquet') / (1024**3)
            print(f"ğŸ“Š í›ˆë ¨ ë°ì´í„° í¬ê¸°: {train_size_gb:.1f} GB")
        except Exception:
            print("ğŸ“Š í›ˆë ¨ ë°ì´í„° í¬ê¸° í™•ì¸ ì‹¤íŒ¨")

        if sample_ratio < 1.0:
            print(f"ğŸ”„ ìƒ˜í”Œë§ ëª¨ë“œ - {sample_ratio*100:.0f}% ë°ì´í„° ì‚¬ìš©")
            full_df = pd.read_parquet('data/train.parquet')
            clicked = full_df[full_df['clicked'] == 1]
            not_clicked = full_df[full_df['clicked'] == 0]

            n_clicked = int(len(clicked) * sample_ratio)
            n_not_clicked = int(len(not_clicked) * sample_ratio)

            print(f"ìƒ˜í”Œë§: í´ë¦­ {len(clicked):,} â†’ {n_clicked:,}, ë¹„í´ë¦­ {len(not_clicked):,} â†’ {n_not_clicked:,}")
            sample_clicked = clicked.sample(min(len(clicked), n_clicked), random_state=42) if n_clicked > 0 else pd.DataFrame()
            sample_not_clicked = not_clicked.sample(min(len(not_clicked), n_not_clicked), random_state=42) if n_not_clicked > 0 else pd.DataFrame()
            self.train_df = pd.concat([sample_clicked, sample_not_clicked], ignore_index=True)

            del full_df, clicked, not_clicked, sample_clicked, sample_not_clicked
            gc.collect()

        elif use_batch:
            print("ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ - ì „ì²´ ë°ì´í„°ë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë”©")
            self.train_df = self.load_data_in_batches('data/train.parquet')
        else:
            print("ğŸ“‚ ì „ì²´ ë°ì´í„° ì§ì ‘ ë¡œë”© ì¤‘...")
            try:
                self.train_df = pd.read_parquet('data/train.parquet')
                print("âœ… ì§ì ‘ ë¡œë”© ì„±ê³µ")
            except MemoryError:
                print("âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±! ë°°ì¹˜ ëª¨ë“œë¡œ ì „í™˜...")
                self.train_df = self.load_data_in_batches('data/train.parquet')

        print("ğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©...")
        self.test_df = pd.read_parquet('data/test.parquet')

        print(f"\nâœ… ë¡œë”© ì™„ë£Œ!")
        print(f"   í›ˆë ¨: {self.train_df.shape}")
        print(f"   í…ŒìŠ¤íŠ¸: {self.test_df.shape}")
        if 'clicked' in self.train_df.columns:
            print(f"   í´ë¦­ë¥ : {self.train_df['clicked'].mean():.4f}")

        return True

    def load_data_in_batches(self, file_path, batch_size=500_000):
        print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸° {batch_size:,}í–‰ìœ¼ë¡œ ì•ˆì „ ë¡œë”©...")
        try:
            print("   ì „ì²´ ë¡œë“œ ì‹œë„ ì¤‘...")
            full_df = pd.read_parquet(file_path)
            print(f"âœ… ì „ì²´ ë¡œë“œ ì„±ê³µ: {full_df.shape}")
            return full_df
        except MemoryError:
            print("   ë©”ëª¨ë¦¬ ë¶€ì¡±! 70% ìƒ˜í”Œë§ìœ¼ë¡œ ëŒ€ì²´...")
            full_df = pd.read_parquet(file_path)
            clicked = full_df[full_df['clicked'] == 1]
            not_clicked = full_df[full_df['clicked'] == 0]

            sample_ratio = 0.7
            n_clicked = int(len(clicked) * sample_ratio)
            n_not_clicked = int(len(not_clicked) * sample_ratio)

            sample_clicked = clicked.sample(min(len(clicked), n_clicked), random_state=42)
            sample_not_clicked = not_clicked.sample(min(len(not_clicked), n_not_clicked), random_state=42)
            result_df = pd.concat([sample_clicked, sample_not_clicked], ignore_index=True)

            del full_df, clicked, not_clicked, sample_clicked, sample_not_clicked
            gc.collect()
            print(f"âœ… ìƒ˜í”Œë§ ë¡œë“œ ì™„ë£Œ: {result_df.shape}")
            return result_df
        except Exception as e:
            print(f"âŒ ë¡œë”© ì‹¤íŒ¨: {e}")
            print("   30% ìƒ˜í”Œë§ìœ¼ë¡œ ì¬ì‹œë„...")
            full_df = pd.read_parquet(file_path)
            return full_df.sample(frac=0.3, random_state=42)

    # ---------- ì „ì²˜ë¦¬ ----------
    def preprocess_features(self):
        print("\nğŸ”§ íŠ¹ì„± ì „ì²˜ë¦¬ ì‹œì‘...")

        # ìˆ˜ì¹˜í˜•
        numeric_cols = [c for c in self.train_df.columns if c.startswith(('feat_', 'history_', 'l_feat_'))]

        # ì¹´í…Œê³ ë¦¬
        categorical_cols = ['gender', 'age_group']
        if 'inventory_id' in self.train_df.columns:
            categorical_cols.append('inventory_id')

        # ì‹œí€€ìŠ¤
        has_sequence = 'seq' in self.train_df.columns

        print(f"ğŸ“Š íŠ¹ì„± ì •ë³´: ìˆ˜ì¹˜í˜• {len(numeric_cols)}ê°œ | ì¹´í…Œê³ ë¦¬ {len(categorical_cols)}ê°œ | ì‹œí€€ìŠ¤ {'ìˆìŒ' if has_sequence else 'ì—†ìŒ'}")

        # ìˆ˜ì¹˜í˜• ê²°ì¸¡ ëŒ€ì²´(í‰ê· )
        print("ğŸ”§ ìˆ˜ì¹˜í˜• íŠ¹ì„± ì²˜ë¦¬...")
        for col in tqdm(numeric_cols, desc="ìˆ˜ì¹˜í˜•"):
            if col in self.train_df.columns:
                mean_val = self.train_df[col].fillna(0).mean()
                self.train_df[col] = self.train_df[col].fillna(mean_val)
                self.test_df[col] = self.test_df[col].fillna(mean_val)

        # ì¹´í…Œê³ ë¦¬ ë¼ë²¨ì¸ì½”ë”©(Train/Test í•©ì³ì„œ fit)
        print("ğŸ”§ ì¹´í…Œê³ ë¦¬ íŠ¹ì„± ì²˜ë¦¬...")
        for col in tqdm(categorical_cols, desc="ì¹´í…Œê³ ë¦¬"):
            if col in self.train_df.columns:
                le = LabelEncoder()
                combined = pd.concat([
                    self.train_df[col].fillna('unknown').astype(str),
                    self.test_df[col].fillna('unknown').astype(str)
                ])
                le.fit(combined)
                self.train_df[col] = le.transform(self.train_df[col].fillna('unknown').astype(str))
                self.test_df[col]  = le.transform(self.test_df[col].fillna('unknown').astype(str))
                self.encoders[col] = le

        # ì‹œí€€ìŠ¤ ì„ë² ë”© ì¶”ì¶œ â†’ ìˆ˜ì¹˜ í”¼ì²˜ë¡œ ì¶”ê°€
        if has_sequence:
            print("ğŸ§  ì‹œí€€ìŠ¤ íŠ¹ì„± ì²˜ë¦¬ (PyTorch GRU + Attention)...")
            self.sequence_model = PyTorchSequenceProcessor(
                vocab_size=50_000, embedding_dim=64, hidden=96, output_dim=32,
                train_cap=100_000, batch_size=256, max_len=50
            )

            train_sequences = self.sequence_model.preprocess_sequences(self.train_df, 'seq')
            test_sequences  = self.sequence_model.preprocess_sequences(self.test_df , 'seq')

            if TORCH_AVAILABLE:
                print("ğŸ”§ PyTorch GRU + Attention ëª¨ë¸ ìƒì„±...")
                self.sequence_model.create_model()
                try:
                    self.sequence_model.train_model(train_sequences, epochs=2)  # ìƒ˜í”Œ í•™ìŠµ
                except Exception as e:
                    print(f"âŒ ì‹œí€€ìŠ¤ í•™ìŠµ ì‹¤íŒ¨ â†’ í†µê³„ íŠ¹ì„±ìœ¼ë¡œ ëŒ€ì²´: {e}")
                    self.sequence_model.model = None
            else:
                print("âš ï¸ PyTorch ì—†ìŒ - í†µê³„ íŠ¹ì„±ìœ¼ë¡œ ëŒ€ì²´")

            print("ğŸ” ì‹œí€€ìŠ¤ íŠ¹ì„± ì¶”ì¶œ...")
            train_seq_features = self.sequence_model.extract_features(train_sequences)
            test_seq_features  = self.sequence_model.extract_features(test_sequences)
            del train_sequences, test_sequences; gc.collect()

            # ì»¬ëŸ¼ ê²°í•©
            seq_feature_names = [f'seq_feat_{i}' for i in range(train_seq_features.shape[1])]
            for i, name in enumerate(seq_feature_names):
                self.train_df[name] = train_seq_features[:, i]
                self.test_df[name]  = test_seq_features[:, i]
            numeric_cols.extend(seq_feature_names)
            print(f"âœ… ì‹œí€€ìŠ¤ íŠ¹ì„± {len(seq_feature_names)}ê°œ ì¶”ê°€")

        # ìµœì¢… ì‚¬ìš© í”¼ì²˜
        self.feature_cols = [c for c in (numeric_cols + categorical_cols) if c in self.train_df.columns]
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: ì´ {len(self.feature_cols)}ê°œ íŠ¹ì„± ì‚¬ìš©")
        return True

    # ---------- XGBoost í•™ìŠµ (ìˆ˜ì •íŒ: dtype ê°•ì œ + QuantileDMatrix í´ë°±) ----------
    def train_xgboost_models(self):
        print("\nğŸš€ XGBoost ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")

        # 1) ì•ˆì „í•œ dtypeìœ¼ë¡œ ê°•ì œ ë³€í™˜(ë©”ëª¨ë¦¬ ì ˆì•½ë„ ë¨)
        X = _ensure_numeric32(self.train_df[self.feature_cols].copy())
        y = self.train_df['clicked'].astype(np.int32).values

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê·¸
        feat_mem_gb = X.memory_usage(deep=True).sum() / (1024**3)
        print(f"ğŸ§® íŠ¹ì„± í…Œì´ë¸” ë©”ëª¨ë¦¬(í›ˆë ¨ ì „ì²´): {feat_mem_gb:.2f} GB")

        pos_ratio = float(np.mean(y))
        scale_pos_weight = (1 - pos_ratio) / max(pos_ratio, 1e-8)
        print(f"ğŸ“Š í´ë¦­ë¥ : {pos_ratio:.4f}")
        print(f"ğŸ“Š Scale pos weight: {scale_pos_weight:.2f}")

        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        print(f"ğŸ“Š ë°ì´í„° ë¶„í• : í›ˆë ¨ {X_train.shape[0]:,}, ê²€ì¦ {X_val.shape[0]:,}")
        print(f"ğŸ“Š í´ë¦­ë¥  ë¶„í¬ - ì „ì²´ {pos_ratio:.4f} | í›ˆë ¨ {y_train.mean():.4f} | ê²€ì¦ {y_val.mean():.4f}")

        # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ float32 ë³´ì¥
        X_train = X_train.astype(np.float32, copy=False)
        X_val   = X_val.astype(np.float32,   copy=False)

        base_cfgs = {
            'xgb_ap_focused': dict(
                objective='binary:logistic',
                eval_metric=['auc', 'aucpr'],
                tree_method='hist',
                max_depth=6,
                learning_rate=0.08,
                n_estimators=600,
                subsample=0.85,
                colsample_bytree=0.85,
                scale_pos_weight=float(scale_pos_weight),
                max_bin=256,                 # ë©”ëª¨ë¦¬ ê°ì†Œ
                n_jobs=-1,
                random_state=42,
                verbosity=0
            ),
            'xgb_balanced': dict(
                objective='binary:logistic',
                eval_metric=['logloss', 'aucpr'],
                tree_method='hist',
                max_depth=7,
                learning_rate=0.06,
                n_estimators=800,
                subsample=0.9,
                colsample_bytree=0.9,
                scale_pos_weight=float(scale_pos_weight),
                reg_alpha=0.1,
                reg_lambda=0.1,
                max_bin=256,                 # ë©”ëª¨ë¦¬ ê°ì†Œ
                n_jobs=-1,
                random_state=42,
                verbosity=0
            )
        }

        self.models = {}

        # ---- 2ë‹¨ê³„: XGBClassifier â†’ ì‹¤íŒ¨ ì‹œ QuantileDMatrix í´ë°± ----
        for name, params in base_cfgs.items():
            print(f"\nğŸ”„ {name} í›ˆë ¨ ì¤‘...")
            try:
                model = xgb.XGBClassifier(**params)
                model.fit(
                    X_train, y_train,
                    eval_set=[(X_val, y_val)],
                    verbose=20,
                    early_stopping_rounds=20
                )

                val_pred = model.predict_proba(X_val)[:, 1]
                comp_score, ap, wll = calculate_competition_score(y_val, val_pred)
                auc = roc_auc_score(y_val, val_pred)
                best_iter = getattr(model, 'best_iteration', None)
                if best_iter is None:
                    best_iter = getattr(model, 'best_ntree_limit', None)
                print(f"   âœ… Best iteration: {best_iter}")
                print(f"   ğŸ“Š {name} ì„±ëŠ¥: ëŒ€íšŒ {comp_score:.4f} | AP {ap:.4f} | WLL {wll:.4f} | AUC {auc:.4f}")

                self.models[name] = [model]  # ê·¸ëŒ€ë¡œ ì‚¬ìš©
                continue

            except Exception as e:
                print(f"âš ï¸ {name} XGBClassifier í•™ìŠµ ì‹¤íŒ¨: {e}")
                print("   â†’ ë©”ëª¨ë¦¬ ì ˆì•½í˜• QuantileDMatrixë¡œ í´ë°±í•©ë‹ˆë‹¤.")

            # ===== í´ë°± ê²½ë¡œ: QuantileDMatrix + xgb.train =====
            try:
                try:
                    QDM = xgb.QuantileDMatrix    # 1.6+ ì—ì„œ ì§€ì›
                except AttributeError:
                    QDM = xgb.DMatrix            # êµ¬ë²„ì „: ì¼ë°˜ DMatrixë¡œ ëŒ€ì²´

                dtrain = QDM(X_train, label=y_train)
                dval   = QDM(X_val,   label=y_val)

                num_boost_round = params.get('n_estimators', 600)
                train_params = params.copy()
                train_params.pop('n_estimators', None)

                booster = xgb.train(
                    train_params,
                    dtrain,
                    num_boost_round=num_boost_round,
                    evals=[(dval, 'val')],
                    early_stopping_rounds=20,
                    verbose_eval=20
                )

                # ì˜ˆì¸¡ìš© ë˜í¼( predict_proba ì¸í„°í˜ì´ìŠ¤ í†µì¼ )
                class _BoosterWrapper:
                    def __init__(self, booster, feature_cols):
                        self.booster = booster
                        self.feature_cols = feature_cols
                        # best_iteration ì†ì„± í´ë°± ì²˜ë¦¬
                        self.best_iteration = getattr(booster, 'best_iteration', None)
                        self.best_ntree_limit = getattr(booster, 'best_ntree_limit', None)

                    def predict_proba(self, Xdf):
                        Xdf = _ensure_numeric32(Xdf[self.feature_cols])
                        dm = xgb.DMatrix(Xdf.values)
                        if self.best_iteration is not None:
                            preds = self.booster.predict(dm, iteration_range=(0, self.best_iteration + 1))
                        elif self.best_ntree_limit is not None:
                            preds = self.booster.predict(dm, ntree_limit=self.best_ntree_limit)
                        else:
                            preds = self.booster.predict(dm)
                        return np.vstack([1 - preds, preds]).T

                wrapper = _BoosterWrapper(booster, self.feature_cols)

                val_pred = wrapper.predict_proba(X_val)[:, 1]
                comp_score, ap, wll = calculate_competition_score(y_val, val_pred)
                auc = roc_auc_score(y_val, val_pred)
                print(f"   âœ… (í´ë°±) {name} ì„±ëŠ¥: ëŒ€íšŒ {comp_score:.4f} | AP {ap:.4f} | WLL {wll:.4f} | AUC {auc:.4f}")

                self.models[name] = [wrapper]

            except Exception as e2:
                print(f"âŒ {name} í´ë°± í•™ìŠµë„ ì‹¤íŒ¨: {e2}")
                print("   â†’ íŒŒë¼ë¯¸í„°ë¥¼ ë” ë³´ìˆ˜ì ìœ¼ë¡œ ë‚®ì¶”ê±°ë‚˜( max_depthâ†“, max_binâ†“, n_estimatorsâ†“ ) ìƒ˜í”Œë§ ë¹„ìœ¨ì„ ë†’ì—¬ì£¼ì„¸ìš”.")

        print("âœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")

    # ---------- ì˜ˆì¸¡ & ì œì¶œ ----------
    def predict_and_submit(self):
        print("\nğŸ¯ ì˜ˆì¸¡ ì‹œì‘...")
        X_test = self.test_df[self.feature_cols].copy()
        all_predictions = []

        for name, fold_models in self.models.items():
            print(f"ğŸ”„ {name} ì˜ˆì¸¡ ì¤‘...")
            fold_preds = []
            for i, model in enumerate(fold_models):
                pred = model.predict_proba(X_test)[:, 1]
                fold_preds.append(pred)
                print(f"   Fold {i+1} ì™„ë£Œ")
            avg_pred = np.mean(fold_preds, axis=0)
            all_predictions.append(avg_pred)

        final_predictions = np.mean(all_predictions, axis=0)

        # ì œì¶œ íŒŒì¼ ìƒì„±
        try:
            submission = pd.read_csv('data/sample_submission.csv')
            submission['clicked'] = final_predictions
            print(f"âœ… ì˜¬ë°”ë¥¸ ID í˜•ì‹ ì‚¬ìš©: {submission.columns.tolist()}")
        except Exception:
            submission = pd.DataFrame({
                'ID': [f'TEST_{i:07d}' for i in range(len(final_predictions))],
                'clicked': final_predictions
            })
            print("âš ï¸ sample_submission.csv ì—†ìŒ â†’ ID ì§ì ‘ ìƒì„±")

        submission_path = 'submission_mac_xgboost_gru.csv'
        submission.to_csv(submission_path, index=False, encoding='utf-8')

        print(f"\nâœ… ì œì¶œ íŒŒì¼ ìƒì„±: {submission_path}")
        print(f"ğŸ“Š ì˜ˆì¸¡ í†µê³„: mean={final_predictions.mean():.4f} | min={final_predictions.min():.4f} | max={final_predictions.max():.4f}")
        return submission_path

    # ---------- ì „ì²´ íŒŒì´í”„ë¼ì¸ ----------
    def run_pipeline(self, sample_ratio=0.3, use_batch=False):
        start_time = time.time()

        if not self.load_data_efficiently(sample_ratio, use_batch):
            return False
        if not self.preprocess_features():
            return False

        self.train_xgboost_models()
        submission_path = self.predict_and_submit()

        elapsed = time.time() - start_time
        print("\n" + "ğŸ‰"*60)
        print("ğŸ‰ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! ğŸ‰")
        print("ğŸ‰"*60)
        print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ ({elapsed/60:.1f}ë¶„)")
        print(f"ğŸ“ ì œì¶œ íŒŒì¼: {submission_path}")
        return True

# =========================
# ì‹¤í–‰ë¶€
# =========================
def main():
    pipeline = MacXGBoostCTR()

    print("\nğŸ“‹ ì‹¤í–‰ ì˜µì…˜:")
    print("1. ğŸš€ ì´ˆê³ ì† ëª¨ë“œ (30% ìƒ˜í”Œë§)")
    print("2. âš¡ ë¹ ë¥¸ ëª¨ë“œ (50% ìƒ˜í”Œë§)")
    print("3. ğŸ¯ ì •í™• ëª¨ë“œ (70% ìƒ˜í”Œë§)")
    print("4. ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë“œ (ì „ì²´ ì§ì ‘ ë¡œë”©) âš ï¸ ë©”ëª¨ë¦¬ ìœ„í—˜")
    print("5. ğŸ›¡ï¸ ì•ˆì „ ìµœê³  ì„±ëŠ¥ (ì „ì²´ ë°°ì¹˜ ë¡œë”©) âœ… ë©”ëª¨ë¦¬ ì•ˆì „")

    choice = input("ì„ íƒ (1-5, ê¸°ë³¸ê°’ 1): ").strip() or '1'

    if choice == '5':
        sample_ratio = 1.0
        use_batch = True
        print(f"\nğŸ›¡ï¸ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì „ì²´ ë°ì´í„° ì•ˆì „ ë¡œë”©!")
    elif choice == '4':
        sample_ratio = 1.0
        use_batch = False
        print(f"\nğŸ† ì „ì²´ ë°ì´í„° ì§ì ‘ ë¡œë”© (ë©”ëª¨ë¦¬ ìœ„í—˜ ê°ìˆ˜)!")
    else:
        sample_ratios = {'1': 0.3, '2': 0.5, '3': 0.7}
        sample_ratio = sample_ratios.get(choice, 0.3)
        use_batch = False
        print(f"\nğŸš€ {sample_ratio*100:.0f}% ë°ì´í„°ë¡œ ì‹¤í–‰ ì‹œì‘!")

    print("=" * 68)
    success = pipeline.run_pipeline(sample_ratio, use_batch)
    print("\nğŸŠ ì„±ê³µ! ì œì¶œ íŒŒì¼ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!" if success else "\nâŒ ì‹¤í–‰ ì‹¤íŒ¨")

if __name__ == "__main__":
    main()
