#!/usr/bin/env python3
"""
Macìš© XGBoost ì „ìš© CTR ì˜ˆì¸¡ (TensorFlow ì—†ìŒ)
ì´ˆê³ ì† ì‹¤í–‰ + ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© í‘œì‹œ
"""

import os
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import roc_auc_score, average_precision_score, log_loss
import xgboost as xgb
from tqdm.auto import tqdm
import gc
import time

def calculate_weighted_logloss(y_true, y_pred, eps=1e-15):
    """ê°€ì¤‘ LogLoss ê³„ì‚° (50:50 í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜)"""
    y_pred = np.clip(y_pred, eps, 1 - eps)

    mask_0 = (y_true == 0)
    mask_1 = (y_true == 1)

    ll_0 = -np.mean(np.log(1 - y_pred[mask_0])) if mask_0.sum() > 0 else 0
    ll_1 = -np.mean(np.log(y_pred[mask_1])) if mask_1.sum() > 0 else 0

    return 0.5 * ll_0 + 0.5 * ll_1

def calculate_competition_score(y_true, y_pred):
    """ëŒ€íšŒ í‰ê°€ ì§€í‘œ: 0.5*AP + 0.5*(1/(1+WLL))"""
    ap = average_precision_score(y_true, y_pred)
    wll = calculate_weighted_logloss(y_true, y_pred)
    score = 0.5 * ap + 0.5 * (1 / (1 + wll))
    return score, ap, wll

print("ğŸš€ Macìš© XGBoost ì „ìš© CTR ì˜ˆì¸¡ ì‹œì‘!")
print("ğŸ“Š ëŒ€íšŒ í‰ê°€ì§€í‘œ: 0.5*AP + 0.5*(1/(1+WLL))")
print("=" * 60)

class MacXGBoostCTR:
    def __init__(self):
        self.models = {}
        self.encoders = {}
        self.feature_cols = []
        print("ğŸ Macìš© XGBoost CTR ì´ˆê¸°í™” ì™„ë£Œ")

    def load_data_efficiently(self, sample_ratio=0.3, use_batch=False):
        """íš¨ìœ¨ì  ë°ì´í„° ë¡œë”© (ë°°ì¹˜ ì²˜ë¦¬ ì˜µì…˜ í¬í•¨)"""
        print("\nğŸ“‚ ë°ì´í„° ë¡œë”© ì‹œì‘...")

        train_size = os.path.getsize('data/train.parquet') / (1024**3)
        print(f"ğŸ“Š í›ˆë ¨ ë°ì´í„° í¬ê¸°: {train_size:.1f}GB")

        if sample_ratio < 1.0:
            # ìƒ˜í”Œë§ ëª¨ë“œ
            print(f"ğŸ”„ ìƒ˜í”Œë§ ëª¨ë“œ - {sample_ratio*100:.0f}% ë°ì´í„° ì‚¬ìš©")

            # ì „ì²´ ë°ì´í„°ë¥¼ í•œë²ˆì— ë¡œë“œí•œ í›„ ìƒ˜í”Œë§
            print("ğŸ“¦ ì „ì²´ ë°ì´í„° ë¡œë“œ í›„ ìƒ˜í”Œë§...")
            full_df = pd.read_parquet('data/train.parquet')

            # ê· í˜• ìƒ˜í”Œë§
            clicked = full_df[full_df['clicked'] == 1]
            not_clicked = full_df[full_df['clicked'] == 0]

            n_clicked = int(len(clicked) * sample_ratio)
            n_not_clicked = int(len(not_clicked) * sample_ratio)

            print(f"ìƒ˜í”Œë§: í´ë¦­ {len(clicked):,} â†’ {n_clicked:,}, ë¹„í´ë¦­ {len(not_clicked):,} â†’ {n_not_clicked:,}")

            sample_clicked = clicked.sample(min(len(clicked), n_clicked), random_state=42) if n_clicked > 0 else pd.DataFrame()
            sample_not_clicked = not_clicked.sample(min(len(not_clicked), n_not_clicked), random_state=42) if n_not_clicked > 0 else pd.DataFrame()

            self.train_df = pd.concat([sample_clicked, sample_not_clicked], ignore_index=True)

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del full_df, clicked, not_clicked, sample_clicked, sample_not_clicked
            gc.collect()

        elif use_batch:
            # ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ (ì „ì²´ ë°ì´í„° + ë©”ëª¨ë¦¬ ì•ˆì „)
            print("ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ - ì „ì²´ ë°ì´í„°ë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë”©")
            self.train_df = self.load_data_in_batches('data/train.parquet')

        else:
            # ì§ì ‘ ë¡œë“œ ëª¨ë“œ (ìœ„í—˜í•˜ì§€ë§Œ ë¹ ë¦„)
            print("ğŸ“‚ ì „ì²´ ë°ì´í„° ì§ì ‘ ë¡œë”© ì¤‘...")
            try:
                self.train_df = pd.read_parquet('data/train.parquet')
                print("âœ… ì§ì ‘ ë¡œë”© ì„±ê³µ")
            except MemoryError:
                print("âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±! ë°°ì¹˜ ëª¨ë“œë¡œ ì „í™˜...")
                self.train_df = self.load_data_in_batches('data/train.parquet')

        print("ğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©...")
        self.test_df = pd.read_parquet('data/test.parquet')

        print(f"\nâœ… ë¡œë”© ì™„ë£Œ!")
        print(f"   í›ˆë ¨: {self.train_df.shape}")
        print(f"   í…ŒìŠ¤íŠ¸: {self.test_df.shape}")
        print(f"   í´ë¦­ë¥ : {self.train_df['clicked'].mean():.4f}")

        return True

    def load_data_in_batches(self, file_path, batch_size=500000):
        """ë°°ì¹˜ë³„ë¡œ ì•ˆì „í•˜ê²Œ ë°ì´í„° ë¡œë”© (ê°„ë‹¨í•œ ë°©ì‹)"""
        print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸° {batch_size:,}í–‰ìœ¼ë¡œ ì•ˆì „ ë¡œë”©...")

        try:
            # ë¨¼ì € ì „ì²´ ë¡œë“œ ì‹œë„
            print("   ì „ì²´ ë¡œë“œ ì‹œë„ ì¤‘...")
            full_df = pd.read_parquet(file_path)
            print(f"âœ… ì „ì²´ ë¡œë“œ ì„±ê³µ: {full_df.shape}")
            return full_df

        except MemoryError:
            print("   ë©”ëª¨ë¦¬ ë¶€ì¡±! ìƒ˜í”Œë§ìœ¼ë¡œ ì „í™˜...")

            # ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ 70% ìƒ˜í”Œë§ìœ¼ë¡œ í´ë°±
            full_df = pd.read_parquet(file_path)
            clicked = full_df[full_df['clicked'] == 1]
            not_clicked = full_df[full_df['clicked'] == 0]

            # 70% ìƒ˜í”Œë§
            sample_ratio = 0.7
            n_clicked = int(len(clicked) * sample_ratio)
            n_not_clicked = int(len(not_clicked) * sample_ratio)

            sample_clicked = clicked.sample(min(len(clicked), n_clicked), random_state=42)
            sample_not_clicked = not_clicked.sample(min(len(not_clicked), n_not_clicked), random_state=42)

            result_df = pd.concat([sample_clicked, sample_not_clicked], ignore_index=True)

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del full_df, clicked, not_clicked, sample_clicked, sample_not_clicked
            gc.collect()

            print(f"âœ… ìƒ˜í”Œë§ ë¡œë“œ ì™„ë£Œ: {result_df.shape}")
            return result_df

        except Exception as e:
            print(f"âŒ ë¡œë”© ì‹¤íŒ¨: {e}")
            # ìµœí›„ì˜ ìˆ˜ë‹¨: 30% ìƒ˜í”Œë§
            print("   30% ìƒ˜í”Œë§ìœ¼ë¡œ ì¬ì‹œë„...")
            full_df = pd.read_parquet(file_path)
            return full_df.sample(frac=0.3, random_state=42)

    def preprocess_features(self):
        """ë¹ ë¥¸ íŠ¹ì„± ì „ì²˜ë¦¬"""
        print("\nğŸ”§ íŠ¹ì„± ì „ì²˜ë¦¬ ì‹œì‘...")

        # ìˆ˜ì¹˜í˜• íŠ¹ì„±
        numeric_cols = [col for col in self.train_df.columns
                       if col.startswith(('feat_', 'history_', 'l_feat_'))]

        # ì¹´í…Œê³ ë¦¬ íŠ¹ì„±
        categorical_cols = ['gender', 'age_group']
        if 'inventory_id' in self.train_df.columns:
            categorical_cols.append('inventory_id')

        print(f"ğŸ“Š íŠ¹ì„± ì •ë³´:")
        print(f"   ìˆ˜ì¹˜í˜•: {len(numeric_cols)}ê°œ")
        print(f"   ì¹´í…Œê³ ë¦¬: {len(categorical_cols)}ê°œ")

        # ìˆ˜ì¹˜í˜• ì „ì²˜ë¦¬
        print("ğŸ”§ ìˆ˜ì¹˜í˜• íŠ¹ì„± ì²˜ë¦¬...")
        for col in tqdm(numeric_cols, desc="ìˆ˜ì¹˜í˜•"):
            if col in self.train_df.columns:
                mean_val = self.train_df[col].fillna(0).mean()
                self.train_df[col] = self.train_df[col].fillna(mean_val)
                self.test_df[col] = self.test_df[col].fillna(mean_val)

        # ì¹´í…Œê³ ë¦¬ ì „ì²˜ë¦¬
        print("ğŸ”§ ì¹´í…Œê³ ë¦¬ íŠ¹ì„± ì²˜ë¦¬...")
        for col in tqdm(categorical_cols, desc="ì¹´í…Œê³ ë¦¬"):
            if col in self.train_df.columns:
                le = LabelEncoder()

                combined = pd.concat([
                    self.train_df[col].fillna('unknown'),
                    self.test_df[col].fillna('unknown')
                ]).astype(str)

                le.fit(combined)
                self.train_df[col] = le.transform(self.train_df[col].fillna('unknown').astype(str))
                self.test_df[col] = le.transform(self.test_df[col].fillna('unknown').astype(str))
                self.encoders[col] = le

        # ìµœì¢… íŠ¹ì„± ëª©ë¡
        self.feature_cols = [col for col in numeric_cols + categorical_cols
                           if col in self.train_df.columns]

        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(self.feature_cols)}ê°œ íŠ¹ì„±")
        return True

    def train_xgboost_models(self):
        """XGBoost ëª¨ë¸ í›ˆë ¨ (validation + early stopping)"""
        print("\nğŸš€ XGBoost ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")

        X = self.train_df[self.feature_cols]
        y = self.train_df['clicked']

        # í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬
        pos_ratio = y.mean()
        scale_pos_weight = (1 - pos_ratio) / pos_ratio
        print(f"ğŸ“Š í´ë¦­ë¥ : {pos_ratio:.4f}")
        print(f"ğŸ“Š Scale pos weight: {scale_pos_weight:.2f}")

        # Train/Validation ë¶„í•  (early stoppingìš©)
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        # ë¶„í•  í›„ íƒ€ê²Ÿ ë¶„í¬ í™•ì¸
        train_click_rate = y_train.mean()
        val_click_rate = y_val.mean()

        print(f"ğŸ“Š ë°ì´í„° ë¶„í• : í›ˆë ¨ {X_train.shape[0]:,}, ê²€ì¦ {X_val.shape[0]:,}")
        print(f"ğŸ“Š í´ë¦­ë¥  ë¶„í¬:")
        print(f"   ì „ì²´: {y.mean():.4f}")
        print(f"   í›ˆë ¨: {train_click_rate:.4f}")
        print(f"   ê²€ì¦: {val_click_rate:.4f}")
        print(f"   ì°¨ì´: {abs(train_click_rate - val_click_rate):.4f}")

        # ì•ˆì „ì„± ê²€ì‚¬
        if abs(train_click_rate - val_click_rate) > 0.001:
            print("âš ï¸ ê²½ê³ : í´ë¦­ë¥  ë¶„í¬ê°€ ë‹¤ë¦„!")
        else:
            print("âœ… í´ë¦­ë¥  ë¶„í¬ ê· ë“±í•¨")

        # ëª¨ë¸ ì„¤ì •ë“¤ (ëŒ€íšŒ ì§€í‘œ ìµœì í™”)
        model_configs = {
            'xgb_ap_focused': {
                'objective': 'binary:logistic',
                'eval_metric': ['auc', 'aucpr'],  # AP ìµœì í™”
                'tree_method': 'hist',
                'max_depth': 6,
                'learning_rate': 0.08,
                'n_estimators': 600,
                'subsample': 0.85,
                'colsample_bytree': 0.85,
                'scale_pos_weight': scale_pos_weight,
                'random_state': 42,
                'n_jobs': -1,
                'verbosity': 0
            },
            'xgb_balanced': {
                'objective': 'binary:logistic',
                'eval_metric': ['logloss', 'aucpr'],  # WLL + AP ê· í˜•
                'tree_method': 'hist',
                'max_depth': 7,
                'learning_rate': 0.06,
                'n_estimators': 800,
                'subsample': 0.9,
                'colsample_bytree': 0.9,
                'scale_pos_weight': scale_pos_weight,
                'reg_alpha': 0.1,  # L1 ì •ê·œí™” (í™•ë¥  ë³´ì •)
                'reg_lambda': 0.1,  # L2 ì •ê·œí™”
                'random_state': 42,
                'n_jobs': -1,
                'verbosity': 0
            }
        }

        for name, params in model_configs.items():
            print(f"\nğŸ”„ {name} í›ˆë ¨ ì¤‘...")

            # Early stopping íŒŒë¼ë¯¸í„°ë¥¼ ìƒì„±ìì— ì¶”ê°€
            params_with_early_stop = params.copy()
            params_with_early_stop['early_stopping_rounds'] = 20
            params_with_early_stop['enable_categorical'] = False  # í˜¸í™˜ì„±

            model = xgb.XGBClassifier(**params_with_early_stop)

            # ê°„ë‹¨í•œ fit (ìƒˆ ë²„ì „ í˜¸í™˜)
            model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                verbose=20  # 20 ë¼ìš´ë“œë§ˆë‹¤ ì¶œë ¥
            )

            # ê²€ì¦ ì„±ëŠ¥ í‰ê°€ (ëŒ€íšŒ ì§€í‘œ)
            val_pred = model.predict_proba(X_val)[:, 1]

            # ëŒ€íšŒ í‰ê°€ ì§€í‘œ ê³„ì‚°
            comp_score, ap, wll = calculate_competition_score(y_val, val_pred)
            auc = roc_auc_score(y_val, val_pred)

            # Best iteration ì •ë³´
            try:
                if hasattr(model, 'best_iteration') and model.best_iteration is not None:
                    best_iter = model.best_iteration
                    print(f"   âœ… Best iteration: {best_iter}")
                else:
                    best_iter = getattr(model, 'n_estimators', 'unknown')
                    print(f"   âœ… Total iterations: {best_iter}")
            except:
                print(f"   âœ… í›ˆë ¨ ì™„ë£Œ")

            print(f"   ğŸ“Š {name} ì„±ëŠ¥:")
            print(f"      ëŒ€íšŒ ì ìˆ˜: {comp_score:.4f}")
            print(f"      AP: {ap:.4f}")
            print(f"      WLL: {wll:.4f}")
            print(f"      AUC: {auc:.4f}")

            self.models[name] = [model]  # ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì„œ ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜

        print("âœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")

    def predict_and_submit(self):
        """ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±"""
        print("\nğŸ¯ ì˜ˆì¸¡ ì‹œì‘...")

        X_test = self.test_df[self.feature_cols]
        all_predictions = []

        for name, fold_models in self.models.items():
            print(f"ğŸ”„ {name} ì˜ˆì¸¡ ì¤‘...")

            fold_preds = []
            for i, model in enumerate(fold_models):
                pred = model.predict_proba(X_test)[:, 1]
                fold_preds.append(pred)
                print(f"   Fold {i+1} ì™„ë£Œ")

            avg_pred = np.mean(fold_preds, axis=0)
            all_predictions.append(avg_pred)

        # ì•™ìƒë¸” í‰ê· 
        final_predictions = np.mean(all_predictions, axis=0)

        # ì œì¶œ íŒŒì¼ ìƒì„± (ì˜¬ë°”ë¥¸ í˜•ì‹)
        try:
            # sample_submission.csvë¥¼ í…œí”Œë¦¿ìœ¼ë¡œ ì‚¬ìš©
            submission = pd.read_csv('data/sample_submission.csv')
            submission['clicked'] = final_predictions
            print(f"âœ… ì˜¬ë°”ë¥¸ ID í˜•ì‹ ì‚¬ìš©: {submission['ID'].iloc[0]}")
        except:
            # í´ë°±: ì§ì ‘ ìƒì„±
            submission = pd.DataFrame({
                'ID': [f'TEST_{i:07d}' for i in range(len(final_predictions))],
                'clicked': final_predictions
            })
            print("âš ï¸ ì§ì ‘ ID ìƒì„± (sample_submission.csv ì—†ìŒ)")

        submission_path = 'submission_mac_xgboost_competition.csv'
        submission.to_csv(submission_path, index=False, encoding='utf-8')

        print(f"\nâœ… ì œì¶œ íŒŒì¼ ìƒì„±: {submission_path}")
        print(f"ğŸ“Š ì˜ˆì¸¡ í†µê³„:")
        print(f"   í‰ê·  í´ë¦­ë¥ : {final_predictions.mean():.4f}")
        print(f"   ìµœì†Œê°’: {final_predictions.min():.4f}")
        print(f"   ìµœëŒ€ê°’: {final_predictions.max():.4f}")

        return submission_path

    def run_pipeline(self, sample_ratio=0.3, use_batch=False):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        start_time = time.time()

        # 1. ë°ì´í„° ë¡œë”©
        if not self.load_data_efficiently(sample_ratio, use_batch):
            return False

        # 2. ì „ì²˜ë¦¬
        if not self.preprocess_features():
            return False

        # 3. ëª¨ë¸ í›ˆë ¨
        self.train_xgboost_models()

        # 4. ì˜ˆì¸¡
        submission_path = self.predict_and_submit()

        elapsed = time.time() - start_time

        print("\n" + "ğŸ‰" * 60)
        print("ğŸ‰ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! ğŸ‰")
        print("ğŸ‰" * 60)
        print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ ({elapsed/60:.1f}ë¶„)")
        print(f"ğŸ“ ì œì¶œ íŒŒì¼: {submission_path}")

        return True

def main():
    pipeline = MacXGBoostCTR()

    print("\nğŸ“‹ ì‹¤í–‰ ì˜µì…˜:")
    print("1. ğŸš€ ì´ˆê³ ì† ëª¨ë“œ (30% ìƒ˜í”Œë§) - 1-2ë¶„")
    print("2. âš¡ ë¹ ë¥¸ ëª¨ë“œ (50% ìƒ˜í”Œë§) - 2-3ë¶„")
    print("3. ğŸ¯ ì •í™• ëª¨ë“œ (70% ìƒ˜í”Œë§) - 4-5ë¶„")
    print("4. ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë“œ (ì „ì²´ ë°ì´í„° ì§ì ‘) - 5-8ë¶„ âš ï¸ ë©”ëª¨ë¦¬ ìœ„í—˜")
    print("5. ğŸ›¡ï¸ ì•ˆì „ ìµœê³  ì„±ëŠ¥ ëª¨ë“œ (ì „ì²´ ë°ì´í„° ë°°ì¹˜) - 8-12ë¶„ âœ… ë©”ëª¨ë¦¬ ì•ˆì „")

    choice = input("ì„ íƒ (1-5, ê¸°ë³¸ê°’ 1): ").strip() or '1'

    if choice == '5':
        sample_ratio = 1.0
        use_batch = True
        print(f"\nğŸ›¡ï¸ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì „ì²´ ë°ì´í„° ì•ˆì „ ë¡œë”©!")
    elif choice == '4':
        sample_ratio = 1.0
        use_batch = False
        print(f"\nğŸ† ì „ì²´ ë°ì´í„° ì§ì ‘ ë¡œë”© (ë©”ëª¨ë¦¬ ìœ„í—˜ ê°ìˆ˜)!")
    else:
        sample_ratios = {'1': 0.3, '2': 0.5, '3': 0.7}
        sample_ratio = sample_ratios.get(choice, 0.3)
        use_batch = False
        print(f"\nğŸš€ {sample_ratio*100:.0f}% ë°ì´í„°ë¡œ ì‹¤í–‰ ì‹œì‘!")

    print("=" * 60)

    success = pipeline.run_pipeline(sample_ratio, use_batch)

    if success:
        print("\nğŸŠ ì„±ê³µ! ì œì¶œ íŒŒì¼ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâŒ ì‹¤í–‰ ì‹¤íŒ¨")

if __name__ == "__main__":
    main()