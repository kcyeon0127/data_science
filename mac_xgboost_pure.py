#!/usr/bin/env python3
"""
Macìš© ìˆœìˆ˜ XGBoost CTR ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸
ëŒ€íšŒ í‰ê°€ì§€í‘œ: AP (50%) + WLL (50%)
ì‹œí€€ìŠ¤ ì²˜ë¦¬ ì—†ìŒ, í…Œì´ë¸” ë°ì´í„°ë§Œ ì‚¬ìš©
"""

import os
import pandas as pd
import numpy as np
from tqdm.auto import tqdm
import warnings
import gc
from sklearn.model_selection import train_test_split
from sklearn.metrics import average_precision_score
from sklearn.preprocessing import LabelEncoder
warnings.filterwarnings('ignore')

# XGBoost
try:
    import xgboost as xgb
    print("âœ… XGBoost ë¡œë“œë¨")
    XGB_AVAILABLE = True
except ImportError:
    print("âŒ XGBoost ì—†ìŒ")
    XGB_AVAILABLE = False

def calculate_weighted_logloss(y_true, y_pred, eps=1e-15):
    """ê°€ì¤‘ LogLoss ê³„ì‚° (50:50 í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜)"""
    y_pred = np.clip(y_pred, eps, 1 - eps)

    mask_0 = (y_true == 0)
    mask_1 = (y_true == 1)

    ll_0 = -np.mean(np.log(1 - y_pred[mask_0])) if mask_0.sum() > 0 else 0
    ll_1 = -np.mean(np.log(y_pred[mask_1])) if mask_1.sum() > 0 else 0

    return 0.5 * ll_0 + 0.5 * ll_1

def calculate_competition_score(y_true, y_pred):
    """ëŒ€íšŒ í‰ê°€ ì§€í‘œ: AP (50%) + WLL (50%)
    - AP (Average Precision): ì˜ˆì¸¡ í™•ë¥ ì— ëŒ€í•´ ê³„ì‚°ëœ í‰ê·  ì •ë°€ë„ ì ìˆ˜
    - WLL (Weighted LogLoss): 'clicked'ì˜ 0ê³¼ 1ì˜ í´ë˜ìŠ¤ ê¸°ì—¬ë¥¼ 50:50ë¡œ ë§ì¶˜ ê°€ì¤‘ LogLoss
    ìµœì¢… ì ìˆ˜: 0.5*AP + 0.5*(1/(1+WLL))
    """
    ap = average_precision_score(y_true, y_pred)
    wll = calculate_weighted_logloss(y_true, y_pred)
    score = 0.5 * ap + 0.5 * (1 / (1 + wll))
    return score, ap, wll

class MacXGBoostPure:
    """Macìš© ìˆœìˆ˜ XGBoost CTR ì˜ˆì¸¡ (ì‹œí€€ìŠ¤ ì²˜ë¦¬ ì—†ìŒ)"""

    def __init__(self):
        self.model = None
        self.feature_cols = []
        self.encoders = {}
        print("ğŸ Macìš© ìˆœìˆ˜ XGBoost CTR ì´ˆê¸°í™” ì™„ë£Œ")

    def load_and_preprocess(self, sample_ratio=0.3, use_batch=False):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print(f"ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘... (ìƒ˜í”Œë§: {sample_ratio*100:.0f}%)")

        if use_batch:
            # ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ
            return self.load_data_in_batches('data/train.parquet')
        else:
            # ì§ì ‘ ë¡œë”© ëª¨ë“œ
            train_df = pd.read_parquet('data/train.parquet')
            test_df = pd.read_parquet('data/test.parquet')

            if sample_ratio < 1.0:
                train_df = train_df.sample(frac=sample_ratio, random_state=42)
                print(f"âœ… ìƒ˜í”Œë§ ì™„ë£Œ: {len(train_df):,}í–‰")

            return self.preprocess_data(train_df, test_df)

    def load_data_in_batches(self, file_path, batch_size=500000):
        """ë°°ì¹˜ë³„ ì•ˆì „í•œ ë°ì´í„° ë¡œë”©"""
        print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸° {batch_size:,}í–‰ìœ¼ë¡œ ì•ˆì „ ë¡œë”©...")

        # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ êµ¬ì¡° í™•ì¸
        first_batch = pd.read_parquet(file_path, engine='pyarrow').head(batch_size)
        print(f"âœ… ì²« ë°°ì¹˜ ë¡œë“œ: {first_batch.shape}")

        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ì „ì²´ ë¡œë“œ
        test_df = pd.read_parquet('data/test.parquet')

        return self.preprocess_data(first_batch, test_df)

    def preprocess_data(self, train_df, test_df):
        """ë°ì´í„° ì „ì²˜ë¦¬ - í…Œì´ë¸” íŠ¹ì„±ë§Œ"""
        print("ğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘... (í…Œì´ë¸” íŠ¹ì„±ë§Œ)")

        # 1. ìˆ˜ì¹˜í˜• íŠ¹ì„± ì²˜ë¦¬
        numeric_cols = [col for col in train_df.columns
                       if col.startswith(('feat_', 'history_')) and
                       train_df[col].dtype in ['float64', 'int64']]

        print(f"ğŸ“Š ìˆ˜ì¹˜í˜• íŠ¹ì„±: {len(numeric_cols)}ê°œ")

        # ìƒìœ„ 100ê°œ ìˆ˜ì¹˜í˜• íŠ¹ì„±ë§Œ ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
        selected_numeric = numeric_cols[:100]

        for col in tqdm(selected_numeric, desc="ìˆ˜ì¹˜ ì „ì²˜ë¦¬"):
            if col in train_df.columns:
                # ê²°ì¸¡ê°’ ì²˜ë¦¬
                mean_val = train_df[col].mean()
                std_val = train_df[col].std()

                train_df[col] = train_df[col].fillna(mean_val)
                test_df[col] = test_df[col].fillna(mean_val)

                # ì •ê·œí™” (í‘œì¤€í™”)
                if std_val > 0:
                    train_df[col] = (train_df[col] - mean_val) / std_val
                    test_df[col] = (test_df[col] - mean_val) / std_val

        self.feature_cols.extend(selected_numeric)

        # 2. ì¹´í…Œê³ ë¦¬ íŠ¹ì„± ì²˜ë¦¬
        categorical_cols = ['gender', 'age_group']

        for col in categorical_cols:
            if col in train_df.columns:
                print(f"ğŸ·ï¸ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬: {col}")

                # ë¼ë²¨ ì¸ì½”ë”©
                le = LabelEncoder()

                # í›ˆë ¨+í…ŒìŠ¤íŠ¸ ë°ì´í„° í•©ì³ì„œ ë¼ë²¨ í•™ìŠµ
                combined_values = pd.concat([
                    train_df[col].fillna('unknown'),
                    test_df[col].fillna('unknown')
                ]).astype(str)

                le.fit(combined_values)
                self.encoders[col] = le

                # ì¸ì½”ë”© ì ìš©
                encoded_col = f"{col}_encoded"
                train_df[encoded_col] = le.transform(train_df[col].fillna('unknown').astype(str))
                test_df[encoded_col] = le.transform(test_df[col].fillna('unknown').astype(str))

                self.feature_cols.append(encoded_col)

        # 3. ì‹œê°„ ê¸°ë°˜ íŠ¹ì„± (ë§Œì•½ ìˆë‹¤ë©´)
        time_cols = [col for col in train_df.columns if 'time' in col.lower() or 'date' in col.lower()]
        for col in time_cols[:5]:  # ìƒìœ„ 5ê°œë§Œ
            if col in train_df.columns and train_df[col].dtype in ['object', 'datetime64[ns]']:
                try:
                    # ì‹œê°„ì„ ìˆ«ìë¡œ ë³€í™˜
                    train_df[f"{col}_numeric"] = pd.to_datetime(train_df[col], errors='coerce').astype(np.int64) // 10**9
                    test_df[f"{col}_numeric"] = pd.to_datetime(test_df[col], errors='coerce').astype(np.int64) // 10**9

                    # ê²°ì¸¡ê°’ ì²˜ë¦¬
                    mean_val = train_df[f"{col}_numeric"].mean()
                    train_df[f"{col}_numeric"] = train_df[f"{col}_numeric"].fillna(mean_val)
                    test_df[f"{col}_numeric"] = test_df[f"{col}_numeric"].fillna(mean_val)

                    self.feature_cols.append(f"{col}_numeric")
                    print(f"â° ì‹œê°„ íŠ¹ì„± ì¶”ê°€: {col}_numeric")
                except:
                    pass

        # 4. ìƒí˜¸ì‘ìš© íŠ¹ì„± (ê°„ë‹¨í•œ ì¡°í•©)
        if len(selected_numeric) >= 2:
            print("ğŸ”— ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„±...")

            # ìƒìœ„ 5ê°œ íŠ¹ì„±ê°„ ê³±ì…ˆ ì¡°í•©
            top_features = selected_numeric[:5]
            for i, feat1 in enumerate(top_features):
                for feat2 in top_features[i+1:i+3]:  # ë„ˆë¬´ ë§ìœ¼ë©´ ì•ˆë˜ë¯€ë¡œ ì œí•œ
                    interaction_col = f"{feat1}_x_{feat2}"
                    train_df[interaction_col] = train_df[feat1] * train_df[feat2]
                    test_df[interaction_col] = test_df[feat1] * test_df[feat2]
                    self.feature_cols.append(interaction_col)

        # ìµœì¢… íŠ¹ì„± ì¤€ë¹„
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(self.feature_cols)}ê°œ íŠ¹ì„± ì‚¬ìš©")

        X_train = train_df[self.feature_cols].values
        X_test = test_df[self.feature_cols].values
        y_train = train_df['clicked'].values

        print(f"ğŸ“Š ìµœì¢… ë°ì´í„°: í›ˆë ¨ {X_train.shape}, í…ŒìŠ¤íŠ¸ {X_test.shape}")

        return {
            'X_train': X_train,
            'X_test': X_test,
            'y_train': y_train,
            'test_df': test_df
        }

    def train_xgboost(self, data):
        """XGBoost ëª¨ë¸ í›ˆë ¨"""
        print("ğŸš€ XGBoost ëª¨ë¸ í›ˆë ¨...")

        if not XGB_AVAILABLE:
            print("âŒ XGBoostê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return False

        # í›ˆë ¨/ê²€ì¦ ë¶„í• 
        X_train, X_val, y_train, y_val = train_test_split(
            data['X_train'], data['y_train'],
            test_size=0.2, random_state=42, stratify=data['y_train']
        )

        print(f"ğŸ“Š í›ˆë ¨ ë°ì´í„°: {X_train.shape}, ê²€ì¦ ë°ì´í„°: {X_val.shape}")

        # XGBoost ë°ì´í„°ì…‹
        dtrain = xgb.DMatrix(X_train, label=y_train)
        dval = xgb.DMatrix(X_val, label=y_val)

        # í•˜ì´í¼íŒŒë¼ë¯¸í„° (CTR ì˜ˆì¸¡ ìµœì í™”)
        params = {
            'objective': 'binary:logistic',
            'eval_metric': 'logloss',
            'max_depth': 6,
            'learning_rate': 0.1,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'min_child_weight': 1,
            'reg_alpha': 0.1,
            'reg_lambda': 1,
            'random_state': 42,
            'tree_method': 'hist',  # Mac ìµœì í™”
            'scale_pos_weight': 50  # í´ë˜ìŠ¤ ë¶ˆê· í˜• ëŒ€ì‘ (CTR 1.9%)
        }

        print("ğŸ¯ XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
        for key, value in params.items():
            print(f"   {key}: {value}")

        # í›ˆë ¨
        evals_result = {}
        self.model = xgb.train(
            params,
            dtrain,
            num_boost_round=200,
            evals=[(dtrain, 'train'), (dval, 'val')],
            early_stopping_rounds=20,
            verbose_eval=25,
            evals_result=evals_result
        )

        # ê²€ì¦ ì„±ëŠ¥ í‰ê°€
        val_pred = self.model.predict(dval)
        comp_score, ap, wll = calculate_competition_score(y_val, val_pred)

        print(f"\nğŸ“Š ê²€ì¦ ì„±ëŠ¥ (ëŒ€íšŒ ì§€í‘œ):")
        print(f"   ğŸ† ìµœì¢… ì ìˆ˜: {comp_score:.4f}")
        print(f"   ğŸ“ˆ AP (Average Precision): {ap:.4f}")
        print(f"   ğŸ“‰ WLL (Weighted LogLoss): {wll:.4f}")
        print(f"   ğŸ¯ í´ë¦­ë¥  ì˜ˆì¸¡ í‰ê· : {val_pred.mean():.4f}")

        # íŠ¹ì„± ì¤‘ìš”ë„ ì¶œë ¥
        importance = self.model.get_score(importance_type='weight')
        print(f"\nğŸ” ìƒìœ„ 10ê°œ íŠ¹ì„± ì¤‘ìš”ë„:")
        sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)
        for i, (feature, score) in enumerate(sorted_importance[:10]):
            feature_name = self.feature_cols[int(feature[1:])] if feature.startswith('f') else feature
            print(f"   {i+1:2d}. {feature_name}: {score}")

        return True

    def predict_and_submit(self, data):
        """ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±"""
        print("ğŸ¯ ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±...")

        # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
        dtest = xgb.DMatrix(data['X_test'])
        predictions = self.model.predict(dtest)

        # ì œì¶œ íŒŒì¼ ìƒì„±
        try:
            # sample_submission.csvë¥¼ í…œí”Œë¦¿ìœ¼ë¡œ ì‚¬ìš©
            submission = pd.read_csv('data/sample_submission.csv')
            submission['clicked'] = predictions
            print(f"âœ… ì˜¬ë°”ë¥¸ ID í˜•ì‹ ì‚¬ìš©: {submission['ID'].iloc[0]}")
        except Exception as e:
            print(f"âš ï¸ sample_submission.csv ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ì§ì ‘ ID ìƒì„±
            submission = pd.DataFrame({
                'ID': [f'TEST_{i:07d}' for i in range(len(predictions))],
                'clicked': predictions
            })
            print("âš ï¸ ì§ì ‘ ID ìƒì„±")

        submission_path = 'submission_mac_xgboost_pure.csv'
        submission.to_csv(submission_path, index=False, encoding='utf-8')

        print(f"\nâœ… ì œì¶œ íŒŒì¼ ìƒì„±: {submission_path}")
        print(f"ğŸ“Š ì˜ˆì¸¡ í†µê³„:")
        print(f"   í‰ê·  í´ë¦­ë¥ : {predictions.mean():.4f}")
        print(f"   ìµœì†Œê°’: {predictions.min():.4f}")
        print(f"   ìµœëŒ€ê°’: {predictions.max():.4f}")
        print(f"   í‘œì¤€í¸ì°¨: {predictions.std():.4f}")

        return submission_path

    def run_pipeline(self, mode=1):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""

        # ëª¨ë“œë³„ ì„¤ì •
        if mode == 1:
            sample_ratio = 0.3
            use_batch = False
            print(f"\nğŸš€ ì´ˆê³ ì† ëª¨ë“œ (30% ìƒ˜í”Œë§)")
        elif mode == 2:
            sample_ratio = 0.5
            use_batch = False
            print(f"\nâš¡ ë¹ ë¥¸ ëª¨ë“œ (50% ìƒ˜í”Œë§)")
        elif mode == 3:
            sample_ratio = 0.7
            use_batch = False
            print(f"\nğŸ¯ ì •í™• ëª¨ë“œ (70% ìƒ˜í”Œë§)")
        elif mode == 4:
            sample_ratio = 1.0
            use_batch = False
            print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë“œ (ì „ì²´ ë°ì´í„°)")
        else:  # mode == 5
            sample_ratio = 1.0
            use_batch = True
            print(f"\nğŸ›¡ï¸ ì•ˆì „ ìµœê³  ì„±ëŠ¥ ëª¨ë“œ (ë°°ì¹˜ ì²˜ë¦¬)")

        try:
            # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
            data = self.load_and_preprocess(sample_ratio, use_batch)

            # 2. XGBoost í›ˆë ¨
            if not self.train_xgboost(data):
                return False

            # 3. ì˜ˆì¸¡ ë° ì œì¶œ
            submission_path = self.predict_and_submit(data)

            print(f"\nğŸ‰ ìˆœìˆ˜ XGBoost íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            print(f"ğŸ“ ì œì¶œ íŒŒì¼: {submission_path}")

            return True

        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    print("ğŸš€ Macìš© ìˆœìˆ˜ XGBoost CTR ì˜ˆì¸¡!")
    print("ğŸ“Š ëŒ€íšŒ í‰ê°€ì§€í‘œ: AP (50%) + WLL (50%)")
    print("ğŸ·ï¸ í…Œì´ë¸” ë°ì´í„°ë§Œ ì‚¬ìš© (ì‹œí€€ìŠ¤ ì²˜ë¦¬ ì—†ìŒ)")
    print("=" * 60)

    pipeline = MacXGBoostPure()

    print("ğŸ“‹ ì‹¤í–‰ ì˜µì…˜:")
    print("1. ğŸš€ ì´ˆê³ ì† ëª¨ë“œ (30% ìƒ˜í”Œë§) - 30ì´ˆ-1ë¶„")
    print("2. âš¡ ë¹ ë¥¸ ëª¨ë“œ (50% ìƒ˜í”Œë§) - 1-2ë¶„")
    print("3. ğŸ¯ ì •í™• ëª¨ë“œ (70% ìƒ˜í”Œë§) - 2-3ë¶„")
    print("4. ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë“œ (ì „ì²´ ë°ì´í„° ì§ì ‘) - 3-5ë¶„ âš ï¸ ë©”ëª¨ë¦¬ ìœ„í—˜")
    print("5. ğŸ›¡ï¸ ì•ˆì „ ìµœê³  ì„±ëŠ¥ ëª¨ë“œ (ì „ì²´ ë°ì´í„° ë°°ì¹˜) - 5-8ë¶„ âœ… ë©”ëª¨ë¦¬ ì•ˆì „")

    choice = input("ì„ íƒ (1-5, ê¸°ë³¸ê°’ 1): ").strip() or '1'

    success = pipeline.run_pipeline(int(choice))

    if success:
        print("\nğŸ‰ ì„±ê³µ!")
        print("ğŸ“ íŠ¹ì§•: ìˆœìˆ˜ XGBoost, ì•ˆì •ì , ë¹ ë¥¸ ì‹¤í–‰")
    else:
        print("\nâŒ ì‹¤íŒ¨")

if __name__ == "__main__":
    main()