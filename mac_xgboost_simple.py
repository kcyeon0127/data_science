#!/usr/bin/env python3
"""
Macìš© XGBoost + ê°„ë‹¨í•œ LSTM CTR ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ (Attention ì œì™¸)
ëŒ€íšŒ í‰ê°€ì§€í‘œ: AP (50%) + WLL (50%)
"""

import os
import pandas as pd
import numpy as np
from tqdm.auto import tqdm
import warnings
import gc
from sklearn.model_selection import train_test_split
from sklearn.metrics import average_precision_score
warnings.filterwarnings('ignore')

# XGBoost
try:
    import xgboost as xgb
    print("âœ… XGBoost ë¡œë“œë¨")
    XGB_AVAILABLE = True
except ImportError:
    print("âŒ XGBoost ì—†ìŒ")
    XGB_AVAILABLE = False

# PyTorch (ê°„ë‹¨í•œ LSTMìš©)
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.utils.data import Dataset, DataLoader
    print("ğŸ§  PyTorch ë¡œë“œë¨ - ê°„ë‹¨í•œ LSTM ì‚¬ìš© ê°€ëŠ¥")

    # ì•ˆì „ì„±ì„ ìœ„í•´ CPU ê°•ì œ ì‚¬ìš© (MPS segfault íšŒí”¼)
    device = torch.device("cpu")
    print("ğŸ’» CPU ëª¨ë“œ (ì•ˆì „ì„± ìš°ì„ , ì†ë„ëŠ” ëŠë¦¼)")

    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    device = None
    print("âš ï¸ PyTorch ì—†ìŒ - ì‹œí€€ìŠ¤ í†µê³„ íŠ¹ì„±ìœ¼ë¡œ ëŒ€ì²´")

def calculate_weighted_logloss(y_true, y_pred, eps=1e-15):
    """ê°€ì¤‘ LogLoss ê³„ì‚° (50:50 í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜)"""
    y_pred = np.clip(y_pred, eps, 1 - eps)

    mask_0 = (y_true == 0)
    mask_1 = (y_true == 1)

    ll_0 = -np.mean(np.log(1 - y_pred[mask_0])) if mask_0.sum() > 0 else 0
    ll_1 = -np.mean(np.log(y_pred[mask_1])) if mask_1.sum() > 0 else 0

    return 0.5 * ll_0 + 0.5 * ll_1

def calculate_competition_score(y_true, y_pred):
    """ëŒ€íšŒ í‰ê°€ ì§€í‘œ: AP (50%) + WLL (50%)
    - AP (Average Precision): ì˜ˆì¸¡ í™•ë¥ ì— ëŒ€í•´ ê³„ì‚°ëœ í‰ê·  ì •ë°€ë„ ì ìˆ˜
    - WLL (Weighted LogLoss): 'clicked'ì˜ 0ê³¼ 1ì˜ í´ë˜ìŠ¤ ê¸°ì—¬ë¥¼ 50:50ë¡œ ë§ì¶˜ ê°€ì¤‘ LogLoss
    ìµœì¢… ì ìˆ˜: 0.5*AP + 0.5*(1/(1+WLL))
    """
    ap = average_precision_score(y_true, y_pred)
    wll = calculate_weighted_logloss(y_true, y_pred)
    score = 0.5 * ap + 0.5 * (1 / (1 + wll))
    return score, ap, wll

class SimpleLSTMModel(nn.Module):
    """ê°„ë‹¨í•œ PyTorch LSTM ì‹œí€€ìŠ¤ ì²˜ë¦¬ ëª¨ë¸ (Attention ì œì™¸)"""

    def __init__(self, vocab_size=50000, embedding_dim=64, lstm_units=128, output_dim=32):
        super().__init__()
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.lstm_units = lstm_units
        self.output_dim = output_dim

        # ì„ë² ë”© ë ˆì´ì–´
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)

        # LSTM ë ˆì´ì–´ (ì–‘ë°©í–¥)
        self.lstm = nn.LSTM(embedding_dim, lstm_units, batch_first=True, bidirectional=True)

        # ì¶œë ¥ ë ˆì´ì–´ (Global Average Pooling + FC)
        self.fc1 = nn.Linear(lstm_units * 2, 128)  # ì–‘ë°©í–¥ì´ë¯€ë¡œ *2
        self.dropout = nn.Dropout(0.3)
        self.fc2 = nn.Linear(128, output_dim)

    def forward(self, x):
        # ì„ë² ë”©
        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)

        # LSTM
        lstm_out, _ = self.lstm(embedded)  # (batch, seq_len, lstm_units*2)

        # Global Average Pooling (Attention ëŒ€ì‹ )
        pooled = torch.mean(lstm_out, dim=1)  # (batch, lstm_units*2)

        # ì¶œë ¥ ë ˆì´ì–´
        x = F.relu(self.fc1(pooled))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))

        return x

class PyTorchSequenceProcessor:
    """PyTorch ê¸°ë°˜ ê°„ë‹¨í•œ ì‹œí€€ìŠ¤ ì²˜ë¦¬"""

    def __init__(self, vocab_size=50000, embedding_dim=64, lstm_units=128):
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.lstm_units = lstm_units
        self.model = None
        self.device = device

    def create_model(self):
        """ëª¨ë¸ ìƒì„±"""
        if not TORCH_AVAILABLE:
            return None

        try:
            self.model = SimpleLSTMModel(
                vocab_size=self.vocab_size,
                embedding_dim=self.embedding_dim,
                lstm_units=self.lstm_units
            )

            # MPS ì¥ì¹˜ë¡œ ì´ë™ ì‹œë„
            if self.device and str(self.device) == 'mps':
                try:
                    self.model = self.model.to(self.device)
                    print("âœ… MPS ì¥ì¹˜ë¡œ ëª¨ë¸ ì´ë™ ì„±ê³µ")
                except Exception as e:
                    print(f"âš ï¸ MPS ì´ë™ ì‹¤íŒ¨, CPU ì‚¬ìš©: {e}")
                    self.device = torch.device("cpu")
                    self.model = self.model.to(self.device)
            elif self.device:
                self.model = self.model.to(self.device)

            return self.model

        except Exception as e:
            print(f"âŒ PyTorch ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def train_model(self, sequences, epochs=3, batch_size=256):
        """ê°„ë‹¨í•œ ìê¸°ì§€ë„ í•™ìŠµ"""
        if not TORCH_AVAILABLE or self.model is None:
            return

        print(f"ğŸš€ PyTorch ê°„ë‹¨í•œ LSTM ëª¨ë¸ í›ˆë ¨ ({epochs} ì—í¬í¬)...")
        print(f"ğŸ”§ ì¥ì¹˜: {self.device}, ì‹œí€€ìŠ¤ ìˆ˜: {len(sequences)}")

        try:
            # ë”ë¯¸ íƒ€ê²Ÿ ìƒì„± (ì‹œí€€ìŠ¤ì˜ í‰ê· ì„ 32ì°¨ì›ìœ¼ë¡œ)
            targets = []
            for seq in sequences:
                non_zero = seq[seq != 0]
                if len(non_zero) > 0:
                    mean_val = np.mean(non_zero)
                    target = np.full(32, mean_val, dtype=np.float32)
                else:
                    target = np.zeros(32, dtype=np.float32)
                targets.append(target)

            targets = np.array(targets)

            # ë°ì´í„°ì…‹ ì¤€ë¹„
            sequences_tensor = torch.tensor(sequences, dtype=torch.long)
            targets_tensor = torch.tensor(targets, dtype=torch.float32)

            if self.device:
                sequences_tensor = sequences_tensor.to(self.device)
                targets_tensor = targets_tensor.to(self.device)

            # í›ˆë ¨
            optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)
            criterion = nn.MSELoss()

            self.model.train()

            for epoch in range(epochs):
                total_loss = 0
                n_batches = 0

                for i in range(0, len(sequences_tensor), batch_size):
                    batch_seq = sequences_tensor[i:i+batch_size]
                    batch_target = targets_tensor[i:i+batch_size]

                    optimizer.zero_grad()
                    output = self.model(batch_seq)
                    loss = criterion(output, batch_target)
                    loss.backward()
                    optimizer.step()

                    total_loss += loss.item()
                    n_batches += 1

                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    if i % (batch_size * 10) == 0:  # 10ë°°ì¹˜ë§ˆë‹¤
                        if torch.backends.mps.is_available():
                            torch.mps.empty_cache()

                avg_loss = total_loss / n_batches
                print(f"   Epoch {epoch+1}: Loss = {avg_loss:.4f}")

        except Exception as e:
            print(f"âŒ PyTorch í›ˆë ¨ ì‹¤íŒ¨: {e}")
            print("ğŸ”„ ì‹œí€€ìŠ¤ íŠ¹ì„±ì„ í†µê³„ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤")

    def extract_features(self, sequences):
        """ì‹œí€€ìŠ¤ì—ì„œ íŠ¹ì„± ì¶”ì¶œ"""
        if not TORCH_AVAILABLE or self.model is None:
            return self.statistical_features(sequences)

        print("ğŸ¯ PyTorch ê°„ë‹¨í•œ LSTM íŠ¹ì„± ì¶”ì¶œ...")

        try:
            sequences_tensor = torch.tensor(sequences, dtype=torch.long)
            if self.device:
                sequences_tensor = sequences_tensor.to(self.device)

            with torch.no_grad():
                batch_size = 256
                features_list = []

                for i in range(0, len(sequences_tensor), batch_size):
                    batch = sequences_tensor[i:i+batch_size]
                    output = self.model(batch)
                    if self.device:
                        output = output.cpu()
                    features_list.append(output.numpy())

                return np.vstack(features_list)

        except Exception as e:
            print(f"âŒ PyTorch íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return self.statistical_features(sequences)

    def statistical_features(self, sequences):
        """í†µê³„ ê¸°ë°˜ ì‹œí€€ìŠ¤ íŠ¹ì„± (í´ë°±)"""
        print("ğŸ“Š í†µê³„ ê¸°ë°˜ ì‹œí€€ìŠ¤ íŠ¹ì„± ì¶”ì¶œ...")

        features = []
        for seq in tqdm(sequences, desc="í†µê³„ íŠ¹ì„±"):
            non_zero = seq[seq != 0]
            if len(non_zero) > 0:
                feat = [
                    np.mean(non_zero),
                    np.std(non_zero),
                    np.max(non_zero),
                    np.min(non_zero),
                    len(non_zero),
                    len(non_zero) / len(seq),  # ë°€ë„
                    np.median(non_zero),
                    np.sum(non_zero > np.mean(non_zero))  # í‰ê·  ì´ìƒ ê°œìˆ˜
                ]
            else:
                feat = [0] * 8

            # 32ì°¨ì›ìœ¼ë¡œ í™•ì¥ (ë°˜ë³µ)
            feat = feat * 4
            features.append(feat)

        return np.array(features)

class MacXGBoostSimple:
    """Macìš© XGBoost + ê°„ë‹¨í•œ LSTM CTR ì˜ˆì¸¡"""

    def __init__(self):
        self.model = None
        self.sequence_processor = PyTorchSequenceProcessor()
        print("ğŸ Macìš© XGBoost + ê°„ë‹¨í•œ LSTM CTR ì´ˆê¸°í™” ì™„ë£Œ")

    def load_and_preprocess(self, sample_ratio=0.3, use_batch=False):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print(f"ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘... (ìƒ˜í”Œë§: {sample_ratio*100:.0f}%)")

        if use_batch:
            # ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ
            return self.load_data_in_batches('data/train.parquet')
        else:
            # ì§ì ‘ ë¡œë”© ëª¨ë“œ
            train_df = pd.read_parquet('data/train.parquet')
            test_df = pd.read_parquet('data/test.parquet')

            if sample_ratio < 1.0:
                train_df = train_df.sample(frac=sample_ratio, random_state=42)
                print(f"âœ… ìƒ˜í”Œë§ ì™„ë£Œ: {len(train_df):,}í–‰")

            return self.preprocess_data(train_df, test_df)

    def load_data_in_batches(self, file_path, batch_size=500000):
        """ë°°ì¹˜ë³„ ì•ˆì „í•œ ë°ì´í„° ë¡œë”©"""
        print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸° {batch_size:,}í–‰ìœ¼ë¡œ ì•ˆì „ ë¡œë”©...")

        # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ êµ¬ì¡° í™•ì¸
        first_batch = pd.read_parquet(file_path, engine='pyarrow').head(batch_size)
        print(f"âœ… ì²« ë°°ì¹˜ ë¡œë“œ: {first_batch.shape}")

        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ì „ì²´ ë¡œë“œ
        test_df = pd.read_parquet('data/test.parquet')

        return self.preprocess_data(first_batch, test_df)

    def preprocess_data(self, train_df, test_df):
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        print("ğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")

        # ê¸°ë³¸ íŠ¹ì„± ì„ íƒ
        feature_cols = []

        # ìˆ˜ì¹˜í˜• íŠ¹ì„±
        numeric_cols = [col for col in train_df.columns
                       if col.startswith(('feat_', 'history_')) and train_df[col].dtype in ['float64', 'int64']]

        print(f"ğŸ“Š ìˆ˜ì¹˜í˜• íŠ¹ì„±: {len(numeric_cols)}ê°œ")

        # ê°„ë‹¨í•œ ì „ì²˜ë¦¬
        for col in tqdm(numeric_cols[:50], desc="ìˆ˜ì¹˜ ì „ì²˜ë¦¬"):  # ìƒìœ„ 50ê°œë§Œ
            if col in train_df.columns:
                mean_val = train_df[col].mean()
                std_val = train_df[col].std()

                # ê²°ì¸¡ê°’ ì²˜ë¦¬
                train_df[col] = train_df[col].fillna(mean_val)
                test_df[col] = test_df[col].fillna(mean_val)

                # ì •ê·œí™”
                if std_val > 0:
                    train_df[col] = (train_df[col] - mean_val) / std_val
                    test_df[col] = (test_df[col] - mean_val) / std_val

                feature_cols.append(col)

        # ì¹´í…Œê³ ë¦¬ íŠ¹ì„± (ê°„ë‹¨í•˜ê²Œ)
        if 'gender' in train_df.columns:
            from sklearn.preprocessing import LabelEncoder
            le = LabelEncoder()
            combined = pd.concat([train_df['gender'].fillna('unknown'),
                                test_df['gender'].fillna('unknown')])
            le.fit(combined.astype(str))
            train_df['gender_encoded'] = le.transform(train_df['gender'].fillna('unknown').astype(str))
            test_df['gender_encoded'] = le.transform(test_df['gender'].fillna('unknown').astype(str))
            feature_cols.append('gender_encoded')

        # ì‹œí€€ìŠ¤ íŠ¹ì„± ì²˜ë¦¬
        sequence_features = self.process_sequences(train_df, test_df)

        # ê¸°ë³¸ íŠ¹ì„±ê³¼ ì‹œí€€ìŠ¤ íŠ¹ì„± ê²°í•©
        X_train = np.column_stack([
            train_df[feature_cols].values,
            sequence_features['train']
        ])
        X_test = np.column_stack([
            test_df[feature_cols].values,
            sequence_features['test']
        ])

        y_train = train_df['clicked'].values

        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: í›ˆë ¨ {X_train.shape}, í…ŒìŠ¤íŠ¸ {X_test.shape}")

        return {
            'X_train': X_train,
            'X_test': X_test,
            'y_train': y_train,
            'test_df': test_df
        }

    def process_sequences(self, train_df, test_df):
        """ì‹œí€€ìŠ¤ íŠ¹ì„± ì²˜ë¦¬"""
        print("ğŸ”„ ì‹œí€€ìŠ¤ íŠ¹ì„± ì²˜ë¦¬...")

        # ê°„ë‹¨í•œ ì‹œí€€ìŠ¤ ìƒì„±
        train_sequences = []
        test_sequences = []

        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë“¤ì„ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
        sequence_cols = [col for col in train_df.columns if col.startswith('feat_')][:20]  # ìƒìœ„ 20ê°œ

        for _, row in tqdm(train_df.iterrows(), total=len(train_df), desc="í›ˆë ¨ ì‹œí€€ìŠ¤"):
            seq = [int(abs(row[col]) * 1000) % 50000 for col in sequence_cols if pd.notna(row[col])]
            seq = seq[:50] + [0] * (50 - len(seq))  # ê¸¸ì´ 50ìœ¼ë¡œ íŒ¨ë”©
            train_sequences.append(seq)

        for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc="í…ŒìŠ¤íŠ¸ ì‹œí€€ìŠ¤"):
            seq = [int(abs(row[col]) * 1000) % 50000 for col in sequence_cols if pd.notna(row[col])]
            seq = seq[:50] + [0] * (50 - len(seq))  # ê¸¸ì´ 50ìœ¼ë¡œ íŒ¨ë”©
            test_sequences.append(seq)

        train_sequences = np.array(train_sequences)
        test_sequences = np.array(test_sequences)

        # PyTorch ëª¨ë¸ ìƒì„± ë° í›ˆë ¨
        self.sequence_processor.create_model()
        if self.sequence_processor.model is not None:
            self.sequence_processor.train_model(train_sequences)

        # ì‹œí€€ìŠ¤ íŠ¹ì„± ì¶”ì¶œ
        train_seq_features = self.sequence_processor.extract_features(train_sequences)
        test_seq_features = self.sequence_processor.extract_features(test_sequences)

        return {
            'train': train_seq_features,
            'test': test_seq_features
        }

    def train_xgboost(self, data):
        """XGBoost ëª¨ë¸ í›ˆë ¨"""
        print("ğŸš€ XGBoost ëª¨ë¸ í›ˆë ¨...")

        X_train, X_val, y_train, y_val = train_test_split(
            data['X_train'], data['y_train'],
            test_size=0.2, random_state=42, stratify=data['y_train']
        )

        # XGBoost ë°ì´í„°ì…‹
        dtrain = xgb.DMatrix(X_train, label=y_train)
        dval = xgb.DMatrix(X_val, label=y_val)

        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        params = {
            'objective': 'binary:logistic',
            'eval_metric': 'logloss',
            'max_depth': 6,
            'learning_rate': 0.1,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42,
            'tree_method': 'hist'  # Mac ìµœì í™”
        }

        # í›ˆë ¨
        self.model = xgb.train(
            params,
            dtrain,
            num_boost_round=100,
            evals=[(dtrain, 'train'), (dval, 'val')],
            early_stopping_rounds=10,
            verbose_eval=20
        )

        # ê²€ì¦ ì„±ëŠ¥
        val_pred = self.model.predict(dval)
        comp_score, ap, wll = calculate_competition_score(y_val, val_pred)

        print(f"\nğŸ“Š ê²€ì¦ ì„±ëŠ¥:")
        print(f"   ëŒ€íšŒ ì ìˆ˜: {comp_score:.4f}")
        print(f"   AP: {ap:.4f}")
        print(f"   WLL: {wll:.4f}")

        return True

    def predict_and_submit(self, data):
        """ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±"""
        print("ğŸ¯ ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±...")

        # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
        dtest = xgb.DMatrix(data['X_test'])
        predictions = self.model.predict(dtest)

        # ì œì¶œ íŒŒì¼ ìƒì„±
        try:
            submission = pd.read_csv('data/sample_submission.csv')
            submission['clicked'] = predictions
            print(f"âœ… ì˜¬ë°”ë¥¸ ID í˜•ì‹: {submission['ID'].iloc[0]}")
        except:
            submission = pd.DataFrame({
                'ID': [f'TEST_{i:07d}' for i in range(len(predictions))],
                'clicked': predictions
            })

        submission_path = 'submission_mac_xgboost_simple.csv'
        submission.to_csv(submission_path, index=False, encoding='utf-8')

        print(f"\nâœ… ì œì¶œ íŒŒì¼ ìƒì„±: {submission_path}")
        print(f"ğŸ“Š ì˜ˆì¸¡ í†µê³„:")
        print(f"   í‰ê·  í´ë¦­ë¥ : {predictions.mean():.4f}")
        print(f"   ìµœì†Œê°’: {predictions.min():.4f}")
        print(f"   ìµœëŒ€ê°’: {predictions.max():.4f}")

        return submission_path

    def run_pipeline(self, mode=1):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""

        # ëª¨ë“œë³„ ì„¤ì •
        if mode == 1:
            sample_ratio = 0.3
            use_batch = False
            print(f"\nğŸš€ ì´ˆê³ ì† ëª¨ë“œ (30% ìƒ˜í”Œë§)")
        elif mode == 2:
            sample_ratio = 0.5
            use_batch = False
            print(f"\nâš¡ ë¹ ë¥¸ ëª¨ë“œ (50% ìƒ˜í”Œë§)")
        elif mode == 3:
            sample_ratio = 0.7
            use_batch = False
            print(f"\nğŸ¯ ì •í™• ëª¨ë“œ (70% ìƒ˜í”Œë§)")
        elif mode == 4:
            sample_ratio = 1.0
            use_batch = False
            print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë“œ (ì „ì²´ ë°ì´í„°)")
        else:  # mode == 5
            sample_ratio = 1.0
            use_batch = True
            print(f"\nğŸ›¡ï¸ ì•ˆì „ ìµœê³  ì„±ëŠ¥ ëª¨ë“œ (ë°°ì¹˜ ì²˜ë¦¬)")

        try:
            # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
            data = self.load_and_preprocess(sample_ratio, use_batch)

            # 2. XGBoost í›ˆë ¨
            if not self.train_xgboost(data):
                return False

            # 3. ì˜ˆì¸¡ ë° ì œì¶œ
            submission_path = self.predict_and_submit(data)

            print(f"\nğŸ‰ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            print(f"ğŸ“ ì œì¶œ íŒŒì¼: {submission_path}")

            return True

        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
            return False

def main():
    print("ğŸš€ Macìš© XGBoost + ê°„ë‹¨í•œ LSTM CTR ì˜ˆì¸¡!")
    print("ğŸ“Š ëŒ€íšŒ í‰ê°€ì§€í‘œ: AP (50%) + WLL (50%)")
    print("ğŸ§  ì‹œí€€ìŠ¤: ê°„ë‹¨í•œ LSTM (Attention ì œì™¸), í…Œì´ë¸”: XGBoost")
    print("=" * 60)

    pipeline = MacXGBoostSimple()

    print("ğŸ“‹ ì‹¤í–‰ ì˜µì…˜:")
    print("1. ğŸš€ ì´ˆê³ ì† ëª¨ë“œ (30% ìƒ˜í”Œë§) - 1-2ë¶„")
    print("2. âš¡ ë¹ ë¥¸ ëª¨ë“œ (50% ìƒ˜í”Œë§) - 2-3ë¶„")
    print("3. ğŸ¯ ì •í™• ëª¨ë“œ (70% ìƒ˜í”Œë§) - 4-5ë¶„")
    print("4. ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë“œ (ì „ì²´ ë°ì´í„° ì§ì ‘) - 5-8ë¶„ âš ï¸ ë©”ëª¨ë¦¬ ìœ„í—˜")
    print("5. ğŸ›¡ï¸ ì•ˆì „ ìµœê³  ì„±ëŠ¥ ëª¨ë“œ (ì „ì²´ ë°ì´í„° ë°°ì¹˜) - 8-12ë¶„ âœ… ë©”ëª¨ë¦¬ ì•ˆì „")

    choice = input("ì„ íƒ (1-5, ê¸°ë³¸ê°’ 1): ").strip() or '1'

    success = pipeline.run_pipeline(int(choice))

    if success:
        print("\nğŸ‰ ì„±ê³µ!")
    else:
        print("\nâŒ ì‹¤íŒ¨")

if __name__ == "__main__":
    main()