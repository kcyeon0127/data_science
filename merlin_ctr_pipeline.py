#!/usr/bin/env python3
"""
NVIDIA Merlin ê¸°ë°˜ CTR ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸
Attention ë©”ì»¤ë‹ˆì¦˜ í¬í•¨ + ìµœì¢… ì œì¶œ CSV ìƒì„±
"""

import os
import pandas as pd
import numpy as np
from tqdm.auto import tqdm
import warnings
warnings.filterwarnings('ignore')

# Merlin ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import cudf
    import nvtabular as nvt
    from nvtabular import ops
    import merlin.models.tf as mm
    from merlin.io import Dataset
    from merlin.schema import Tags
    import tensorflow as tf

    print("âœ… Merlin ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ")
    MERLIN_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ Merlin ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ: {e}")
    print("ğŸ’¡ ì„¤ì¹˜ ëª…ë ¹ì–´: pip install merlin-dataloader merlin-models nvtabular cudf-cu11")
    MERLIN_AVAILABLE = False

class MerlinCTRPipeline:
    def __init__(self, data_dir='data'):
        self.data_dir = data_dir
        self.train_path = os.path.join(data_dir, 'train.parquet')
        self.test_path = os.path.join(data_dir, 'test.parquet')
        self.model = None
        self.workflow = None

        # GPU ì„¤ì •
        if MERLIN_AVAILABLE:
            try:
                # GPU ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •
                gpus = tf.config.experimental.list_physical_devices('GPU')
                if gpus:
                    tf.config.experimental.set_memory_growth(gpus[0], True)
                    print("âœ… GPU ë©”ëª¨ë¦¬ ì„¤ì • ì™„ë£Œ")
            except:
                print("âš ï¸ GPU ì„¤ì • ì‹¤íŒ¨, CPU ì‚¬ìš©")

    def load_data(self):
        """ë°ì´í„° ë¡œë“œ (Merlin ìµœì í™”)"""
        print("ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")

        if not os.path.exists(self.train_path):
            print(f"âŒ {self.train_path} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False

        try:
            if MERLIN_AVAILABLE:
                # CuDFë¡œ ë¡œë“œ (GPU ê°€ì†)
                self.train_df = cudf.read_parquet(self.train_path)
                self.test_df = cudf.read_parquet(self.test_path)
                print(f"âœ… GPU ê°€ì† ë¡œë“œ: í›ˆë ¨ {self.train_df.shape}, í…ŒìŠ¤íŠ¸ {self.test_df.shape}")
            else:
                # Pandas í´ë°±
                self.train_df = pd.read_parquet(self.train_path)
                self.test_df = pd.read_parquet(self.test_path)
                print(f"âœ… CPU ë¡œë“œ: í›ˆë ¨ {self.train_df.shape}, í…ŒìŠ¤íŠ¸ {self.test_df.shape}")

            return True

        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def create_nvtabular_workflow(self):
        """NVTabular ì „ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° ìƒì„±"""
        print("ğŸ”§ ì „ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° ìƒì„± ì¤‘...")

        # ì¹´í…Œê³ ë¦¬ ì»¬ëŸ¼ë“¤
        categorical_cols = ['gender', 'age_group', 'inventory_id', 'l_feat_14']

        # ì—°ì†í˜• ì»¬ëŸ¼ë“¤
        continuous_cols = [col for col in self.train_df.columns
                          if col.startswith(('feat_', 'history_')) and col not in categorical_cols]

        # ì‹œí€€ìŠ¤ ì»¬ëŸ¼
        sequence_cols = ['seq'] if 'seq' in self.train_df.columns else []

        print(f"ğŸ“Š ì»¬ëŸ¼ ì •ë³´:")
        print(f"  ì¹´í…Œê³ ë¦¬: {len(categorical_cols)}ê°œ")
        print(f"  ì—°ì†í˜•: {len(continuous_cols)}ê°œ")
        print(f"  ì‹œí€€ìŠ¤: {len(sequence_cols)}ê°œ")

        # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
        cat_features = categorical_cols >> ops.Categorify(out_dtype="int32")
        cont_features = continuous_cols >> ops.FillMissing() >> ops.Normalize()

        # ì‹œí€€ìŠ¤ ì²˜ë¦¬ (ìˆëŠ” ê²½ìš°)
        if sequence_cols:
            # ì‹œí€€ìŠ¤ë¥¼ ê³ ì • ê¸¸ì´ë¡œ íŒ¨ë”©
            seq_features = sequence_cols >> ops.ListSlice(0, 50) >> ops.Categorify(out_dtype="int32")
            workflow_ops = cat_features + cont_features + seq_features
        else:
            workflow_ops = cat_features + cont_features

        # íƒ€ê²Ÿ ì»¬ëŸ¼
        target = ['clicked'] >> ops.LambdaOp(lambda x: x.astype('float32'))

        self.workflow = nvt.Workflow(workflow_ops + target)

        return True

    def preprocess_data(self):
        """ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰"""
        print("âš¡ GPU ê°€ì† ì „ì²˜ë¦¬ ì‹œì‘...")

        try:
            # í›ˆë ¨ ë°ì´í„° ì „ì²˜ë¦¬
            train_dataset = Dataset(self.train_df)
            self.workflow.fit(train_dataset)

            train_processed = self.workflow.transform(train_dataset)
            test_dataset = Dataset(self.test_df)
            test_processed = self.workflow.transform(test_dataset)

            print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ")

            return train_processed, test_processed

        except Exception as e:
            print(f"âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None, None

    def create_attention_model(self, schema):
        """Attention ê¸°ë°˜ CTR ëª¨ë¸ ìƒì„±"""
        print("ğŸ§  Attention ê¸°ë°˜ ëª¨ë¸ ìƒì„± ì¤‘...")

        # ì…ë ¥ ë¸”ë¡
        inputs = mm.InputBlockV2(schema)

        # ì„ë² ë”© ë ˆì´ì–´
        embeddings = mm.EmbeddingFeatures(inputs)

        # Multi-Head Attention ë¸”ë¡
        attention_output = mm.Block([
            mm.MLPBlock([512, 256], activation="relu", dropout=0.2),
            # ì‹œí€€ìŠ¤ê°€ ìˆëŠ” ê²½ìš° Attention ì ìš©
            mm.SequenceEmbeddingFeatures(
                aggregation="concat",
                schema=schema
            ) if any("seq" in col.name for col in schema) else mm.MLPBlock([256]),
        ])(embeddings)

        # Multi-Head Self-Attention (ì»¤ìŠ¤í…€)
        attention_layer = tf.keras.layers.MultiHeadAttention(
            num_heads=8,
            key_dim=64,
            dropout=0.1,
            name="multi_head_attention"
        )

        # Attention ì ìš© (3D í…ì„œê°€ í•„ìš”í•˜ë¯€ë¡œ reshape)
        reshaped = tf.keras.layers.Reshape((-1, 1))(attention_output)
        attended = attention_layer(reshaped, reshaped)
        flattened = tf.keras.layers.Flatten()(attended)

        # ìµœì¢… ì˜ˆì¸¡ ë ˆì´ì–´
        dense_layers = mm.MLPBlock([256, 128, 64], activation="relu", dropout=0.3)(flattened)

        # ì´ì§„ ë¶„ë¥˜ íƒœìŠ¤í¬
        predictions = mm.BinaryClassificationTask("clicked")(dense_layers)

        # ëª¨ë¸ ìƒì„±
        model = mm.Model(inputs, predictions)

        return model

    def train_model(self, train_dataset, valid_dataset=None):
        """ëª¨ë¸ í›ˆë ¨"""
        print("ğŸš€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")

        try:
            schema = train_dataset.schema

            # Attention ëª¨ë¸ ìƒì„±
            self.model = self.create_attention_model(schema)

            # ì»´íŒŒì¼
            self.model.compile(
                optimizer='adam',
                run_eagerly=False,
                metrics=[
                    tf.keras.metrics.AUC(name='auc'),
                    tf.keras.metrics.Precision(name='precision'),
                    tf.keras.metrics.Recall(name='recall')
                ]
            )

            print("ğŸ“‹ ëª¨ë¸ êµ¬ì¡°:")
            self.model.summary()

            # ì½œë°± ì„¤ì •
            callbacks = [
                tf.keras.callbacks.EarlyStopping(
                    monitor='val_loss' if valid_dataset else 'loss',
                    patience=3,
                    restore_best_weights=True
                ),
                tf.keras.callbacks.ReduceLROnPlateau(
                    monitor='val_loss' if valid_dataset else 'loss',
                    factor=0.5,
                    patience=2
                )
            ]

            # í›ˆë ¨
            history = self.model.fit(
                train_dataset,
                validation_data=valid_dataset,
                epochs=10,
                batch_size=4096,
                callbacks=callbacks,
                verbose=1
            )

            print("âœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
            return history

        except Exception as e:
            print(f"âŒ ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            return None

    def predict_and_submit(self, test_dataset):
        """ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±"""
        print("ğŸ¯ ì˜ˆì¸¡ ì‹œì‘...")

        try:
            # ì˜ˆì¸¡
            predictions = self.model.predict(test_dataset, batch_size=8192)

            # ì œì¶œ íŒŒì¼ í˜•íƒœë¡œ ë³€í™˜
            if hasattr(predictions, 'numpy'):
                pred_probs = predictions.numpy().flatten()
            else:
                pred_probs = predictions.flatten()

            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ID ê°€ì ¸ì˜¤ê¸°
            if hasattr(self.test_df, 'to_pandas'):
                test_ids = self.test_df.to_pandas().index
            else:
                test_ids = self.test_df.index

            # ì œì¶œ íŒŒì¼ ìƒì„±
            submission = pd.DataFrame({
                'id': test_ids,
                'clicked': pred_probs
            })

            # ì €ì¥
            submission_path = 'submission_merlin_attention.csv'
            submission.to_csv(submission_path, index=False)

            print(f"âœ… ì œì¶œ íŒŒì¼ ìƒì„±: {submission_path}")
            print(f"ğŸ“Š ì˜ˆì¸¡ í†µê³„:")
            print(f"  í‰ê·  í´ë¦­ë¥ : {pred_probs.mean():.4f}")
            print(f"  ìµœì†Œê°’: {pred_probs.min():.4f}")
            print(f"  ìµœëŒ€ê°’: {pred_probs.max():.4f}")

            return submission_path

        except Exception as e:
            print(f"âŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return None

    def run_pipeline(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("ğŸš€ Merlin CTR íŒŒì´í”„ë¼ì¸ ì‹œì‘!")

        # 1. ë°ì´í„° ë¡œë“œ
        if not self.load_data():
            return False

        # 2. ì „ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° ìƒì„±
        if not self.create_nvtabular_workflow():
            return False

        # 3. ë°ì´í„° ì „ì²˜ë¦¬
        train_processed, test_processed = self.preprocess_data()
        if train_processed is None:
            return False

        # 4. Train/Validation ë¶„í• 
        train_len = len(train_processed)
        val_len = int(train_len * 0.2)

        valid_processed = train_processed.to_ddf().tail(val_len)
        train_processed = train_processed.to_ddf().head(train_len - val_len)

        print(f"ğŸ“Š ë°ì´í„° ë¶„í• : í›ˆë ¨ {train_len - val_len:,}, ê²€ì¦ {val_len:,}")

        # 5. ëª¨ë¸ í›ˆë ¨
        history = self.train_model(train_processed, valid_processed)
        if history is None:
            return False

        # 6. ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±
        submission_path = self.predict_and_submit(test_processed)
        if submission_path is None:
            return False

        print("ğŸ‰ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        return True

def fallback_baseline():
    """Merlinì´ ì—†ì„ ë•Œì˜ ê°„ë‹¨í•œ ë² ì´ìŠ¤ë¼ì¸"""
    print("ğŸ”„ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ë¡œ í´ë°±...")

    try:
        # ë°ì´í„° ë¡œë“œ
        train_df = pd.read_parquet('data/train.parquet')
        test_df = pd.read_parquet('data/test.parquet')

        print(f"ğŸ“Š ë°ì´í„° í¬ê¸°: í›ˆë ¨ {train_df.shape}, í…ŒìŠ¤íŠ¸ {test_df.shape}")

        # ê°„ë‹¨í•œ íŠ¹ì„± ì„ íƒ
        numeric_cols = [col for col in train_df.columns if col.startswith('feat_')]

        X = train_df[numeric_cols].fillna(0)
        y = train_df['clicked']
        X_test = test_df[numeric_cols].fillna(0)

        # ê°„ë‹¨í•œ ëª¨ë¸ í›ˆë ¨
        from sklearn.linear_model import LogisticRegression
        from sklearn.preprocessing import StandardScaler

        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        X_test_scaled = scaler.transform(X_test)

        model = LogisticRegression(random_state=42, max_iter=1000)
        model.fit(X_scaled, y)

        # ì˜ˆì¸¡
        predictions = model.predict_proba(X_test_scaled)[:, 1]

        # ì œì¶œ íŒŒì¼
        submission = pd.DataFrame({
            'id': test_df.index,
            'clicked': predictions
        })

        submission.to_csv('submission_baseline.csv', index=False)
        print("âœ… ë² ì´ìŠ¤ë¼ì¸ ì œì¶œ íŒŒì¼ ìƒì„±: submission_baseline.csv")

        return True

    except Exception as e:
        print(f"âŒ ë² ì´ìŠ¤ë¼ì¸ ì‹¤íŒ¨: {e}")
        return False

def main():
    if not MERLIN_AVAILABLE:
        print("ğŸ’¡ Merlinì„ ì„¤ì¹˜í•˜ë©´ GPU ê°€ì†ê³¼ Attention ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
        print("ğŸ“ ì„¤ì¹˜ ë°©ë²•:")
        print("  conda install -c nvidia -c rapidsai -c conda-forge cudf nvtabular")
        print("  pip install merlin-models")
        print()

        choice = input("ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ë¡œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
        if choice.lower() == 'y':
            return fallback_baseline()
        else:
            print("Merlin ì„¤ì¹˜ í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return False

    # Merlin íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = MerlinCTRPipeline()
    success = pipeline.run_pipeline()

    if success:
        print("ğŸ‰ Merlin CTR íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print("ğŸ“ ìƒì„±ëœ íŒŒì¼:")
        print("  - submission_merlin_attention.csv (ìµœì¢… ì œì¶œ íŒŒì¼)")
    else:
        print("âŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨")
        print("ğŸ’¡ ë² ì´ìŠ¤ë¼ì¸ìœ¼ë¡œ í´ë°± ì‹œë„...")
        fallback_baseline()

if __name__ == "__main__":
    main()