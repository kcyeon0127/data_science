#!/usr/bin/env python3
"""
CTR ì˜ˆì¸¡ ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ì²´í¬í¬ì¸íŠ¸ ê¸°ëŠ¥ìœ¼ë¡œ ì•ˆì „í•œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
"""

import os
import pandas as pd
import numpy as np
from tqdm.auto import tqdm
import gc
from ctr_preprocessing import CTRDataPreprocessor
from checkpoint_manager import CheckpointManager

def prepare_preprocessor(train_path, checkpoint_manager, chunk_size=50000, force_retrain=False):
    """ì „ì²˜ë¦¬ê¸° ì¤€ë¹„ (ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ í•™ìŠµ)"""

    if not force_retrain:
        # ê¸°ì¡´ ì „ì²˜ë¦¬ê¸° ë¡œë“œ ì‹œë„
        preprocessor = checkpoint_manager.load_preprocessor()
        if preprocessor is not None:
            print("âœ… ê¸°ì¡´ ì „ì²˜ë¦¬ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return preprocessor

    print("ğŸ”§ ìƒˆë¡œìš´ ì „ì²˜ë¦¬ê¸°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤...")

    # ìƒˆ ì „ì²˜ë¦¬ê¸° ìƒì„±
    preprocessor = CTRDataPreprocessor()

    print("\n=== 1ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ ìŠ¤ìº” ===")
    scan_success = preprocessor.scan_categorical_values(train_path, chunk_size=chunk_size)

    if not scan_success:
        print("âš ï¸ ì¹´í…Œê³ ë¦¬ ìŠ¤ìº” ì‹¤íŒ¨")
        return None

    print("\n=== 2ë‹¨ê³„: ì „ì²˜ë¦¬ê¸° í•™ìŠµ ===")
    # ì ì ˆí•œ í¬ê¸°ì˜ ìƒ˜í”Œë¡œ ì „ì²˜ë¦¬ê¸° í•™ìŠµ (ì²­í¬ í¬ê¸°ì— ë¹„ë¡€)
    sample_size = min(50000, chunk_size * 2)  # ì²­í¬ í¬ê¸°ì˜ 2ë°° ë˜ëŠ” ìµœëŒ€ 50k
    print(f"ğŸ“Š í•™ìŠµ ìƒ˜í”Œ í¬ê¸°: {sample_size:,}í–‰")

    sample_df = pd.read_parquet(train_path)
    sample_df = sample_df.head(sample_size)

    with tqdm(total=6, desc="ì „ì²˜ë¦¬ê¸° í•™ìŠµ") as pbar:
        sample_processed = preprocessor.preprocess_pipeline(
            sample_df,
            is_training=True,
            target_col='clicked',
            pbar=pbar
        )

    print(f"ì „ì²˜ë¦¬ê¸° í•™ìŠµ ì™„ë£Œ! ê²°ê³¼: {sample_processed.shape}")

    # ì „ì²˜ë¦¬ê¸° ì €ì¥
    checkpoint_manager.save_preprocessor(preprocessor)

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del sample_df, sample_processed
    gc.collect()

    return preprocessor

def process_chunks_with_checkpoint(train_path, preprocessor, checkpoint_manager, chunk_size=100000):
    """ì²´í¬í¬ì¸íŠ¸ ê¸°ëŠ¥ìœ¼ë¡œ ì²­í¬ ì²˜ë¦¬"""

    # ì§„í–‰ ìƒí™© í™•ì¸
    progress = checkpoint_manager.load_progress()

    # ì „ì²´ ë°ì´í„° í¬ê¸° í™•ì¸ (íŒŒì¼ í¬ë§·ì— ë”°ë¼)
    if train_path.endswith('.csv'):
        # CSV: í–‰ ìˆ˜ íš¨ìœ¨ì  ê³„ì‚°
        try:
            import subprocess
            result = subprocess.run(['wc', '-l', train_path], capture_output=True, text=True)
            total_rows = int(result.stdout.split()[0]) - 1  # í—¤ë” ì œì™¸
            print(f"ğŸ“Š ì´ ë°ì´í„° í–‰ ìˆ˜: {total_rows:,}í–‰ (CSV)")
        except:
            # í´ë°±: pandasë¡œ í™•ì¸
            temp_df = pd.read_csv(train_path, nrows=1)  # í—¤ë”ë§Œ ì½ê¸°
            with open(train_path, 'r') as f:
                total_rows = sum(1 for _ in f) - 1  # í—¤ë” ì œì™¸
            print(f"ğŸ“Š ì´ ë°ì´í„° í–‰ ìˆ˜: {total_rows:,}í–‰ (CSV ì¹´ìš´íŠ¸)")
    else:
        # Parquet: ê¸°ì¡´ ë°©ì‹
        import pyarrow.parquet as pq
        try:
            parquet_file = pq.ParquetFile(train_path)
            total_rows = parquet_file.metadata.num_rows
            print(f"ğŸ“Š ì´ ë°ì´í„° í–‰ ìˆ˜: {total_rows:,}í–‰ (Parquet ë©”íƒ€ë°ì´í„°)")
        except:
            temp_df = pd.read_parquet(train_path)
            total_rows = len(temp_df)
            del temp_df
            gc.collect()
            print(f"ğŸ“Š ì´ ë°ì´í„° í–‰ ìˆ˜: {total_rows:,}í–‰ (Parquet ì „ì²´ë¡œë“œ)")

    total_chunks = (total_rows // chunk_size) + 1

    # ì‹œì‘ ì§€ì  ê²°ì • ë° ì²­í¬ í¬ê¸° í˜¸í™˜ì„± í™•ì¸
    if progress:
        old_chunk_size = progress.get('chunk_size', chunk_size)
        if old_chunk_size != chunk_size:
            print(f"âš ï¸ ì²­í¬ í¬ê¸° ë³€ê²½ ê°ì§€: {old_chunk_size:,} â†’ {chunk_size:,}")
            print(f"ğŸ”„ ìƒˆë¡œìš´ ì²­í¬ í¬ê¸°ë¡œ ì²˜ìŒë¶€í„° ì¬ì‹œì‘í•©ë‹ˆë‹¤.")
            start_chunk = 0
            # ê¸°ì¡´ ì²­í¬ íŒŒì¼ë“¤ê³¼ ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ ë°±ì—…
            import time
            backup_dir = f"checkpoints_backup_{int(time.time())}"
            import shutil
            if os.path.exists(checkpoint_manager.checkpoint_dir):
                shutil.copytree(checkpoint_manager.checkpoint_dir, backup_dir)
                print(f"ğŸ“¦ ê¸°ì¡´ ì²­í¬ë“¤ì„ {backup_dir}ì— ë°±ì—…í–ˆìŠµë‹ˆë‹¤.")
        else:
            start_chunk = progress['current_chunk']
            print(f"ğŸ”„ ì²­í¬ {start_chunk}ë¶€í„° ì¬ì‹œì‘í•©ë‹ˆë‹¤. (ì²­í¬ í¬ê¸°: {chunk_size:,})")
    else:
        start_chunk = 0
        print(f"ğŸš€ ì²˜ìŒë¶€í„° ì²­í¬ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

    print(f"ì´ {total_chunks}ê°œ ì²­í¬ ì¤‘ {start_chunk}ë¶€í„° ì²˜ë¦¬")

    # ì²­í¬ë³„ ì²˜ë¦¬
    with tqdm(total=total_chunks, initial=start_chunk, desc="ì²­í¬ ì²˜ë¦¬") as pbar:
        for chunk_idx in range(start_chunk, total_chunks):
            try:
                # ì²­í¬ ë²”ìœ„ ê³„ì‚°
                start_row = chunk_idx * chunk_size
                end_row = min(start_row + chunk_size, total_rows)

                # ì´ë¯¸ ì²˜ë¦¬ëœ ì²­í¬ì¸ì§€ í™•ì¸
                chunk_file = os.path.join(checkpoint_manager.checkpoint_dir, f'chunk_{chunk_idx:04d}.parquet')

                if os.path.exists(chunk_file):
                    pbar.set_postfix_str(f"ì²­í¬ {chunk_idx} ìŠ¤í‚µ (ì´ë¯¸ ì²˜ë¦¬ë¨)")
                    pbar.update(1)
                    continue

                # ì²­í¬ ë¡œë“œ (íŒŒì¼ í¬ë§·ì— ë”°ë¼ ë‹¤ë¥¸ ë°©ì‹)
                try:
                    if train_path.endswith('.csv'):
                        # CSV: ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²­í¬ ë¡œë“œ
                        chunk_df = pd.read_csv(
                            train_path,
                            skiprows=range(1, start_row + 1) if start_row > 0 else None,
                            nrows=chunk_size,
                            low_memory=False
                        )

                        if len(chunk_df) == 0:
                            print(f"âš ï¸ ì²­í¬ {chunk_idx}: ë¹ˆ ì²­í¬, ì™„ë£Œ")
                            break

                        print(f"ğŸ“Š ì²­í¬ {chunk_idx} ë¡œë“œ ì™„ë£Œ: {len(chunk_df):,}í–‰ (CSV ìŠ¤íŠ¸ë¦¬ë°)")

                    else:
                        # Parquet: ì „ì²´ íŒŒì¼ ë¡œë“œ (ê¸°ì¡´ ë°©ì‹)
                        print(f"âš ï¸ Parquet ë°©ì‹: ì „ì²´ íŒŒì¼ ë¡œë“œ ì¤‘...")

                        # ë©”ëª¨ë¦¬ ì²´í¬
                        import psutil
                        memory_before = psutil.virtual_memory().percent
                        if memory_before > 70:
                            print(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë†’ìŒ ({memory_before:.1f}%) - CSV ë³€í™˜ ê¶Œì¥")

                        full_df = pd.read_parquet(train_path)

                        # ì²­í¬ ë²”ìœ„ í™•ì¸
                        if start_row >= len(full_df):
                            print(f"âš ï¸ ì²­í¬ {chunk_idx}: ë²”ìœ„ ì´ˆê³¼, ìŠ¤í‚µ")
                            del full_df
                            break

                        actual_end = min(end_row, len(full_df))
                        chunk_df = full_df.iloc[start_row:actual_end].copy()

                        # ì¦‰ì‹œ ì „ì²´ ë°ì´í„° ì‚­ì œ
                        del full_df
                        gc.collect()

                        if len(chunk_df) == 0:
                            print(f"âš ï¸ ì²­í¬ {chunk_idx}: ë¹ˆ ì²­í¬, ìŠ¤í‚µ")
                            continue

                        print(f"ğŸ“Š ì²­í¬ {chunk_idx} ë¡œë“œ ì™„ë£Œ: {len(chunk_df):,}í–‰ (Parquet ì „ì²´ë¡œë“œ)")

                except Exception as e:
                    print(f"âŒ ì²­í¬ {chunk_idx} ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue

                # ì „ì²˜ë¦¬ ì ìš© (ìƒì„¸ ë©”ëª¨ë¦¬ ì¶”ì )
                try:
                    import psutil
                    process = psutil.Process()

                    memory_before = psutil.virtual_memory().percent
                    process_memory_before = process.memory_info().rss / 1024 / 1024  # MB

                    processed_chunk = preprocessor.preprocess_pipeline(
                        chunk_df,
                        is_training=False,
                        target_col='clicked'
                    )

                    memory_after = psutil.virtual_memory().percent
                    process_memory_after = process.memory_info().rss / 1024 / 1024  # MB

                    memory_diff = process_memory_after - process_memory_before

                    print(f"ğŸ” ì²­í¬ {chunk_idx} ë©”ëª¨ë¦¬: {process_memory_before:.1f}MB â†’ {process_memory_after:.1f}MB (ì°¨ì´: {memory_diff:+.1f}MB)")

                    if memory_after > 85:
                        print(f"âš ï¸ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë†’ìŒ: {memory_after:.1f}%")

                    if memory_diff > 100:  # 100MB ì´ìƒ ì¦ê°€ì‹œ ëˆ„ìˆ˜ ì˜ì‹¬
                        print(f"ğŸš¨ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì˜ì‹¬! ì²­í¬ë‹¹ {memory_diff:.1f}MB ì¦ê°€")

                except Exception as e:
                    print(f"âŒ ì²­í¬ {chunk_idx} ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    # ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ì¬ì‹œë„
                    del chunk_df
                    gc.collect()
                    continue

                # ì²­í¬ ì €ì¥
                processed_chunk.to_parquet(chunk_file, compression='snappy')

                # ì§„í–‰ ìƒí™© ì €ì¥
                progress_info = {
                    'current_chunk': chunk_idx + 1,
                    'total_chunks': total_chunks,
                    'total_rows': total_rows,
                    'chunk_size': chunk_size
                }
                checkpoint_manager.save_progress(progress_info)

                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del chunk_df, processed_chunk
                gc.collect()

                pbar.set_postfix_str(f"ì²­í¬ {chunk_idx}: {end_row-start_row:,}í–‰ ì™„ë£Œ")
                pbar.update(1)

            except Exception as e:
                print(f"\nâŒ ì²­í¬ {chunk_idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                print(f"ğŸ’¡ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì¬ì‹œì‘ ê°€ëŠ¥: python run_preprocessing.py --resume")
                return False

    print("âœ… ëª¨ë“  ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ!")
    return True

def combine_chunks(checkpoint_manager):
    """ì²˜ë¦¬ëœ ì²­í¬ë“¤ì„ ê²°í•©"""

    print("\n=== ì²­í¬ ê²°í•© ===")
    chunk_files = checkpoint_manager.list_processed_chunks()

    if not chunk_files:
        print("âŒ ì²˜ë¦¬ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False

    print(f"ğŸ“ {len(chunk_files)}ê°œ ì²­í¬ë¥¼ ê²°í•©í•©ë‹ˆë‹¤.")

    # ë°°ì¹˜ë³„ë¡œ ê²°í•© (ë©”ëª¨ë¦¬ ì•ˆì „)
    batch_size = 5
    combined_parts = []

    for i in range(0, len(chunk_files), batch_size):
        batch_files = chunk_files[i:i+batch_size]
        print(f"ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘... ({len(batch_files)}ê°œ)")

        batch_chunks = []
        for file in tqdm(batch_files, desc="ì²­í¬ ë¡œë“œ"):
            chunk = pd.read_parquet(file)
            batch_chunks.append(chunk)

        # ë°°ì¹˜ ê²°í•©
        batch_combined = pd.concat(batch_chunks, ignore_index=True)

        # ë°°ì¹˜ ì €ì¥
        batch_file = os.path.join(checkpoint_manager.checkpoint_dir, f'batch_{i//batch_size}.parquet')
        batch_combined.to_parquet(batch_file, compression='snappy')
        combined_parts.append(batch_file)

        del batch_chunks, batch_combined
        gc.collect()

    # ìµœì¢… ê²°í•© (ë©”ëª¨ë¦¬ ì•ˆì „)
    print("ìµœì¢… ê²°í•© ì¤‘...")
    if len(combined_parts) == 1:
        # ë°°ì¹˜ê°€ í•˜ë‚˜ë¿ì´ë©´ ë°”ë¡œ ë¡œë“œ
        final_data = pd.read_parquet(combined_parts[0])
    else:
        # ì—¬ëŸ¬ ë°°ì¹˜ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ê²°í•© (ë©”ëª¨ë¦¬ ì•ˆì „)
        final_data = None
        for i, part_file in enumerate(tqdm(combined_parts, desc="ë°°ì¹˜ ìˆœì°¨ ê²°í•©")):
            chunk = pd.read_parquet(part_file)
            if final_data is None:
                final_data = chunk
            else:
                final_data = pd.concat([final_data, chunk], ignore_index=True)
                del chunk
                gc.collect()

                # ë©”ëª¨ë¦¬ ì²´í¬
                import psutil
                memory_percent = psutil.virtual_memory().percent
                if memory_percent > 85:
                    print(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë†’ìŒ: {memory_percent:.1f}%")

    print(f"ìµœì¢… ë°ì´í„° í¬ê¸°: {final_data.shape}")
    return final_data

def main():
    import argparse
    parser = argparse.ArgumentParser(description='CTR ì˜ˆì¸¡ ë°ì´í„° ì „ì²˜ë¦¬')
    parser.add_argument('--resume', action='store_true', help='ì¤‘ê°„ë¶€í„° ì¬ì‹œì‘')
    parser.add_argument('--retrain', action='store_true', help='ì „ì²˜ë¦¬ê¸° ì¬í•™ìŠµ')
    parser.add_argument('--chunk-size', type=int, default=100000, help='ì²­í¬ í¬ê¸°')
    parser.add_argument('--data-path', default='data/train.parquet', help='ë°ì´í„° íŒŒì¼ ê²½ë¡œ')
    args = parser.parse_args()

    print("ğŸ”„ CTR ì˜ˆì¸¡ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")

    # ë©”ëª¨ë¦¬ ì²´í¬ ë° ì²­í¬ í¬ê¸° ìë™ ì¡°ì •
    import psutil
    available_memory_gb = psutil.virtual_memory().available / (1024**3)
    print(f"ğŸ’¾ ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {available_memory_gb:.1f}GB")

    # ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•˜ë©´ ì²­í¬ í¬ê¸° ìë™ ê°ì†Œ
    if available_memory_gb < 2 and args.chunk_size > 10000:
        suggested_chunk_size = 10000
        print(f"âš ï¸ ë©”ëª¨ë¦¬ ë¶€ì¡± ê°ì§€! ì²­í¬ í¬ê¸°ë¥¼ {suggested_chunk_size:,}ë¡œ ì¡°ì •í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        print(f"í˜„ì¬ ì„¤ì •: {args.chunk_size:,} â†’ ê¶Œì¥: {suggested_chunk_size:,}")
    elif available_memory_gb < 4 and args.chunk_size > 25000:
        suggested_chunk_size = 25000
        print(f"âš ï¸ ë©”ëª¨ë¦¬ ì œí•œì ! ì²­í¬ í¬ê¸°ë¥¼ {suggested_chunk_size:,}ë¡œ ì¡°ì •í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        print(f"í˜„ì¬ ì„¤ì •: {args.chunk_size:,} â†’ ê¶Œì¥: {suggested_chunk_size:,}")

    # ê²½ë¡œ ì„¤ì •
    checkpoint_manager = CheckpointManager()

    if not os.path.exists(args.data_path):
        print(f"âŒ {args.data_path} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    try:
        # 1. ì „ì²˜ë¦¬ê¸° ì¤€ë¹„
        print("\nğŸ”§ ì „ì²˜ë¦¬ê¸° ì¤€ë¹„...")
        preprocessor = prepare_preprocessor(
            args.data_path,
            checkpoint_manager,
            chunk_size=args.chunk_size,
            force_retrain=args.retrain
        )

        if preprocessor is None:
            print("âŒ ì „ì²˜ë¦¬ê¸° ì¤€ë¹„ ì‹¤íŒ¨")
            return

        # 2. ì²­í¬ ì²˜ë¦¬
        print("\nğŸ“¦ ì²­í¬ ì²˜ë¦¬...")
        success = process_chunks_with_checkpoint(
            args.data_path,
            preprocessor,
            checkpoint_manager,
            chunk_size=args.chunk_size
        )

        if not success:
            print("âŒ ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨")
            return

        # 3. ì²­í¬ ê²°í•© ë° ë¶„í• 
        print("\nğŸ”— ì²­í¬ ê²°í•©...")
        combined_data = combine_chunks(checkpoint_manager)

        if combined_data is None:
            print("âŒ ì²­í¬ ê²°í•© ì‹¤íŒ¨")
            return

        # 4. Train/Val ë¶„í• 
        print("\nâœ‚ï¸ ë°ì´í„° ë¶„í• ...")
        feature_cols = [col for col in combined_data.columns if col != 'clicked']
        X = combined_data[feature_cols]
        y = combined_data['clicked']

        from sklearn.model_selection import train_test_split
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        # 5. ê²°ê³¼ ì €ì¥
        print("\nğŸ’¾ ê²°ê³¼ ì €ì¥...")
        os.makedirs('processed_data', exist_ok=True)

        save_files = [
            ("X_train", X_train),
            ("X_val", X_val),
            ("y_train", y_train),
            ("y_val", y_val)
        ]

        for name, data in tqdm(save_files, desc="ì €ì¥"):
            data.to_parquet(f'processed_data/{name}.parquet')

        print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ğŸ“Š ê²°ê³¼: í›ˆë ¨ {X_train.shape}, ê²€ì¦ {X_val.shape}")

        # ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬ ì—¬ë¶€ ë¬»ê¸°
        response = input("\nğŸ—‘ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤ì„ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
        if response.lower() == 'y':
            checkpoint_manager.clear_checkpoints()

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("\nğŸ’¡ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì¬ì‹œì‘ ê°€ëŠ¥:")
        print("python run_preprocessing.py --resume")

if __name__ == "__main__":
    main()