#!/usr/bin/env python3
"""
ì²´í¬í¬ì¸íŠ¸ ê¸°ëŠ¥ì´ ìˆëŠ” CTR ì˜ˆì¸¡ ë°ì´í„° ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
ì „ì²˜ë¦¬ê¸° ì €ì¥/ë¡œë“œ ë° ì¤‘ê°„ ì¬ì‹œì‘ ê¸°ëŠ¥ í¬í•¨
"""

import os
import pandas as pd
import numpy as np
from tqdm.auto import tqdm
import gc
import pickle
import json
from ctr_preprocessing import CTRDataPreprocessor
from preprocessing_utils import CTRPreprocessingUtils

class CheckpointManager:
    def __init__(self, checkpoint_dir='checkpoints'):
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)

    def save_preprocessor(self, preprocessor, filename='preprocessor.pkl'):
        """ì „ì²˜ë¦¬ê¸° ì €ì¥"""
        filepath = os.path.join(self.checkpoint_dir, filename)
        with open(filepath, 'wb') as f:
            pickle.dump(preprocessor, f)
        print(f"âœ… ì „ì²˜ë¦¬ê¸° ì €ì¥: {filepath}")

    def load_preprocessor(self, filename='preprocessor.pkl'):
        """ì „ì²˜ë¦¬ê¸° ë¡œë“œ"""
        filepath = os.path.join(self.checkpoint_dir, filename)
        if os.path.exists(filepath):
            with open(filepath, 'rb') as f:
                preprocessor = pickle.load(f)
            print(f"âœ… ì „ì²˜ë¦¬ê¸° ë¡œë“œ: {filepath}")
            return preprocessor
        return None

    def save_progress(self, progress_info):
        """ì§„í–‰ ìƒí™© ì €ì¥"""
        filepath = os.path.join(self.checkpoint_dir, 'progress.json')
        with open(filepath, 'w') as f:
            json.dump(progress_info, f, indent=2)
        print(f"ğŸ“Š ì§„í–‰ìƒí™© ì €ì¥: {progress_info['current_chunk']}/{progress_info['total_chunks']}")

    def load_progress(self):
        """ì§„í–‰ ìƒí™© ë¡œë“œ"""
        filepath = os.path.join(self.checkpoint_dir, 'progress.json')
        if os.path.exists(filepath):
            with open(filepath, 'r') as f:
                progress = json.load(f)
            print(f"ğŸ“Š ì§„í–‰ìƒí™© ë¡œë“œ: {progress['current_chunk']}/{progress['total_chunks']}")
            return progress
        return None

    def list_processed_chunks(self):
        """ì²˜ë¦¬ëœ ì²­í¬ íŒŒì¼ ëª©ë¡"""
        chunk_files = []
        for file in os.listdir(self.checkpoint_dir):
            if file.startswith('chunk_') and file.endswith('.parquet'):
                chunk_files.append(os.path.join(self.checkpoint_dir, file))
        return sorted(chunk_files)

def prepare_preprocessor(train_path, checkpoint_manager, chunk_size=50000, force_retrain=False):
    """ì „ì²˜ë¦¬ê¸° ì¤€ë¹„ (ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ í•™ìŠµ)"""

    if not force_retrain:
        # ê¸°ì¡´ ì „ì²˜ë¦¬ê¸° ë¡œë“œ ì‹œë„
        preprocessor = checkpoint_manager.load_preprocessor()
        if preprocessor is not None:
            print("âœ… ê¸°ì¡´ ì „ì²˜ë¦¬ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return preprocessor

    print("ğŸ”§ ìƒˆë¡œìš´ ì „ì²˜ë¦¬ê¸°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤...")

    # ìƒˆ ì „ì²˜ë¦¬ê¸° ìƒì„±
    preprocessor = CTRDataPreprocessor()

    print("\n=== 1ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ ìŠ¤ìº” ===")
    scan_success = preprocessor.scan_categorical_values(train_path, chunk_size=chunk_size)

    if not scan_success:
        print("âš ï¸ ì¹´í…Œê³ ë¦¬ ìŠ¤ìº” ì‹¤íŒ¨")
        return None

    print("\n=== 2ë‹¨ê³„: ì „ì²˜ë¦¬ê¸° í•™ìŠµ ===")
    # ì‘ì€ ìƒ˜í”Œë¡œ ì „ì²˜ë¦¬ê¸° í•™ìŠµ
    sample_df = pd.read_parquet(train_path)
    sample_df = sample_df.head(10000)

    with tqdm(total=6, desc="ì „ì²˜ë¦¬ê¸° í•™ìŠµ") as pbar:
        sample_processed = preprocessor.preprocess_pipeline(
            sample_df,
            is_training=True,
            target_col='clicked',
            pbar=pbar
        )

    print(f"ì „ì²˜ë¦¬ê¸° í•™ìŠµ ì™„ë£Œ! ê²°ê³¼: {sample_processed.shape}")

    # ì „ì²˜ë¦¬ê¸° ì €ì¥
    checkpoint_manager.save_preprocessor(preprocessor)

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del sample_df, sample_processed
    gc.collect()

    return preprocessor

def process_chunks_with_checkpoint(train_path, preprocessor, checkpoint_manager, chunk_size=100000):
    """ì²´í¬í¬ì¸íŠ¸ ê¸°ëŠ¥ìœ¼ë¡œ ì²­í¬ ì²˜ë¦¬"""

    # ì§„í–‰ ìƒí™© í™•ì¸
    progress = checkpoint_manager.load_progress()

    # ì „ì²´ ë°ì´í„° í¬ê¸° í™•ì¸
    temp_df = pd.read_parquet(train_path)
    total_rows = len(temp_df)
    total_chunks = (total_rows // chunk_size) + 1
    del temp_df
    gc.collect()

    # ì‹œì‘ ì§€ì  ê²°ì •
    if progress:
        start_chunk = progress['current_chunk']
        print(f"ğŸ”„ ì²­í¬ {start_chunk}ë¶€í„° ì¬ì‹œì‘í•©ë‹ˆë‹¤.")
    else:
        start_chunk = 0
        print(f"ğŸš€ ì²˜ìŒë¶€í„° ì²­í¬ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

    print(f"ì´ {total_chunks}ê°œ ì²­í¬ ì¤‘ {start_chunk}ë¶€í„° ì²˜ë¦¬")

    # ì²­í¬ë³„ ì²˜ë¦¬
    with tqdm(total=total_chunks, initial=start_chunk, desc="ì²­í¬ ì²˜ë¦¬") as pbar:
        for chunk_idx in range(start_chunk, total_chunks):
            try:
                # ì²­í¬ ë²”ìœ„ ê³„ì‚°
                start_row = chunk_idx * chunk_size
                end_row = min(start_row + chunk_size, total_rows)

                # ì´ë¯¸ ì²˜ë¦¬ëœ ì²­í¬ì¸ì§€ í™•ì¸
                chunk_file = os.path.join(checkpoint_manager.checkpoint_dir, f'chunk_{chunk_idx:04d}.parquet')

                if os.path.exists(chunk_file):
                    pbar.set_postfix_str(f"ì²­í¬ {chunk_idx} ìŠ¤í‚µ (ì´ë¯¸ ì²˜ë¦¬ë¨)")
                    pbar.update(1)
                    continue

                # ì²­í¬ ë¡œë“œ
                full_df = pd.read_parquet(train_path)
                chunk_df = full_df.iloc[start_row:end_row].copy()
                del full_df
                gc.collect()

                # ì „ì²˜ë¦¬ ì ìš©
                processed_chunk = preprocessor.preprocess_pipeline(
                    chunk_df,
                    is_training=False,
                    target_col='clicked'
                )

                # ì²­í¬ ì €ì¥
                processed_chunk.to_parquet(chunk_file, compression='snappy')

                # ì§„í–‰ ìƒí™© ì €ì¥
                progress_info = {
                    'current_chunk': chunk_idx + 1,
                    'total_chunks': total_chunks,
                    'total_rows': total_rows,
                    'chunk_size': chunk_size
                }
                checkpoint_manager.save_progress(progress_info)

                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del chunk_df, processed_chunk
                gc.collect()

                memory_mb = chunk_df.memory_usage(deep=True).sum() / 1024 / 1024 if 'chunk_df' in locals() else 0
                pbar.set_postfix_str(f"ì²­í¬ {chunk_idx}: {end_row-start_row:,}í–‰ ì™„ë£Œ")
                pbar.update(1)

            except Exception as e:
                print(f"\nâŒ ì²­í¬ {chunk_idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                print(f"ğŸ’¡ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì¬ì‹œì‘ ê°€ëŠ¥: python run_preprocessing_checkpoint.py --resume")
                return False

    print("âœ… ëª¨ë“  ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ!")
    return True

def combine_chunks(checkpoint_manager):
    """ì²˜ë¦¬ëœ ì²­í¬ë“¤ì„ ê²°í•©"""

    print("\n=== ì²­í¬ ê²°í•© ===")
    chunk_files = checkpoint_manager.list_processed_chunks()

    if not chunk_files:
        print("âŒ ì²˜ë¦¬ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False

    print(f"ğŸ“ {len(chunk_files)}ê°œ ì²­í¬ë¥¼ ê²°í•©í•©ë‹ˆë‹¤.")

    # ë°°ì¹˜ë³„ë¡œ ê²°í•© (ë©”ëª¨ë¦¬ ì•ˆì „)
    batch_size = 5
    combined_parts = []

    for i in range(0, len(chunk_files), batch_size):
        batch_files = chunk_files[i:i+batch_size]
        print(f"ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘... ({len(batch_files)}ê°œ)")

        batch_chunks = []
        for file in tqdm(batch_files, desc="ì²­í¬ ë¡œë“œ"):
            chunk = pd.read_parquet(file)
            batch_chunks.append(chunk)

        # ë°°ì¹˜ ê²°í•©
        batch_combined = pd.concat(batch_chunks, ignore_index=True)

        # ë°°ì¹˜ ì €ì¥
        batch_file = os.path.join(checkpoint_manager.checkpoint_dir, f'batch_{i//batch_size}.parquet')
        batch_combined.to_parquet(batch_file, compression='snappy')
        combined_parts.append(batch_file)

        del batch_chunks, batch_combined
        gc.collect()

    # ìµœì¢… ê²°í•©
    print("ìµœì¢… ê²°í•© ì¤‘...")
    final_chunks = []
    for part_file in tqdm(combined_parts, desc="ë°°ì¹˜ ë¡œë“œ"):
        chunk = pd.read_parquet(part_file)
        final_chunks.append(chunk)

    final_data = pd.concat(final_chunks, ignore_index=True)
    print(f"ìµœì¢… ë°ì´í„° í¬ê¸°: {final_data.shape}")

    return final_data

def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--resume', action='store_true', help='ì¤‘ê°„ë¶€í„° ì¬ì‹œì‘')
    parser.add_argument('--retrain', action='store_true', help='ì „ì²˜ë¦¬ê¸° ì¬í•™ìŠµ')
    parser.add_argument('--chunk-size', type=int, default=100000, help='ì²­í¬ í¬ê¸°')
    args = parser.parse_args()

    print("ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ê¸°ëŠ¥ì´ ìˆëŠ” CTR ì „ì²˜ë¦¬ ì‹œì‘")

    # ê²½ë¡œ ì„¤ì •
    train_path = 'data/train.parquet'
    checkpoint_manager = CheckpointManager()

    if not os.path.exists(train_path):
        print(f"âŒ {train_path} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    try:
        # 1. ì „ì²˜ë¦¬ê¸° ì¤€ë¹„
        print("\nğŸ”§ ì „ì²˜ë¦¬ê¸° ì¤€ë¹„...")
        preprocessor = prepare_preprocessor(
            train_path,
            checkpoint_manager,
            chunk_size=args.chunk_size//2,
            force_retrain=args.retrain
        )

        if preprocessor is None:
            print("âŒ ì „ì²˜ë¦¬ê¸° ì¤€ë¹„ ì‹¤íŒ¨")
            return

        # 2. ì²­í¬ ì²˜ë¦¬
        print("\nğŸ“¦ ì²­í¬ ì²˜ë¦¬...")
        success = process_chunks_with_checkpoint(
            train_path,
            preprocessor,
            checkpoint_manager,
            chunk_size=args.chunk_size
        )

        if not success:
            print("âŒ ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨")
            return

        # 3. ì²­í¬ ê²°í•© ë° ë¶„í• 
        print("\nğŸ”— ì²­í¬ ê²°í•©...")
        combined_data = combine_chunks(checkpoint_manager)

        if combined_data is None:
            print("âŒ ì²­í¬ ê²°í•© ì‹¤íŒ¨")
            return

        # 4. Train/Val ë¶„í• 
        print("\nâœ‚ï¸ ë°ì´í„° ë¶„í• ...")
        feature_cols = [col for col in combined_data.columns if col != 'clicked']
        X = combined_data[feature_cols]
        y = combined_data['clicked']

        from sklearn.model_selection import train_test_split
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        # 5. ê²°ê³¼ ì €ì¥
        print("\nğŸ’¾ ê²°ê³¼ ì €ì¥...")
        os.makedirs('processed_data', exist_ok=True)

        save_files = [
            ("X_train", X_train),
            ("X_val", X_val),
            ("y_train", y_train),
            ("y_val", y_val)
        ]

        for name, data in tqdm(save_files, desc="ì €ì¥"):
            data.to_parquet(f'processed_data/{name}.parquet')

        print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ğŸ“Š ê²°ê³¼: í›ˆë ¨ {X_train.shape}, ê²€ì¦ {X_val.shape}")

        # ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬ ì—¬ë¶€ ë¬»ê¸°
        response = input("\nğŸ—‘ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤ì„ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
        if response.lower() == 'y':
            import shutil
            shutil.rmtree(checkpoint_manager.checkpoint_dir)
            print("âœ… ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì‚­ì œ ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("\nğŸ’¡ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì¬ì‹œì‘ ê°€ëŠ¥:")
        print("python run_preprocessing_checkpoint.py --resume")

if __name__ == "__main__":
    main()