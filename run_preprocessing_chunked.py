#!/usr/bin/env python3
"""
ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ CTR ì˜ˆì¸¡ ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
í° ë°ì´í„°ì…‹ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì…ë‹ˆë‹¤.
"""

import os
import pandas as pd
import numpy as np
from tqdm.auto import tqdm
import gc
from ctr_preprocessing import CTRDataPreprocessor
from preprocessing_utils import CTRPreprocessingUtils

def process_data_in_chunks(file_path, chunk_size=100000, preprocessor=None, is_training=True, target_col='clicked'):
    """ë°ì´í„°ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ - ë©”ëª¨ë¦¬ ì•ˆì „í•œ ë°©ì‹"""

    print(f"ì²­í¬ ë°©ì‹ìœ¼ë¡œ ë°ì´í„° ì²˜ë¦¬: ì²­í¬ í¬ê¸° {chunk_size:,}í–‰")

    processed_chunks = []
    chunk_count = 0
    temp_files = []

    try:
        # íŒŒì¼ í¬ê¸°ë§Œ ë¨¼ì € í™•ì¸ (ì „ì²´ ë¡œë“œ ì—†ì´)
        sample_df = pd.read_parquet(file_path, nrows=1000)
        file_size = os.path.getsize(file_path)
        sample_memory = sample_df.memory_usage(deep=True).sum()
        estimated_rows = int((file_size / sample_memory) * 1000)

        del sample_df
        gc.collect()

        num_chunks = (estimated_rows // chunk_size) + 1
        print(f"ì˜ˆìƒ {estimated_rows:,}í–‰ì„ {chunk_size:,}ê°œì”© {num_chunks}ê°œ ì²­í¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

        # íŒŒì¼ì„ ì‘ì€ ë‹¨ìœ„ë¡œ ì½ì–´ê°€ë©° ì²˜ë¦¬
        with tqdm(total=num_chunks, desc=f"ì²­í¬ ì²˜ë¦¬ ({chunk_size:,}í–‰ì”©)") as pbar:
            offset = 0

            while True:
                try:
                    # ì²­í¬ ë‹¨ìœ„ë¡œ ì½ê¸°
                    chunk = pd.read_parquet(file_path)

                    # ì‹¤ì œ ì²­í¬ ë¶„í• 
                    start_idx = offset
                    end_idx = min(offset + chunk_size, len(chunk))

                    if start_idx >= len(chunk):
                        break

                    current_chunk = chunk.iloc[start_idx:end_idx].copy()
                    del chunk  # ì¦‰ì‹œ ë©”ëª¨ë¦¬ í•´ì œ
                    gc.collect()

                    chunk_count += 1

                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
                    memory_mb = current_chunk.memory_usage(deep=True).sum() / 1024 / 1024
                    pbar.set_postfix_str(f"ì²­í¬ {chunk_count}, ë©”ëª¨ë¦¬: {memory_mb:.1f}MB")

                    # ì „ì²˜ë¦¬ ì ìš©
                    if preprocessor:
                        chunk_processed = preprocessor.preprocess_pipeline(
                            current_chunk,
                            is_training=is_training,
                            target_col=target_col
                        )
                    else:
                        chunk_processed = current_chunk.copy()

                    # ì²˜ë¦¬ëœ ì²­í¬ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)
                    temp_file = f'temp_chunk_{chunk_count}.parquet'
                    chunk_processed.to_parquet(temp_file, compression='snappy')
                    temp_files.append(temp_file)

                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    del current_chunk, chunk_processed
                    gc.collect()

                    offset += chunk_size
                    pbar.update(1)

                    # ì¤‘ê°„ ì§„í–‰ ìƒí™© ì¶œë ¥
                    if chunk_count % 10 == 0:
                        print(f"\nì²­í¬ {chunk_count}ê°œ ì²˜ë¦¬ ì™„ë£Œ, ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
                        gc.collect()

                except Exception as e:
                    print(f"\nì²­í¬ {chunk_count} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    break

        # ì„ì‹œ íŒŒì¼ë“¤ì„ ë‹¤ì‹œ ë¡œë“œí•˜ì—¬ processed_chunks ìƒì„±
        print(f"\nì„ì‹œ íŒŒì¼ë“¤ì„ ë¡œë“œ ì¤‘... ({len(temp_files)}ê°œ)")
        for temp_file in tqdm(temp_files, desc="ì²­í¬ ë¡œë“œ"):
            try:
                chunk_data = pd.read_parquet(temp_file)
                processed_chunks.append(chunk_data)
            except Exception as e:
                print(f"ì„ì‹œ íŒŒì¼ {temp_file} ë¡œë“œ ì˜¤ë¥˜: {e}")

        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        for temp_file in temp_files:
            try:
                os.remove(temp_file)
            except:
                pass

        print(f"ì´ {len(processed_chunks)}ê°œ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ")
        return processed_chunks

    except Exception as e:
        print(f"ì²­í¬ ì²˜ë¦¬ ì „ì²´ ì˜¤ë¥˜: {e}")

        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        for temp_file in temp_files:
            try:
                os.remove(temp_file)
            except:
                pass

        return []

def process_data_simple_chunks(file_path, chunk_size, preprocessor, is_training, target_col):
    """ê°„ë‹¨í•œ ì²­í¬ ì²˜ë¦¬ (PyArrow ì—†ì„ ë•Œ)"""
    print("ê°„ë‹¨í•œ ì²­í¬ ì²˜ë¦¬ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤...")

    # ì „ì²´ ë°ì´í„° ë¡œë“œ í›„ ë¶„í• 
    print("ì „ì²´ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
    full_df = pd.read_parquet(file_path)
    total_rows = len(full_df)

    print(f"ì´ {total_rows:,}í–‰ì„ {chunk_size:,}ê°œì”© ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    processed_chunks = []
    num_chunks = (total_rows // chunk_size) + (1 if total_rows % chunk_size else 0)

    with tqdm(total=num_chunks, desc=f"ì²­í¬ ì²˜ë¦¬ ({chunk_size:,}í–‰ì”©)") as pbar:
        for i in range(0, total_rows, chunk_size):
            try:
                end_idx = min(i + chunk_size, total_rows)
                chunk = full_df.iloc[i:end_idx].copy()

                memory_mb = chunk.memory_usage(deep=True).sum() / 1024 / 1024
                pbar.set_postfix_str(f"ë©”ëª¨ë¦¬: {memory_mb:.1f}MB, í–‰: {len(chunk):,}")

                # ì „ì²˜ë¦¬ ì ìš©
                if preprocessor:
                    chunk_processed = preprocessor.preprocess_pipeline(
                        chunk,
                        is_training=is_training,
                        target_col=target_col
                    )
                else:
                    chunk_processed = chunk

                processed_chunks.append(chunk_processed)

            except Exception as e:
                print(f"ì²­í¬ {i//chunk_size + 1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

            pbar.update(1)

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del chunk
            if 'chunk_processed' in locals():
                del chunk_processed
            gc.collect()

    # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ ì‚­ì œ
    del full_df
    gc.collect()

    return processed_chunks

def combine_chunks_efficiently(chunks, output_path):
    """ì²­í¬ë“¤ì„ ë°°ì¹˜ë³„ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ê²°í•©"""
    print(f"ì²­í¬ ê²°í•© ì¤‘... ì´ {len(chunks)}ê°œ ì²­í¬")

    if len(chunks) == 0:
        raise ValueError("ê²°í•©í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

    if len(chunks) == 1:
        return chunks[0]

    batch_size = 5  # í•œë²ˆì— 5ê°œì”© ê²°í•©
    temp_files = []

    try:
        # ë°°ì¹˜ë³„ ê²°í•©
        with tqdm(total=(len(chunks) // batch_size) + 1, desc="ë°°ì¹˜ë³„ ê²°í•©") as pbar:
            for i in range(0, len(chunks), batch_size):
                batch_chunks = chunks[i:i+batch_size]

                pbar.set_postfix_str(f"ë°°ì¹˜ {i//batch_size + 1}: {len(batch_chunks)}ê°œ ì²­í¬")

                # ë°°ì¹˜ ë‚´ ì²­í¬ë“¤ ê²°í•©
                if len(batch_chunks) == 1:
                    batch_combined = batch_chunks[0]
                else:
                    batch_combined = pd.concat(batch_chunks, ignore_index=True)

                # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                temp_file = output_path.replace('.parquet', f'_batch_{i//batch_size}.parquet')
                batch_combined.to_parquet(temp_file, compression='snappy')
                temp_files.append(temp_file)

                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del batch_chunks, batch_combined
                gc.collect()

                pbar.update(1)

        print(f"\në°°ì¹˜ë³„ ê²°í•© ì™„ë£Œ. {len(temp_files)}ê°œ ë°°ì¹˜ íŒŒì¼ ìƒì„±.")

        # ìµœì¢… ê²°í•©
        print("ìµœì¢… ë°°ì¹˜ íŒŒì¼ë“¤ì„ ê²°í•© ì¤‘...")
        final_chunks = []

        with tqdm(total=len(temp_files), desc="ìµœì¢… ê²°í•©") as pbar:
            for temp_file in temp_files:
                chunk = pd.read_parquet(temp_file)
                final_chunks.append(chunk)
                pbar.set_postfix_str(f"ë¡œë“œ: {len(chunk):,}í–‰")
                pbar.update(1)

        # ìµœì¢… ê²°í•©
        print("ì „ì²´ ë°ì´í„° ê²°í•© ì¤‘...")
        final_combined = pd.concat(final_chunks, ignore_index=True)

        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        print("ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì¤‘...")
        for temp_file in temp_files:
            try:
                os.remove(temp_file)
            except:
                pass

        print(f"ê²°í•© ì™„ë£Œ! ìµœì¢… í¬ê¸°: {final_combined.shape}")
        return final_combined

    except Exception as e:
        print(f"ì²­í¬ ê²°í•© ì¤‘ ì˜¤ë¥˜: {e}")

        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        for temp_file in temp_files:
            try:
                os.remove(temp_file)
            except:
                pass

        # í´ë°±: ì²« ë²ˆì§¸ ì²­í¬ë§Œ ë°˜í™˜
        print("í´ë°±: ì²« ë²ˆì§¸ ì²­í¬ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return chunks[0]

def main():
    print("ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ CTR ì˜ˆì¸¡ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
    print(f"í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬ë¥¼ í™•ì¸í•˜ì—¬ ì²­í¬ í¬ê¸°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.")

    # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ í™•ì¸
    import psutil
    available_memory_gb = psutil.virtual_memory().available / (1024**3)
    print(f"ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {available_memory_gb:.1f}GB")

    # ì²­í¬ í¬ê¸° ì„¤ì • (ì»¤ë§¨ë“œ ë¼ì¸ ì¸ìë‚˜ í™˜ê²½ ë³€ìˆ˜ë¡œ ì¡°ì • ê°€ëŠ¥)
    import sys
    if len(sys.argv) > 1:
        try:
            chunk_size = int(sys.argv[1])
            print(f"ğŸ“‹ ì‚¬ìš©ì ì§€ì • ì²­í¬ í¬ê¸°: {chunk_size:,}í–‰")
        except ValueError:
            chunk_size = 100000
            print("âš ï¸  ì˜ëª»ëœ ì²­í¬ í¬ê¸°ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    else:
        # ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ìë™ ì²­í¬ í¬ê¸° ì¡°ì •
        if available_memory_gb < 4:
            chunk_size = 50000
            print("âš ï¸  ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì‘ì€ ì²­í¬ í¬ê¸°ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        elif available_memory_gb < 8:
            chunk_size = 100000
            print("ğŸ”§ ì¤‘ê°„ ì²­í¬ í¬ê¸°ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        else:
            chunk_size = 200000
            print("ğŸš€ í° ì²­í¬ í¬ê¸°ë¡œ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    print(f"ğŸ’¾ ì²­í¬ í¬ê¸°: {chunk_size:,}í–‰")

    # ë°ì´í„° ê²½ë¡œ ì„¤ì •
    train_path = 'data/train.parquet'
    test_path = 'data/test.parquet'

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(train_path):
        print(f"ì˜¤ë¥˜: {train_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    if not os.path.exists(test_path):
        print(f"ê²½ê³ : {test_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í›ˆë ¨ ë°ì´í„°ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        test_path = None

    # ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    preprocessor = CTRDataPreprocessor()
    utils = CTRPreprocessingUtils()

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('processed_data', exist_ok=True)

    try:
        print("\n=== 1ë‹¨ê³„: ì „ì²´ ë°ì´í„° ì¹´í…Œê³ ë¦¬ ìŠ¤ìº” ===")
        # ì „ì²´ ë°ì´í„°ì˜ ì¹´í…Œê³ ë¦¬ ê°’ë“¤ì„ ë¯¸ë¦¬ ìŠ¤ìº”
        scan_success = preprocessor.scan_categorical_values(train_path, chunk_size=chunk_size//2)

        if not scan_success:
            print("âš ï¸  ì¹´í…Œê³ ë¦¬ ìŠ¤ìº” ì‹¤íŒ¨. ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

        print("\n=== 2ë‹¨ê³„: ìƒ˜í”Œ ë°ì´í„°ë¡œ ì „ì²˜ë¦¬ê¸° í•™ìŠµ ===")
        # ì‘ì€ ìƒ˜í”Œë¡œ ì „ì²˜ë¦¬ê¸° í•™ìŠµ (ì¸ì½”ë”, ìŠ¤ì¼€ì¼ëŸ¬ ë“±)
        sample_df = pd.read_parquet(train_path)
        sample_df = sample_df.head(10000)  # ì²« 10000í–‰ë§Œ ì‚¬ìš©
        print(f"ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ: {sample_df.shape}")

        # ìƒ˜í”Œë¡œ ì „ì²˜ë¦¬ê¸° í•™ìŠµ
        with tqdm(total=6, desc="ì „ì²˜ë¦¬ê¸° í•™ìŠµ") as pbar:
            sample_processed = preprocessor.preprocess_pipeline(
                sample_df,
                is_training=True,
                target_col='clicked',
                pbar=pbar
            )

        print(f"ì „ì²˜ë¦¬ê¸° í•™ìŠµ ì™„ë£Œ! ìƒ˜í”Œ ê²°ê³¼: {sample_processed.shape}")
        del sample_df, sample_processed
        gc.collect()

        print("\n=== 3ë‹¨ê³„: í›ˆë ¨ ë°ì´í„° ì²­í¬ ì²˜ë¦¬ ===")
        # ì „ì²´ í›ˆë ¨ ë°ì´í„° ì²˜ë¦¬
        train_chunks = process_data_in_chunks(
            train_path,
            chunk_size=chunk_size,
            preprocessor=preprocessor,
            is_training=False,  # ì´ë¯¸ í•™ìŠµëœ ì „ì²˜ë¦¬ê¸° ì‚¬ìš©
            target_col='clicked'
        )

        print("\n=== 4ë‹¨ê³„: í›ˆë ¨ ë°ì´í„° ê²°í•© ë° ë¶„í•  ===")
        # ì²­í¬ ê²°í•©
        train_combined = combine_chunks_efficiently(train_chunks, 'processed_data/train_combined.parquet')

        # íƒ€ê²Ÿ ë¶„ë¦¬
        feature_cols = [col for col in train_combined.columns if col != 'clicked']
        X = train_combined[feature_cols]
        y = train_combined['clicked']

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del train_combined, train_chunks
        gc.collect()

        print("í›ˆë ¨/ê²€ì¦ ë¶„í•  ì¤‘...")
        from sklearn.model_selection import train_test_split

        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë¶„í• 
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        print(f"ë¶„í•  ì™„ë£Œ:")
        print(f"  í›ˆë ¨ ì„¸íŠ¸: {X_train.shape}")
        print(f"  ê²€ì¦ ì„¸íŠ¸: {X_val.shape}")

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del X, y
        gc.collect()

        print("\n=== 5ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ ===")
        X_test = None
        if test_path:
            test_chunks = process_data_in_chunks(
                test_path,
                chunk_size=chunk_size,
                preprocessor=preprocessor,
                is_training=False
            )

            X_test = combine_chunks_efficiently(test_chunks, 'processed_data/test_combined.parquet')

            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ íƒ€ê²Ÿ ì»¬ëŸ¼ ì œê±° (ìˆë‹¤ë©´)
            if 'clicked' in X_test.columns:
                X_test = X_test.drop('clicked', axis=1)

            print(f"  í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {X_test.shape}")
            del test_chunks
            gc.collect()

        print("\n=== 6ë‹¨ê³„: ê²°ê³¼ ì €ì¥ ===")
        save_files = [
            ("X_train", X_train),
            ("X_val", X_val),
            ("y_train", y_train),
            ("y_val", y_val)
        ]

        if X_test is not None:
            save_files.append(("X_test", X_test))

        with tqdm(total=len(save_files), desc="ë°ì´í„° ì €ì¥") as pbar:
            for name, data in save_files:
                try:
                    data.to_parquet(f'processed_data/{name}.parquet')
                    pbar.set_postfix_str(f"{name}: {data.shape}")
                    pbar.update(1)

                    # ì €ì¥ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
                    del data
                    gc.collect()

                except Exception as e:
                    print(f"{name} ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

        print("\n=== 7ë‹¨ê³„: í”¼ì²˜ ì •ë³´ ì €ì¥ ===")
        # í”¼ì²˜ ì •ë³´ ì €ì¥
        feature_info = {
            'total_features': len(feature_cols),
            'feature_names': feature_cols,
            'categorical_features': [col for col in feature_cols if col in ['gender', 'age_group', 'inventory_id', 'l_feat_14']],
            'numeric_features': [col for col in feature_cols if col.startswith(('feat_', 'history_'))],
            'engineered_features': [col for col in feature_cols if any(suffix in col for suffix in ['_sin', '_cos', '_log1p', '_sqrt', '_bin', '_enc', '_mult', '_add', '_ratio'])]
        }

        import json
        with open('processed_data/feature_info.json', 'w') as f:
            json.dump(feature_info, f, indent=2)

        print("\n" + "="*60)
        print("âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print("ìƒì„±ëœ íŒŒì¼ë“¤:")
        print("- processed_data/X_train.parquet")
        print("- processed_data/X_val.parquet")
        print("- processed_data/y_train.parquet")
        print("- processed_data/y_val.parquet")
        if X_test is not None:
            print("- processed_data/X_test.parquet")
        print("- processed_data/feature_info.json")
        print(f"ì´ {len(feature_cols)}ê°œì˜ í”¼ì²˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("="*60)

        return True

    except Exception as e:
        print(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ë©”ëª¨ë¦¬ ë¶€ì¡±ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ë” ì‘ì€ ì²­í¬ í¬ê¸°ë¡œ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.")
        return False

if __name__ == "__main__":
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
    import psutil
    process = psutil.Process()

    print(f"ì‹œì‘ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {process.memory_info().rss / 1024 / 1024:.1f}MB")

    success = main()

    if success:
        print(f"ìµœì¢… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {process.memory_info().rss / 1024 / 1024:.1f}MB")
    else:
        print("ì²˜ë¦¬ ì‹¤íŒ¨. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")