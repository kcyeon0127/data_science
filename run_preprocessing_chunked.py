#!/usr/bin/env python3
"""
ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ CTR ì˜ˆì¸¡ ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
í° ë°ì´í„°ì…‹ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì…ë‹ˆë‹¤.
"""

import os
import pandas as pd
import numpy as np
from tqdm.auto import tqdm
import gc
from ctr_preprocessing import CTRDataPreprocessor
from preprocessing_utils import CTRPreprocessingUtils

def process_data_in_chunks(file_path, chunk_size=100000, preprocessor=None, is_training=True, target_col='clicked'):
    """ë°ì´í„°ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬"""

    # ì „ì²´ í–‰ ìˆ˜ ë¨¼ì € í™•ì¸
    temp_df = pd.read_parquet(file_path, columns=['clicked'] if 'train' in file_path else [])
    total_rows = len(temp_df)
    del temp_df
    gc.collect()

    print(f"ì´ {total_rows:,}í–‰ì„ {chunk_size:,}ê°œì”© ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    processed_chunks = []

    # ì²­í¬ë³„ ì²˜ë¦¬
    with tqdm(total=total_rows//chunk_size + 1, desc=f"ì²­í¬ ì²˜ë¦¬ ({chunk_size:,}í–‰ì”©)") as pbar:
        for chunk in pd.read_parquet(file_path, chunksize=chunk_size):
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
            pbar.set_postfix_str(f"ë©”ëª¨ë¦¬: {chunk.memory_usage(deep=True).sum() / 1024 / 1024:.1f}MB")

            # ì „ì²˜ë¦¬ ì ìš©
            try:
                if preprocessor:
                    chunk_processed = preprocessor.preprocess_pipeline(
                        chunk,
                        is_training=is_training,
                        target_col=target_col
                    )
                else:
                    chunk_processed = chunk

                processed_chunks.append(chunk_processed)

            except Exception as e:
                print(f"ì²­í¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                # ì‹¤íŒ¨í•œ ì²­í¬ëŠ” ê¸°ë³¸ ì²˜ë¦¬ë§Œ ìˆ˜í–‰
                processed_chunks.append(chunk)

            pbar.update(1)

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del chunk
            gc.collect()

    return processed_chunks

def combine_chunks_efficiently(chunks, output_path):
    """ì²­í¬ë“¤ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê²°í•©"""
    print(f"ì²­í¬ ê²°í•© ì¤‘... ì´ {len(chunks)}ê°œ ì²­í¬")

    with tqdm(total=len(chunks), desc="ì²­í¬ ê²°í•©") as pbar:
        # ì²« ë²ˆì§¸ ì²­í¬ë¡œ ì‹œì‘
        combined = chunks[0].copy()
        pbar.update(1)

        # ë‚˜ë¨¸ì§€ ì²­í¬ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ê²°í•©
        for i, chunk in enumerate(chunks[1:], 1):
            try:
                combined = pd.concat([combined, chunk], ignore_index=True)
                pbar.set_postfix_str(f"í˜„ì¬ í¬ê¸°: {len(combined):,}í–‰")
                pbar.update(1)

                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del chunks[i-1]  # ì´ë¯¸ ì‚¬ìš©í•œ ì²­í¬ ì‚­ì œ
                if i % 5 == 0:  # 5ê°œë§ˆë‹¤ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                    gc.collect()

            except MemoryError:
                print(f"ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ {i}ë²ˆì§¸ ì²­í¬ì—ì„œ ì¤‘ê°„ ì €ì¥í•©ë‹ˆë‹¤.")
                # ì¤‘ê°„ ì €ì¥
                temp_path = output_path.replace('.parquet', f'_temp_{i}.parquet')
                combined.to_parquet(temp_path)
                del combined
                gc.collect()
                combined = chunk.copy()

    return combined

def main():
    print("ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ CTR ì˜ˆì¸¡ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
    print(f"í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬ë¥¼ í™•ì¸í•˜ì—¬ ì²­í¬ í¬ê¸°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.")

    # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ í™•ì¸
    import psutil
    available_memory_gb = psutil.virtual_memory().available / (1024**3)
    print(f"ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {available_memory_gb:.1f}GB")

    # ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ì²­í¬ í¬ê¸° ì¡°ì •
    if available_memory_gb < 4:
        chunk_size = 50000
        print("âš ï¸  ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì‘ì€ ì²­í¬ í¬ê¸°ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
    elif available_memory_gb < 8:
        chunk_size = 100000
        print("ğŸ”§ ì¤‘ê°„ ì²­í¬ í¬ê¸°ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
    else:
        chunk_size = 200000
        print("ğŸš€ í° ì²­í¬ í¬ê¸°ë¡œ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    # ë°ì´í„° ê²½ë¡œ ì„¤ì •
    train_path = 'data/train.parquet'
    test_path = 'data/test.parquet'

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(train_path):
        print(f"ì˜¤ë¥˜: {train_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    if not os.path.exists(test_path):
        print(f"ê²½ê³ : {test_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í›ˆë ¨ ë°ì´í„°ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        test_path = None

    # ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    preprocessor = CTRDataPreprocessor()
    utils = CTRPreprocessingUtils()

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('processed_data', exist_ok=True)

    try:
        print("\n=== 1ë‹¨ê³„: ìƒ˜í”Œ ë°ì´í„°ë¡œ ì „ì²˜ë¦¬ê¸° í•™ìŠµ ===")
        # ì‘ì€ ìƒ˜í”Œë¡œ ì „ì²˜ë¦¬ê¸° í•™ìŠµ (ì¸ì½”ë”, ìŠ¤ì¼€ì¼ëŸ¬ ë“±)
        sample_df = pd.read_parquet(train_path, nrows=10000)
        print(f"ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ: {sample_df.shape}")

        # ìƒ˜í”Œë¡œ ì „ì²˜ë¦¬ê¸° í•™ìŠµ
        with tqdm(total=6, desc="ì „ì²˜ë¦¬ê¸° í•™ìŠµ") as pbar:
            sample_processed = preprocessor.preprocess_pipeline(
                sample_df,
                is_training=True,
                target_col='clicked',
                pbar=pbar
            )

        print(f"ì „ì²˜ë¦¬ê¸° í•™ìŠµ ì™„ë£Œ! ìƒ˜í”Œ ê²°ê³¼: {sample_processed.shape}")
        del sample_df, sample_processed
        gc.collect()

        print("\n=== 2ë‹¨ê³„: í›ˆë ¨ ë°ì´í„° ì²­í¬ ì²˜ë¦¬ ===")
        # ì „ì²´ í›ˆë ¨ ë°ì´í„° ì²˜ë¦¬
        train_chunks = process_data_in_chunks(
            train_path,
            chunk_size=chunk_size,
            preprocessor=preprocessor,
            is_training=False,  # ì´ë¯¸ í•™ìŠµëœ ì „ì²˜ë¦¬ê¸° ì‚¬ìš©
            target_col='clicked'
        )

        print("\n=== 3ë‹¨ê³„: í›ˆë ¨ ë°ì´í„° ê²°í•© ë° ë¶„í•  ===")
        # ì²­í¬ ê²°í•©
        train_combined = combine_chunks_efficiently(train_chunks, 'processed_data/train_combined.parquet')

        # íƒ€ê²Ÿ ë¶„ë¦¬
        feature_cols = [col for col in train_combined.columns if col != 'clicked']
        X = train_combined[feature_cols]
        y = train_combined['clicked']

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del train_combined, train_chunks
        gc.collect()

        print("í›ˆë ¨/ê²€ì¦ ë¶„í•  ì¤‘...")
        from sklearn.model_selection import train_test_split

        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë¶„í• 
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        print(f"ë¶„í•  ì™„ë£Œ:")
        print(f"  í›ˆë ¨ ì„¸íŠ¸: {X_train.shape}")
        print(f"  ê²€ì¦ ì„¸íŠ¸: {X_val.shape}")

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del X, y
        gc.collect()

        print("\n=== 4ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ ===")
        X_test = None
        if test_path:
            test_chunks = process_data_in_chunks(
                test_path,
                chunk_size=chunk_size,
                preprocessor=preprocessor,
                is_training=False
            )

            X_test = combine_chunks_efficiently(test_chunks, 'processed_data/test_combined.parquet')

            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ íƒ€ê²Ÿ ì»¬ëŸ¼ ì œê±° (ìˆë‹¤ë©´)
            if 'clicked' in X_test.columns:
                X_test = X_test.drop('clicked', axis=1)

            print(f"  í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {X_test.shape}")
            del test_chunks
            gc.collect()

        print("\n=== 5ë‹¨ê³„: ê²°ê³¼ ì €ì¥ ===")
        save_files = [
            ("X_train", X_train),
            ("X_val", X_val),
            ("y_train", y_train),
            ("y_val", y_val)
        ]

        if X_test is not None:
            save_files.append(("X_test", X_test))

        with tqdm(total=len(save_files), desc="ë°ì´í„° ì €ì¥") as pbar:
            for name, data in save_files:
                try:
                    data.to_parquet(f'processed_data/{name}.parquet')
                    pbar.set_postfix_str(f"{name}: {data.shape}")
                    pbar.update(1)

                    # ì €ì¥ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
                    del data
                    gc.collect()

                except Exception as e:
                    print(f"{name} ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

        print("\n=== 6ë‹¨ê³„: í”¼ì²˜ ì •ë³´ ì €ì¥ ===")
        # í”¼ì²˜ ì •ë³´ ì €ì¥
        feature_info = {
            'total_features': len(feature_cols),
            'feature_names': feature_cols,
            'categorical_features': [col for col in feature_cols if col in ['gender', 'age_group', 'inventory_id', 'l_feat_14']],
            'numeric_features': [col for col in feature_cols if col.startswith(('feat_', 'history_'))],
            'engineered_features': [col for col in feature_cols if any(suffix in col for suffix in ['_sin', '_cos', '_log1p', '_sqrt', '_bin', '_enc', '_mult', '_add', '_ratio'])]
        }

        import json
        with open('processed_data/feature_info.json', 'w') as f:
            json.dump(feature_info, f, indent=2)

        print("\n" + "="*60)
        print("âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print("ìƒì„±ëœ íŒŒì¼ë“¤:")
        print("- processed_data/X_train.parquet")
        print("- processed_data/X_val.parquet")
        print("- processed_data/y_train.parquet")
        print("- processed_data/y_val.parquet")
        if X_test is not None:
            print("- processed_data/X_test.parquet")
        print("- processed_data/feature_info.json")
        print(f"ì´ {len(feature_cols)}ê°œì˜ í”¼ì²˜ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("="*60)

        return True

    except Exception as e:
        print(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ë©”ëª¨ë¦¬ ë¶€ì¡±ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ë” ì‘ì€ ì²­í¬ í¬ê¸°ë¡œ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.")
        return False

if __name__ == "__main__":
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
    import psutil
    process = psutil.Process()

    print(f"ì‹œì‘ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {process.memory_info().rss / 1024 / 1024:.1f}MB")

    success = main()

    if success:
        print(f"ìµœì¢… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {process.memory_info().rss / 1024 / 1024:.1f}MB")
    else:
        print("ì²˜ë¦¬ ì‹¤íŒ¨. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")