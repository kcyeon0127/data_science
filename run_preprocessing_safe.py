#!/usr/bin/env python3
"""
ë©”ëª¨ë¦¬ ì•ˆì „í•œ CTR ì˜ˆì¸¡ ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ì‹¤ì œ ì²­í¬ ë‹¨ìœ„ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
"""

import os
import pandas as pd
import numpy as np
from tqdm.auto import tqdm
import gc
from ctr_preprocessing import CTRDataPreprocessor
from preprocessing_utils import CTRPreprocessingUtils

def process_single_chunk_file(file_path, start_row, chunk_size, preprocessor, is_training, target_col):
    """ë‹¨ì¼ ì²­í¬ë¥¼ íŒŒì¼ì—ì„œ ì½ì–´ì„œ ì²˜ë¦¬"""
    try:
        # pandasì˜ skiprowsì™€ nrowsë¥¼ ì‚¬ìš©í•´ ì²­í¬ë§Œ ì½ê¸°
        chunk = pd.read_parquet(file_path)

        # ì‹¤ì œ ì²­í¬ í¬ê¸° ê³„ì‚°
        total_rows = len(chunk)
        end_row = min(start_row + chunk_size, total_rows)

        if start_row >= total_rows:
            return None, True  # ì™„ë£Œ ì‹ í˜¸

        # ì²­í¬ ë¶„í• 
        chunk_data = chunk.iloc[start_row:end_row].copy()
        del chunk
        gc.collect()

        # ì „ì²˜ë¦¬ ì ìš©
        if preprocessor:
            processed_chunk = preprocessor.preprocess_pipeline(
                chunk_data,
                is_training=is_training,
                target_col=target_col
            )
        else:
            processed_chunk = chunk_data

        return processed_chunk, False  # ì™„ë£Œë˜ì§€ ì•ŠìŒ

    except Exception as e:
        print(f"ì²­í¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return None, True

def safe_chunked_preprocessing():
    """ì•ˆì „í•œ ì²­í¬ ê¸°ë°˜ ì „ì²˜ë¦¬"""
    print("ë©”ëª¨ë¦¬ ì•ˆì „í•œ CTR ì˜ˆì¸¡ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")

    # ë©”ëª¨ë¦¬ ì²´í¬
    import psutil
    available_memory_gb = psutil.virtual_memory().available / (1024**3)
    print(f"ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {available_memory_gb:.1f}GB")

    # ë§¤ìš° ì•ˆì „í•œ ì²­í¬ í¬ê¸° ì„¤ì •
    if available_memory_gb < 4:
        chunk_size = 25000
        print("âš ï¸ ë§¤ìš° ì‘ì€ ì²­í¬ë¡œ ì²˜ë¦¬")
    elif available_memory_gb < 8:
        chunk_size = 50000
        print("ğŸ”§ ì‘ì€ ì²­í¬ë¡œ ì²˜ë¦¬")
    else:
        chunk_size = 100000
        print("ğŸš€ í‘œì¤€ ì²­í¬ë¡œ ì²˜ë¦¬")

    # ë°ì´í„° ê²½ë¡œ
    train_path = 'data/train.parquet'
    test_path = 'data/test.parquet'

    if not os.path.exists(train_path):
        print(f"ì˜¤ë¥˜: {train_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False

    # ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    preprocessor = CTRDataPreprocessor()

    try:
        print("\n=== 1ë‹¨ê³„: ë°ì´í„° í¬ê¸° í™•ì¸ ===")
        # ì‘ì€ ìƒ˜í”Œë¡œ í¬ê¸° ì¶”ì •
        sample = pd.read_parquet(train_path)
        total_rows = len(sample)
        print(f"ì´ ë°ì´í„° í¬ê¸°: {total_rows:,}í–‰")

        num_chunks = (total_rows // chunk_size) + 1
        print(f"ì˜ˆìƒ ì²­í¬ ìˆ˜: {num_chunks}")

        # ìƒ˜í”Œ ì¼ë¶€ë¡œ ì „ì²˜ë¦¬ê¸° í•™ìŠµ
        print("\n=== 2ë‹¨ê³„: ì „ì²˜ë¦¬ê¸° í•™ìŠµ ===")
        sample_small = sample.head(10000)
        del sample
        gc.collect()

        # ì¹´í…Œê³ ë¦¬ ìŠ¤ìº” (ì‘ì€ ìƒ˜í”Œë¡œ)
        scan_success = preprocessor.scan_categorical_values(train_path, chunk_size=chunk_size//2)

        # ì „ì²˜ë¦¬ê¸° í•™ìŠµ
        with tqdm(total=6, desc="ì „ì²˜ë¦¬ê¸° í•™ìŠµ") as pbar:
            sample_processed = preprocessor.preprocess_pipeline(
                sample_small,
                is_training=True,
                target_col='clicked',
                pbar=pbar
            )

        print(f"ì „ì²˜ë¦¬ê¸° í•™ìŠµ ì™„ë£Œ! ê²°ê³¼: {sample_processed.shape}")
        del sample_small, sample_processed
        gc.collect()

        print("\n=== 3ë‹¨ê³„: ì²­í¬ë³„ ì²˜ë¦¬ ë° ì €ì¥ ===")
        processed_files = []

        with tqdm(total=num_chunks, desc="ì²­í¬ ì²˜ë¦¬") as pbar:
            current_row = 0
            chunk_count = 0

            while current_row < total_rows:
                chunk_count += 1

                # ë‹¨ì¼ ì²­í¬ ì²˜ë¦¬
                processed_chunk, is_done = process_single_chunk_file(
                    train_path, current_row, chunk_size,
                    preprocessor, False, 'clicked'
                )

                if processed_chunk is not None:
                    # ì²˜ë¦¬ëœ ì²­í¬ë¥¼ ë°”ë¡œ íŒŒì¼ë¡œ ì €ì¥
                    output_file = f'processed_chunk_{chunk_count}.parquet'
                    processed_chunk.to_parquet(output_file, compression='snappy')
                    processed_files.append(output_file)

                    memory_mb = processed_chunk.memory_usage(deep=True).sum() / 1024 / 1024
                    pbar.set_postfix_str(f"ì²­í¬ {chunk_count}: {len(processed_chunk):,}í–‰, {memory_mb:.1f}MB")

                    del processed_chunk
                    gc.collect()

                current_row += chunk_size
                pbar.update(1)

                if is_done:
                    break

        print(f"\nì´ {len(processed_files)}ê°œ ì²­í¬ íŒŒì¼ ìƒì„±ë¨")

        print("\n=== 4ë‹¨ê³„: ìµœì¢… ê²°í•© ===")
        # ì‘ì€ ë°°ì¹˜ë¡œ ê²°í•©
        batch_size = 5
        final_data_parts = []

        for i in range(0, len(processed_files), batch_size):
            batch_files = processed_files[i:i+batch_size]

            print(f"ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘... ({len(batch_files)}ê°œ íŒŒì¼)")

            batch_data = []
            for file in batch_files:
                chunk_data = pd.read_parquet(file)
                batch_data.append(chunk_data)

            # ë°°ì¹˜ ê²°í•©
            if batch_data:
                batch_combined = pd.concat(batch_data, ignore_index=True)

                # ë°°ì¹˜ ê²°ê³¼ ì €ì¥
                batch_output = f'batch_combined_{i//batch_size}.parquet'
                batch_combined.to_parquet(batch_output, compression='snappy')
                final_data_parts.append(batch_output)

                del batch_data, batch_combined
                gc.collect()

        print("\n=== 5ë‹¨ê³„: Train/Val ë¶„í•  ===")
        # ì²« ë²ˆì§¸ ë°°ì¹˜ë§Œ ì‚¬ìš©í•´ì„œ ë¶„í•  (ë©”ëª¨ë¦¬ ì ˆì•½)
        if final_data_parts:
            print("ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ train/val ë¶„í• ...")
            combined_data = pd.read_parquet(final_data_parts[0])

            # íƒ€ê²Ÿ ë¶„ë¦¬
            feature_cols = [col for col in combined_data.columns if col != 'clicked']
            X = combined_data[feature_cols]
            y = combined_data['clicked']

            from sklearn.model_selection import train_test_split
            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )

            print(f"ë¶„í•  ê²°ê³¼:")
            print(f"  í›ˆë ¨: {X_train.shape}")
            print(f"  ê²€ì¦: {X_val.shape}")

            # ê²°ê³¼ ì €ì¥
            os.makedirs('processed_data', exist_ok=True)
            X_train.to_parquet('processed_data/X_train.parquet')
            X_val.to_parquet('processed_data/X_val.parquet')
            y_train.to_parquet('processed_data/y_train.parquet')
            y_val.to_parquet('processed_data/y_val.parquet')

            print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")

        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        print("\n=== 6ë‹¨ê³„: ì„ì‹œ íŒŒì¼ ì •ë¦¬ ===")
        for file in processed_files + final_data_parts:
            try:
                os.remove(file)
            except:
                pass

        print("ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
        return True

    except Exception as e:
        print(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        print(traceback.format_exc())
        return False

if __name__ == "__main__":
    success = safe_chunked_preprocessing()
    if success:
        print("ì „ì²˜ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("ì „ì²˜ë¦¬ ì‹¤íŒ¨")