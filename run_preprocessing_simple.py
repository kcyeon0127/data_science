#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ CTR ì˜ˆì¸¡ ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ë©”ëª¨ë¦¬ ë¬¸ì œë¥¼ ìµœì†Œí™”í•œ ë²„ì „
"""

import os
import pandas as pd
import numpy as np
from tqdm.auto import tqdm
import gc
from ctr_preprocessing import CTRDataPreprocessor
from preprocessing_utils import CTRPreprocessingUtils

def load_data_safely(file_path, max_rows=None):
    """ì•ˆì „í•˜ê²Œ ë°ì´í„° ë¡œë“œ"""
    print(f"ë°ì´í„° ë¡œë“œ ì¤‘: {file_path}")

    try:
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size_gb = os.path.getsize(file_path) / (1024**3)
        print(f"íŒŒì¼ í¬ê¸°: {file_size_gb:.2f} GB")

        # ì‘ì€ ìƒ˜í”Œë¡œ êµ¬ì¡° í™•ì¸
        sample = pd.read_parquet(file_path, nrows=1000) if max_rows else pd.read_parquet(file_path)

        if max_rows and len(sample) > max_rows:
            print(f"ë°ì´í„°ë¥¼ {max_rows:,}í–‰ìœ¼ë¡œ ì œí•œí•©ë‹ˆë‹¤.")
            sample = sample.head(max_rows)

        return sample

    except Exception as e:
        print(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

def main():
    print("ê°„ë‹¨í•œ CTR ì˜ˆì¸¡ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")

    # ë©”ëª¨ë¦¬ ì²´í¬
    import psutil
    available_memory_gb = psutil.virtual_memory().available / (1024**3)
    print(f"ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {available_memory_gb:.1f}GB")

    # ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ì²˜ë¦¬ ë°©ì‹ ê²°ì •
    if available_memory_gb < 4:
        max_rows = 100000
        print(f"âš ï¸  ë©”ëª¨ë¦¬ ë¶€ì¡±: ìƒ˜í”Œ ë°ì´í„°ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤ ({max_rows:,}í–‰)")
    elif available_memory_gb < 8:
        max_rows = 500000
        print(f"ğŸ”§ ë©”ëª¨ë¦¬ ì œí•œì : ì¼ë¶€ ë°ì´í„°ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤ ({max_rows:,}í–‰)")
    else:
        max_rows = None
        print("ğŸš€ ì „ì²´ ë°ì´í„° ì²˜ë¦¬ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")

    # ë°ì´í„° ê²½ë¡œ ì„¤ì •
    train_path = 'data/train.parquet'
    test_path = 'data/test.parquet'

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(train_path):
        print(f"ì˜¤ë¥˜: {train_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False

    # ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    preprocessor = CTRDataPreprocessor()
    utils = CTRPreprocessingUtils()

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('processed_data', exist_ok=True)

    try:
        print("\n=== 1ë‹¨ê³„: í›ˆë ¨ ë°ì´í„° ë¡œë“œ ===")
        train_df = load_data_safely(train_path, max_rows)
        if train_df is None:
            return False

        print(f"í›ˆë ¨ ë°ì´í„° í˜•íƒœ: {train_df.shape}")
        print(f"CTR: {train_df['clicked'].mean():.4f}")
        print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {train_df.memory_usage(deep=True).sum() / 1024 / 1024:.1f}MB")

        print("\n=== 2ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬ ===")
        with tqdm(total=6, desc="í›ˆë ¨ ë°ì´í„° ì „ì²˜ë¦¬") as pbar:
            train_processed = preprocessor.preprocess_pipeline(
                train_df,
                is_training=True,
                target_col='clicked',
                pbar=pbar
            )

        print(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {train_processed.shape}")

        # ì›ë³¸ ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ
        del train_df
        gc.collect()

        print("\n=== 3ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ ===")
        test_processed = None
        if os.path.exists(test_path):
            test_df = load_data_safely(test_path, max_rows)
            if test_df is not None:
                print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° í˜•íƒœ: {test_df.shape}")

                with tqdm(total=6, desc="í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬") as pbar:
                    test_processed = preprocessor.preprocess_pipeline(
                        test_df,
                        is_training=False,
                        pbar=pbar
                    )

                print(f"í…ŒìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì™„ë£Œ: {test_processed.shape}")
                del test_df
                gc.collect()

        print("\n=== 4ë‹¨ê³„: í›ˆë ¨/ê²€ì¦ ë¶„í•  ===")
        # íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬
        feature_cols = [col for col in train_processed.columns if col != 'clicked']
        X = train_processed[feature_cols]
        y = train_processed['clicked']

        print(f"í”¼ì²˜ ìˆ˜: {len(feature_cols)}")

        # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ë¶„í• 
        from sklearn.model_selection import train_test_split

        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        print(f"í›ˆë ¨ ì„¸íŠ¸: {X_train.shape}")
        print(f"ê²€ì¦ ì„¸íŠ¸: {X_val.shape}")

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del train_processed, X, y
        gc.collect()

        print("\n=== 5ë‹¨ê³„: ê°„ë‹¨í•œ ë¶„ì„ ===")
        # ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•˜ê¸° ìœ„í•´ ì‘ì€ ìƒ˜í”Œë¡œë§Œ ë¶„ì„
        if len(X_train) > 10000:
            sample_X = X_train.sample(n=10000, random_state=42)
            sample_y = y_train.loc[sample_X.index]

            try:
                print("í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ (ìƒ˜í”Œ)...")
                feature_importance = utils.analyze_feature_importance(
                    sample_X, sample_y,
                    feature_names=sample_X.columns,
                    method='mutual_info',
                    k=min(20, len(sample_X.columns))
                )

                print("ìƒìœ„ 5ê°œ ì¤‘ìš” í”¼ì²˜:")
                for i, row in feature_importance.head(5).iterrows():
                    print(f"  {row['feature']}: {row['importance']:.4f}")

            except Exception as e:
                print(f"í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")

        print("\n=== 6ë‹¨ê³„: ë°ì´í„° ì €ì¥ ===")
        save_files = [
            ("X_train", X_train),
            ("X_val", X_val),
            ("y_train", y_train),
            ("y_val", y_val)
        ]

        if test_processed is not None:
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ íƒ€ê²Ÿ ì»¬ëŸ¼ ì œê±°
            test_features = test_processed[feature_cols] if 'clicked' not in test_processed.columns else test_processed.drop('clicked', axis=1)
            save_files.append(("X_test", test_features))

        with tqdm(total=len(save_files), desc="ë°ì´í„° ì €ì¥") as pbar:
            for name, data in save_files:
                try:
                    # ë°ì´í„° íƒ€ì… ìµœì í™”
                    optimized_data = data.copy()

                    # ì •ìˆ˜í˜• ìµœì í™”
                    int_cols = optimized_data.select_dtypes(include=['int64']).columns
                    for col in int_cols:
                        optimized_data[col] = pd.to_numeric(optimized_data[col], downcast='integer')

                    # ì‹¤ìˆ˜í˜• ìµœì í™”
                    float_cols = optimized_data.select_dtypes(include=['float64']).columns
                    for col in float_cols:
                        optimized_data[col] = pd.to_numeric(optimized_data[col], downcast='float')

                    # ì €ì¥
                    optimized_data.to_parquet(f'processed_data/{name}.parquet', compression='snappy')

                    memory_saved = (data.memory_usage(deep=True).sum() - optimized_data.memory_usage(deep=True).sum()) / 1024 / 1024
                    pbar.set_postfix_str(f"{name}: {optimized_data.shape}, {memory_saved:.1f}MB ì ˆì•½")
                    pbar.update(1)

                    del optimized_data
                    gc.collect()

                except Exception as e:
                    print(f"{name} ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                    # ìµœì í™” ì—†ì´ ì €ì¥ ì‹œë„
                    try:
                        data.to_parquet(f'processed_data/{name}.parquet')
                        pbar.set_postfix_str(f"{name}: {data.shape} (ìµœì í™” ì‹¤íŒ¨)")
                        pbar.update(1)
                    except Exception as e2:
                        print(f"{name} ì €ì¥ ì™„ì „ ì‹¤íŒ¨: {e2}")

        print("\n=== 7ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ì €ì¥ ===")
        # í”¼ì²˜ ì •ë³´ ì €ì¥
        feature_info = {
            'total_features': len(feature_cols),
            'feature_names': feature_cols[:50],  # ì²˜ìŒ 50ê°œë§Œ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)
            'data_shape': {
                'train': list(X_train.shape),
                'val': list(X_val.shape),
                'test': list(test_processed.shape) if test_processed is not None else None
            },
            'memory_optimized': True,
            'sample_processed': max_rows is not None
        }

        import json
        with open('processed_data/feature_info.json', 'w') as f:
            json.dump(feature_info, f, indent=2)

        print("\n" + "="*60)
        print("âœ… ê°„ë‹¨í•œ ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print("ìƒì„±ëœ íŒŒì¼ë“¤:")
        for name, _ in save_files:
            file_path = f'processed_data/{name}.parquet'
            if os.path.exists(file_path):
                size_mb = os.path.getsize(file_path) / 1024 / 1024
                print(f"- {file_path} ({size_mb:.1f}MB)")
        print("- processed_data/feature_info.json")

        if max_rows:
            print(f"\nâš ï¸  ë©”ëª¨ë¦¬ ì œí•œìœ¼ë¡œ {max_rows:,}í–‰ë§Œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print("ì „ì²´ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•´ì„œëŠ” ë” ë§ì€ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        print("="*60)
        return True

    except Exception as e:
        print(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        print(traceback.format_exc())
        return False

if __name__ == "__main__":
    import psutil
    process = psutil.Process()

    print(f"ì‹œì‘ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {process.memory_info().rss / 1024 / 1024:.1f}MB")

    success = main()

    print(f"ìµœì¢… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {process.memory_info().rss / 1024 / 1024:.1f}MB")

    if success:
        print("ì²˜ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")